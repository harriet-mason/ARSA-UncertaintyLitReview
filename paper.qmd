---
title: "The Noisy Work of Uncertainty Visualisation Research: A Review"
author: Harriet Mason
bibliography: references.bib
date: last-modified
toc: true
number-sections: true
format: pdf
editor_options: 
  chunk_output_type: console
---

<!-- TODO 
- convert to American spelling
- May not need to reference the same paper again and again
- add visually integrable to fig-tax once I work out what it is
- look through for papers that can be used to cite motivation for uncertainty visualisation experiments
-->

```{r}
#| echo: false
#| message: false
#| warning: false


# load Libraries
library(tidyverse)
library(biscale)
library(RColorBrewer)
library(scales)
library(sf)
library(ggrepel)
library(urbnmapr)

```
# Background

From entertainment choices to news articles to insurance plans, the modern citizen is so over run with information in every aspect of their life it can be overwhelming. In this overflow of information, tools that can effectively summarize information down into simple and clear ideas become more valuable. Information visualisations remain one of the most powerful tools for fast and reliable science communication. 

There are many stages in our analysis that might benefit from the power of data visualisation, however this does not mean it is appropriately utilized or done with with effectiveness. Unlike an infographic, a statistical graphic is one of intent, and all enhancements made on a statistical graphic should be in pursuit of that goal (@Vanderplas2020). Visualization is an important step in exploratory data analysis and communication. At the exploration stage, it is utilized to **learn** what is important about a data set while the communication stage is utilized to **tell** others about what we have learned. Each of these visualisation goals utilize an important attribute of visualisation. In both these cases, visualisations are powerful tools that can help us understand **how** our data or results might diverge from pre-conceived notions or what we expect. 

Uncertainty visualisation is a relatively new field in research. Early papers that specifically reference "uncertainty visualisation" appear in the late 80s [@Ibrekk1987], with geospatial information visualisation literature in the early 90s declaring this to be essential aspect of information display [@MacEachren1992; @Carr1992]. These early experiments typically involved showing participants a distribution, such as those depicted in @fig-ibrekk, and asking the viewers to extract a probability or average. Despite the new terminology visualisation of uncertainty has been present since the earliest times. For example, box plots or histograms can be considered to be displaying uncertainty in the sense of variability in observations sampled from a population distribution.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-ibrekk
#| fig-cap: "A replication of the the uncertainty visualisations shown by @Ibrekk1987 in one of the earliest uncertainty visualisation experiments. This early experiment is a good example of many of the issues that are still common in uncertainty visualisation today. For example, the '95% confidence interval' is more accurately a '95% prediction interval'. Additionally graphics that depict different mathematical objects that are also different on their visual components are compared because of a percieved relation to uncertainty. So visualisations of the mean, PDF and CDF, are all discussed as though they all contain the relevant statistical information. The axis also have different scales, and visualisation methods that are now unpopular for displaying proportions, such as a pie chart, are used."
#| fig-subcap: 
#|   - "Picture 1"
#|   - "Picture 2"
#|   - "Picture 3"
#|   - "Picture 4"
#|   - "Picture 5"
#|   - "Picture 6"
#|   - "Picture 7"
#|   - "Picture 8"
#|   - "Picture 9"
#| layout-ncol: 3

# Generate data
set.seed(1)
x=rnorm(1000, 8, 4)
ib_data <- tibble(x=ifelse(x<0, -x, x))

# Picture 1
p1 <- ib_data %>%
  summarise(avg = mean(x),
          conf_95a = quantile(x, probs=c(0.025)),
          conf_95b = quantile(x, probs=c(0.975))) %>%
  ggplot(aes(y="NA")) +
  geom_point(aes(x=avg)) +
  geom_errorbar(aes(xmin = conf_95a, xmax = conf_95b), width = 0.1) +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,19),
                     labels= ggplot2:::interleave(as.character(c(seq(0,18, 2), 19)), rep("", 11))[c(0:19, 21)],
                     limits=c(0,19)) +
  theme_classic() +
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=1/10)

  
# Picture 2
p2 <- ib_data |>
  mutate(x = ifelse(x>18, 18, x),
         binx = cut(x, breaks=seq(0,18,2))) |>
  group_by(binx) |>
  summarise(n = n()) |>
  mutate(Probability = n / sum(n)) |>
  ggplot(aes(x=binx, y=Probability)) +
  geom_col(fill="black", colour="white") +
  scale_x_discrete(name = "INCHES OF SNOW",
                   labels= paste0(seq(0,16,2), sep = "-", seq(2,18,2))) +
  scale_y_continuous(breaks = seq(0.00, 0.25, 0.05)) +
  theme_classic() + 
  theme(aspect.ratio=0.33)

# Picture 3
# library(ggpattern)
p3 <- ib_data |>
  mutate(x = ifelse(x>18, 18, x),
         binx = cut(x, 
                     breaks=seq(0,18,2), 
                     labels= paste0(seq(0,16,2), sep = "-", seq(2,18,2)))) |>
  group_by(binx) |>
  summarise(n = n()) |>
  mutate(Probability = n / sum(n),
         csum = rev(cumsum(rev(Probability))), 
         pos = Probability/2 + lead(csum, 1),
         pos = if_else(is.na(pos), Probability/2, pos)) |>
  ggplot(aes(x="", y=Probability, fill=binx)) +
  geom_bar(stat="identity", width=1) +
  geom_text_repel(aes(y = pos, label = paste0(round(Probability*100), sep="", "%")),
                   size = 3, nudge_x = 0.6, show.legend = FALSE, segment.color = 'transparent') +
  #geom_label(aes(label = paste0(round(Probability*100), sep="", "%")),
  #          position = position_stack(vjust = 0.5)) + 
  scale_fill_grey() +
  coord_polar("y", start=0) +
  labs(fill = "INCHES OF SNOW") + 
  theme_void() + 
  theme(aspect.ratio=1)

# Picture 4
p4 <- ib_data |>
  ggplot(aes(x=x)) +
  geom_density() + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2))) +
  scale_y_continuous(name = "Probability density",
                     breaks = seq(0.00, 0.20, 0.02)) +
  theme_classic() + 
  theme(aspect.ratio=0.33)



# Picture 5
p5 <- ib_data |>
  ggplot(aes(y="", x=x)) +
  geom_violin() + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.4)

# Picture 6
set.seed(1)
x=rnorm(5000, 8, 4)
ib_data2 <- tibble(x=ifelse(x<0, -x, x)) |>
  mutate(x=ifelse(x>=18, 18-rexp(5000,rate=0), x))
p6 <- ib_data2 |>
  ggplot(aes(y="", x=x)) +
  geom_jitter(size=0.05) + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.1)


# Picture 7
p7 <- ib_data2 |>
  arrange(x) |>
  mutate(group = rep(1:50, each=100))|>
  group_by(group) |>
  summarise(x = max(x, na.rm=TRUE)) |>
  add_row(group=c(0,51), x = c(0,20)) |>
  ggplot(aes(x=x)) +
  geom_linerange(ymin = 0.1, ymax = 1) + 
  geom_linerange(y=1, xmin = -0.03, xmax = 20.03)+ 
  geom_linerange(y=0.1, xmin = -0.03, xmax = 20.03)+ 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  scale_y_continuous(limits=c(0,1)) + 
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.1)

# Picture 8
p8 <- ib_data %>%
  reframe(x = quantile(x, probs=c(0.25, 0.50, 0.75)))|>
  add_row(x = c(0,20)) |>
  arrange(x) |>
  mutate(quantile = c("min", "q1", "med", "q3", "max")) |>
  pivot_wider(names_from = quantile, values_from = x) |>
  ggplot(aes(y="")) +
  #geom_point(aes(x=med)) +
  geom_errorbar(aes(y="", xmin = min, xmax = max), width = 0.2) +
  geom_crossbar(aes(y="", x=med, xmin = q1, xmax = q3), width = 0.5) +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,20),
                     labels= ggplot2:::interleave(as.character(c(seq(0,20, 2))), rep("", 11))[1:21],
                     limits=c(0,20)) +
  theme_classic() +
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=1/10)


# Picture 9
p9 <- ib_data |>
  ggplot(aes(x)) +
  stat_ecdf(geom = "step") +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,20),
                     labels= ggplot2:::interleave(as.character(c(seq(0,20, 2))), rep("", 11))[1:21],
                     limits=c(0,20)) +
  scale_y_continuous(name = "Cumulative probability",
                     breaks=seq(0,1,0.1),
                     labels= seq(0,1,0.1),
                     limits=c(0,1)) +
  theme_classic() +
  theme(aspect.ratio=4/10)

# Display Plots
p1
p2
p3
p4
p5
p6
p7
p8
p9
```

However, uncertainty visualisation takes many forms today, It can be considered to expand the application of data visualisation towards making better decisions, and research is concerned about how the construction of plots and information contained might affect the resulting decisions. With the abundance of publications it is timely to consider a review of the state-of-the-art. In fact, there have already been several reviews published. 

Interestingly, these reviews often do not offer overarching rules for tried and tested uncertainty visualisation, but rather comment on the *difficulties* faced when trying to summarise the papers from this field. @Kinkeldey2014 found most experiments on the methods for uncertainty visualisation evaluation to be ad hoc, with no commonly agreed upon methodology or formalisation and no greater goal of describing general principals. @Hullman2016 commented on the difficulty in taking overarching themes from uncertainty visualisation, as several conflated issues make it unclear if subjects did poorly in an experiment because they misunderstood a visualisation, because the question was misinterpreted, or because they used a specific heuristic. @Spiegelhalter2017 commented that different plots are good for different things, and disagreed with the goal of identifying a universal "best" plot for all people and circumstances. @Griethe2006 was unable to find common themes, but instead listed the findings and opinions of a collection of papers.

There are several reasons provided to explain the difficulty in generalising the uncertainty visualisation literature. Some suggested or agree with the idea that visualisation typologies should move away from data types, uncertainty categories, and representation types and towards "task-centred typologies" [@Kinkeldey2014; @Hullman2016]. Other papers indirectly hint at this by arguing that the choice of best visualisation is highly dependent on the specific goal [@Griethe2006; @Spiegelhalter2017]. Others contend that visualisations are highly dependent on the audience and there is no such thing as a "best" visualisation that will be accessible to all members [@Kinkeldey2014; @Spiegelhalter2017]. These concerns appear across all application areas, and an underlying thread is that the problem arises from a lack of a cohesive and encompassing **definition of uncertainty**. 

The review provided here attempts to address these issues. We highlight the three key issues facing the uncertainty visualisation literature, specifically the lack of a definition of uncertainty, the confusion as to what *is* an "uncertainty visualisation", and the difficulties with measuring "uncertainty". When disucssing each of these issues, we will discuss how this concept is understood in existing literature, why this understanding may be flawed, and the issues that arrise due to these flaws. By considering the *purpose* of visualizing uncertainty we avoid providing a simple list of the current methods, and instead consider if the current visualisation tools are sufficient to achieve that purpose and if not, where there is need for new methodology. The ideal role of this review to help synthesize this noisy field, by collecting together essential components, summarizing different viewpoints the current literature, and provide a guide for potential research topics. 

# The purpose of uncertainty visualisation
## Other motivations 
## Signal supression

# When is signal supression appropriate?
Signal suppression is not always possible in a visualisation. Whether or not uncertainty should be used for signal-suppression largely depends on the purpose of the visualisation and the definition of uncertainty. 

## What is uncertainty
### Current definitions of uncertainty
Most definitions of uncertainty treat it as a feature of the data, as something that exists in our data no matter it's purpose. This is not entirely accurate. 

This view of uncertainty leads to a wide range of definitions that are designed to be encompassing, but struggle to capture the full picture. Some works [@Hullman2018; @Maceachren2012; @Thomson2005] focus narrowly on specific terms defined mathematically, such as probability, variance, error, or precision. Others [@Griethe2006; @Pang1997; @Pham2009; @Boukhelifa2017] include broader loosely related elements, such as missing values. Some authors use taxonomies to split uncertainty based on an endless stream of ever changing boundaries, such as whether the uncertainty is due to true randomness or a lack of knowledge [@Spiegelhalter2017; @Hullman2016; @utypo], if the uncertainty is in the  attribute, spatial elements, or temporal element of the data [@Kinkeldey2014], whether the uncertainty is scientific (e.g. error) or human (e.g. disagreement among parties) [@Benjamin2018], if the uncertainty is random or systematic [@Sanyal2009], statistical or bounded [@Gschwandtnei2016; @Olston2002], recorded as accuracy or precision [@Griethe2006; @Benjamin2018], which stage of the data analysis pipeline the uncertainty comes from [@utypo], how quantifiable the uncertainty is [@Spiegelhalter2017; @utypo], etc. In their own way, each of these approaches show an aspect of uncertainty that an author felt was important to differentiate. 

The constantly shifting definitions make it hard to pin down and quantify uncertainty. The ramifications of this is seen throughout the literature, as a survey of visualisation authors cited "not knowing how to calculate uncertainty" as one of the primary reasons they did not include it in visualisations [@Hullman2020a]. 




The constanly shifting definitions might be a result of the view of uncertainty as a latent variable that can exist without an explicit purpose. This is not true, as uncertainty only exists as a by-product of inference.


### The case for uncertainty as a by product of inference
It is easy to see uncertainties relationship to inference when we consider what might *not* be considered uncertain rather than just trying to think about what *is*. 

Descriptive statistics simply describe our sample as it is and summarizes large data down into an easy to swallow format. Descriptive statistics are not seen as the primary goal of modern statistics, however, this was not always the case. Around the 19th century in England, *positivism* was the popular philosophical approach to science (positivists included famous statisticians such as Francis Galton and Karl Pearson) and practitioners of the approach believed statistics ended with descriptive statistics as science must be based on actual experience and observations, therefore anything that refers to the unobservable (such as new observations or population statistics) is not true science [@Otsuka2023]. In order to make statements about population statistics, future values, or new observations we need to perform inference, which requires the assumption of the "uniformity of nature" (i.e. that unobserved phenomena should be similar to observed phenomena) [@Otsuka2023]. This subtle shift, from descriptive statistics to inferential statistics was shunned during the positivism era *due to the fact it introduced the unknowable*, or in other words, uncertainty. 

This approach to uncertainty is embedded in the idea that descriptive statistics do not have uncertainty, which some readers may disagree with. Specifically, because it means uncertainty is *not* a latent attribute of data, but rather an attribute of a specific hypothesis or estimate. Deniers of this fact follow a consistent logical path and it is easy to identify the common mistake. We know that variance and probability are typically considered types of "uncertainty" *and* descriptive statistics can have variance and probabilities *therefore* descriptive statistics must have uncertainty. The flaw in this logic comes from the first step, assuming that the tools with which we measure uncertainty *are* uncertainty in of themselves. This confusion is common and there are many papers that spend a great deal of time clarifying the difference. @Begg2014 highlight that uncertainty is related to not knowing a specific value, while variability refers to the range of values a quantity can take at different locations, times or instances. @Spiegelhalter2017 made sure to comment on the difference between precise random events (such as the probability associated with a coin flip), and uncertainty (such as the estimated probability associated with a coin that might be biased). The variance of a sample variance can be calculated and know, therefore it is not uncertain but rather it a precise description of dispersion. If we were to discuss drawing a new observation, or estimating the true mean of a population *then* the variance would become relevant in our discussions of uncertainty. 

Several authors have connected the issues around defining uncertainty to inference, however it is often discussed as a *task* or *goal* dependence. Multiple authors have commented on the need to consider quantifying and expressing uncertainty at every stage of a project as the "goal" shapes every step of the analysis [@Kinkeldey2014; @Hullman2016; @Refsgaard2007]. @Otsuka2023 suggested that the process of observing data to perform statistics is largely dependent on our goals, because the process of boiling real world entities down into probabilistic objects (or "probabilistic kind" as he puts it) depends on the relationship we seek to identify with our data. @Meng2014 commented what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the the question we seek to answer. @Kale2019 discussed how the choices we make in our analysis impact our outcomes and introduce uncertainty. @Carlin2023 mentions that each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to statistical analysis that is devoid of meaning. @Wallsten1997 argue that the best method for evaluating or combining subjective probabilities depends on the uncertainty the decision maker wants to represent and why it matters. @Fischhoff2014 looks at uncertainty visualisation for decision making decides that we should have different ways of communicating uncertainty based off what the user is supposed to do with it. The importance of inference when discussing uncertainty is never directly acknowledge, but it is always present. It is clear that the relationship between uncertainty and inference is noticed at every stage in an analysis, however combining these uncertainties into a single "uncertainty" value is near impossible [@Spiegelhalter2017]. 



With this understanding it becomes clear to see why uncertainty is tied to an endless string of examples in the data analysis pipeline. Uncertainty examples include imputed data, model selection, inherent randomness, biased sampling, etc, not because these things *are* uncertainty, but because they *create* uncertainty when we perform inference. Whether or not these elements are relevant is highly dependent on what statistic you are trying to draw inference on, and by extension, the purpose of your visualisation.

## EDA vs Communication
## If signal supression is not the goal

# How should be test uncertainty visualisations?
## Uncertainty as signal
## Uncertainty as noise
## Other uncertainty experiments 
-----------------------------------------------

# UP TO HERE FOR SORTING

# What is uncertainty?


## Defining uncertainty with respect to inference


For example, @Boukhelifa2012 tried to quantify the strength of the intuitive connection between a line attribute called "sketchiness" and uncertainty. Participants were shown the six scenarios depicted in @fig-sketchy and asked to interpret what they believed the squiggly line indicates. The authors were aware that uncertainty seemed to have some "task" dependence but interpreted this as *context* dependence rather than dependence on a particular inferential statistic. Therefore, "sketchiness" was added to aspects of the visualization that had no obvious inferential statistic associated with it, so participants simply ignored the "sketchiness" or assumed it represented something else, such as an alternative option. In another experiment, 

@Hofman2020 asked participants to judge the effectiveness of a particular treatment. One group was shown the prediction interval around the mean, which indicates the uncertainty associated with treatment *for a particular person* and another group was shown the the sampling interval which depicted the uncertainty associated with the *average effectiveness of the treatment*. There was no critical assessment by the authors on *why* these two distributions should be compared. The contribution of this experiment hinges on the belief that sharing a common descriptive statistic (in this case, the mean) is grounds to compare two unrelated inferential statistics, a belief which is clearly false when viewing uncertainty through the lens of inference.

![The graphics displayed by @Boukhelifa2012 to identify if there is an intuitive connection between sketchiness and uncertainty. These graphic were made without concern as to what (i.e. which statistic) in the image is supposed to be uncertain. This leads to the images being difficult to connect to uncertainty even if we assume tha that is what it is supposed to represent. For example, (S5) implies that participants should have read the plot to mean that the creators of a rail network map would be "uncertain" about the existence of a train line, something that is defies common logic.](sketchiness.png){#fig-sketchy width=50%}

Additionally, if we view uncertainty through the lens of inference then it is obvious that bias and variance both contribute to uncertainty regardless as to which stage of the analysis they appear. Many papers in uncertainty visualisation are not working from this null hypothesis. For example, @Padilla2021 found that high uncertainty in the model estimates (calculated uncertainty) and low forecaster confidence (which is typically an expression of suspected bias in the model) both caused participants to have decreased confidence in their results and suggested modelers express both if they are relevant. In another example, @Kale2019 discusses the importance of communicating decisions made in the data analysis pipeline and being aware of the alternatives. While there is nothing wrong with the results of these papers, the fact that they were published shows some level of surprise by these results. This work indicates that there was a significant number of authors in the field were not already aware that choices introduced early in the data analysis pipeline create bias and therefore uncertainty in our final values that should be communicated. 

Existing mathematical definition of uncertainty (or related concepts) have been unable to unify this range of diverse concepts. For example, @Thomson2005 suggests a mathematical formula for *examples* of uncertainty, @Meng2014 mathematically defined the variance introduced to a model by the array of model choices, and information theory tries to quantify uncertainty using the idea of entropy. These existing methods are useful for calculating uncertainty as a specific stage of the analysis, but not thorough enough for an analyst to understand the causes of uncertainty and quantify it. The reality is that a lot of uncertainty visualisation authors do not seem to have an intuitive understanding of uncertainties connection to inference, when inference is being performed, and how to communicate this complicated by product of inference. This misunderstanding is not due to laziness or the fault of the authors, but rather is likely caused by the confusing set of existing definitions that leads authors to be unaware what they are supposed to be visualising.  Therefore, it is clear the field would greatly benefit from a strict definition of uncertainty. The task of providing an encapsulating mathematical definition of uncertainty is far beyond the scope of this work, however focusing on its relationship to inference is likely a fruitful step forward.

# What is an uncertainty visualisation?
Given that there is no definition of "uncertainty" readers may wonder how we can define "uncertainty visualisations". This is a reasonable issue given that you cannot quantify something that is not defined, and you can't visualise something you cannot quantify. This section discusses how the problems caused by the absent definition of uncertainty bleed into our understandings of uncertainty visualisation and creates a field that cannot clearly state what is or is not within its domain.

## Definitions of "uncertainty visualisation"
What exactly is or is not an uncertainty visualisation is just as poorly defined as the definition of uncertainty, so authors seem to decide for themselves what is an uncertainty visualisation. This appears in the literature as a mountain of seemingly conflicting statements about what is, or is not, an "uncertainty visualisation". For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, however at least one or both of these charts are used in a significant number of uncertainty visualisation experiments [@Ibrekk1987; @Olston2002; @Zhao2023; @Hofmann2012]. @Wickham2011 suggests their product plot framework, which includes histograms, should have a way to measure uncertainty, but does not consider that a histogram is *already* a depiction of PDF, something that is often considered an uncertainty visualisation by other authors. This completely untethered concept of an "uncertainty visualiation" leads to many user experiments where the visualisations used have a shocking information asymetry.

Visualisation authors are almost unanimous in commenting that the "information" in two plots must be the same in order for the visual techniques to be compared [@Cleveland1984; @Kinkeldey2014]. @Kinkeldey2014 adopts an existing definition also that suggests two graphics are informationally equivalent if all the information in one plot is inferable from the other and vice-versa, but adds that two plots are computationally equivalent if that information can be extracted from both plots with similar easy and speed. Many visualisation authors ignore this concept and simply compare two visualisations because they both belong to the class of "uncertainty visualisations". @Ibrekk1987 compared a 6 visualisations of a PDF,  a box plot, a CDF and a mean with a confidence interval on this basis. They found that people are better at extracting the mean from a plot when they are shown a plot that contains a mean with a confidence interval than when they are shown a box plot, or other visualisations that did not allow for the mean to just be read off the plot. @Hullman2015 compared the static error bars and violin plots of the marginal distributions of two variables ($A$ and $B$) to an animated plot that depicted outcomes of the joint distribution of $A$ and $B$ in each frame and found that the visualisation of the joint distibution was better at answering questions about the joint distribution than the visualisations of the marginal distribution. @Hofman2020 commented that "theoretically" the sampling distribution of the mean and the prediction interval of a new observation are equal "so long as one knows the sample size", but does not seem to provide participants with that sample size, or recognise the assumptions and background knowledge that would be required to compare the two. @Hofman2020 and @Zhang2022 compared prediction and sampling distributions because they are both "uncertainty" that is typically depicted around the mean. They found that people are better at answering questions about a prediction interval when shown a prediction interval instead of a sampling distribution. 

This collection of examples starts to paint a pretty clear picture. Visualisations with rather shocking information asymmetry are regularly compared because they are both "uncertainty visualisations". This results in a series of experiments where the visual aspects of the graphic are not even required to anticipate the experimental results. Ideally this problem would be avoided by defining the information in a graphic using *The Grammar of Graphics* since it is a sufficient summary of the information in a graphic and it allows a plot to be considered a statistic [@Vanderplas2020]. The process for summarising the information in a plot using *The Grammar of Graphics* is depicted in @fig-pipeline [@Leland2005]. Unfortunately, *The Grammar of Graphics* is not perfectly suited to summarise the information in an uncertainty visualisation since it assumes that we start from a finite data set. Very often uncertainty is expressed using a distribution or a resampling method that does not have an upper limit on the "sample size". Additionally many uncertainty visualisation experiments notice an interaction between sample size and graphic effectiveness [@Kale2018; @Newburger2022; @Hofmann2012], an effect that is likely caused by a difference in human perception of mass versus sample. Additionally, *The Grammar of Graphics* behaves strangely when the sample size is adjusted and it the distance between two graphics becomes less consistent. @fig-infintegraph shows how three graphics defined by *The Grammar of Graphics* can change as n approaches infinity. The two graphics that are very similar according to the *The Grammar of Graphics* diverge into different graphics, and two graphics that are very different, converge into the same graphic. This means the statistics visualised in the plot as well as the *The Grammar of Graphics* may not be an adequate to summarise the information in a plot that is being used for uncertainty visualisation. Some authors have noticed difficulties with visualising distributions using `ggplot2` and created extensions that make showing distributions more intuitive [@Pu2020; @Kay2023], however these extensions do not address the core discrepancies with uncertainty visualisation and *The Grammar of Graphics*. This does not mean *The Grammar of Graphics* should be abandoned when we attempt to summaise the information in an uncertainty visualisation, very often visations use the same sample to generate all their plots, so this issue is not always present, however these limitations should be kept in mind when analysing uncertainty visualisations using *The Grammar of Graphics*.

![(The grammar of graphics data analysis pipeline from @Leland2005 written in black, with the tidy data equivalent written in blue. The product plot framework and the grammar of probabilistic graphics frameworks are in green. It can be seen that the grammar of graphics provides a set of instructions that create a graphic, however the inferential power of this graphic is not considered and relies on many additional assumptions that are not defined by the grammar.](grammarofgraphicspipeline.jpeg){#fig-pipeline}
 
 ![This graphic shows three simple graphics made from the same data set that diverge according to the grammar of graphics. Despite being distinct graphics from a stage as low down as the statistic (or data manipulation in the tidy data framework), as our sample size increases to infinity, the grahics converge to the same distribution and become visually indistinguishable. ](infinitegraph.jpeg){#fig-infinitegraph}
 
An alternative way authors consider describing an uncertainty visualisation is to use taxonomies. However, these taxonomies tend to blur the line between the mathematical elements of uncertainty and the visual depiction of those mathematical objects. Some taxonomies of uncertainty highlight how confusing most authors find the concept, and how difficult it is to articulate the rules of an uncertainty visualisation when uncertainty itself if not defined. @Kinkeldey2014 categorised uncertainty according to five criteria depicted in @fig-tax which considers if a visualisation is implicit or explicit, intrinsic or extrinsic, visually integrable or seperable, coincident or adjacent, and static or dynamic. A similar version of this taxonomy was presented by @uncertchap2022 who commented that visualisation can be organised into two categories, "graphical annotations of distributional properties" and "visual encodings of uncertainty" which seems to functionally align with the intrinsic/extrinsic distinction by @Kinkeldey2014. @Griethe2006 organised uncertainty visualisations into two cases (1) a hypothesis test was preformed to confirm the validity of the visualisation and (2) the visualisation has uncertainty depicted. @Potter2012 organised several existing uncertainty visualisations into groups based on the dimensionality of the data (1D, 2D, 3D, and No Dimension) and the dimensionality of the uncertainty (Scalar, Vector, Tensor).  However, because the term "PDF", function that is used to describe a random variable, is used to describe both the data and the uncertainty for all dimensions. @Grewal2021 created a taxonomy that mapped uncertainty visualisations to some point in a 2D space defined by the "domain expertise" and "continuum of discreteness" (that scaled from "point estimate" to "continuous distribution"). 

![Visualisation of @Kinkeldey2014 uncertainty visualisation taxonomy. It can be seen that most of the categories can be considered a change to the typical error bar chart. The taxonomy categorises the visualisations based on five criteria, implicit (a sample) or explicit (a depiction of mass); Intrinsic (alter existing symbols symbols to represent uncertainty) or extrinsic (add new objects to represent uncertainty); visually integral or separable (the uncertainty can be separated from the data and read independently) ; coincidence (uncertainty and data are represented in the same plot) or adjacent; static or dynamic (animated, interactive etc). Many of the distinctions conflate changes of the input data and statistics calculated and with visual changes, highlighting how defining uncertainty and visualisation together conflates several elements of a graphic.](uncertvistax.jpeg){#fig-tax}

The authors of these taxonomies did not intend for them to be complete descriptions of uncertainty visualisations, however they do paint a picture of the what authors consider to be important. There are common themes in the taxonomies that ensure we differentiate between a sample and a mass, consider if uncertainty should be depicted with the estimate or as a second variable, identify the parallel of hypothesis testing, and consider the dimensionality, sample size and precision depicted by the visualisation. These taxonomies may highlight perceived important differences, but they do not help us understand what *is* or *is not* an uncertainty visualisation. For that we need to understand *why* uncertainty visualisation exists as a sub-field at all.

## Uncertainty visualisations should perform signal supression
It is rarely made clear exactly how uncertainty visualisation is different to normal information visualisation. Visualising a graphic is nothing more than computing a statistic, mapping that statistic to a visual feature and rendering that graphic. Nothing in this process suggests visualising an estimate of the variance should be any different to visualising an estimate of the mean, however this field *does* implicitly treat them different. Why? 

Uncertainty visualisation papers usually provide one of two primary justifications for their existence. The first justification is that uncertainty is fundamentally differently to other variables due to the psychological heuristics involved in interpreting uncertainty. Therefore uncertainty visualisation is different to normal visualisation as authors must consider the psychological effects of what they are visualising [@Spiegelhalter2017; @Hullman2019]. The second justification assets that uncertainty is an additional variable and it is of vital importance to interpreting an estimate. Therefore uncertainty visualisation is a high dimensional visualisation problem, as we need to figure out how to seamlessly add uncertainty into already existing graphics [@moritz2017trust; @Griethe2006]. Why some authors pick one motivation over another is rarely explained and both leave the role of uncertainty in the visualisation unclear. The literature never explains if uncertainty should be treated as a variable, as metadata, or as something else entirely [@Kinkeldey2014]. @Hullman2016 commented that it is straightforward to show a value but it is much more complex to show uncertainty, but did not explain *why* Interestingly, these motivations do not remotely line up with the most common reason that people *outside* the uncertainty visualisation community thinks visualising uncertainty is important. The true reason that uncertainty visualisation is differerent to normal information visualisation, is that normal information visualisation is designed to *find* signals in our data, while uncertainty visualisation should be designed to *supress* it. 

Interviews with experts in visualisation and statistics often back up "signal suppression" as the primary motivation for uncertainty visualisation. Failling to communicate uncertainty is often considered to be akin to fraud or lying [@Hullman2020a; @Manski2020] indicating that uncertainty should exist to prevent undue certainty in visual inference. The relationship between signal supression and uncertainty visualisation is made more explicity by other authors [@Griethe2006]. Therefore uncertainty acts as a form of statistical "hedging" for signals found in an analysis. This aligns nicely with our understanding of uncertainty as a by product of inference. The variable we are trying to perform inference on is the signal, while the while the visualisation of noise that prevents the signal from being read is the uncertainty. It is nice that the connection to inference easily extends from uncertainty to uncertainty visualisation, however many of the confusions extend as well. 

There are two primary reasons for making a visualisation, to perform exploratory data analysis (EDA) or communication. Communication involves identifying a signal we want to communicate and designing the visualisation that best conveys that, while EDA involves creating a versatile visualisation without an explicit purpose and using it to extract several signals. Since EDA is the visual parallel to descriptive statistics, it is performed without an explicit hypothesis which means there *is* no uncertainty in the visualisation. Similarly to our issues with descritive statistics, this is not well understood by the uncertainty visualisation community. Some authors simply confuse varianc for uncertainty. For example @Potter2010 aimed to create a summary plot that "concisely presented data with uncertainty information" to create an exploratory visualisation tool that visualised uncertainty. Other authors regonise inference will occur (in some shape or form) and believe uncertainty *should* be visualised but do not recognise *how* uncertainty would be visualised. @Hullman2021 argued that there is no such thing as a "model-free" visualisation, therefore visualisation require robust visualisations of uncertainty as we are always performing inference.  @Griethe2006 commented that "if visualization is used as a means to explore a data volume or to communicate its contents the uncertainty has to be included". While we agree people cannot prevent themselves from performing inference, that does not mean it is possible for uncertainty to be included in an EDA visualisation. A versatile visualisation such as a scatter plot allows for a viewer to consider several hypothesis at once, each of which will be a different inferential statistic with a different distribution that depicts its uncertainty. It is likely impossible to suppress all possible signals at once.

{{< pagebreak >}}

-----------------------------------------------

# UP TO HERE FOR RESTRUCTURE/REWRITE

### Interpretation and semantics
Interpretation and semantics experiments are seeking to identify a dimension (or visual task) that uncertainty naturally maps to. These experiments inherently view uncertainty as a variable that is separate to the variable on which we have mapped our signal. For example, lets say we have a map were maximum daily temperature is presented using points where the colour (red for hot and blue for cold) of the point is associated with the temperature, and the blurriness of that point is associated with the variance *of* that temperature. It is highly likely that our brains will not flatten that into a single variable depicting noise and signal, but rather *separately* extract the temperature (colour) and uncertainty (blur) information as two independent variables. If the variables are extracted separately, there is no guarantee that the uncertainty will act as an appropriate signal suppressor. This problem has been noticed by others in the field, that typically use this method (specifically in the spatial uncertainty context) and a desire for representations that integrate uncertainty and signal is one of the reasons for the invention of the value-suppressing uncertainty pallet [@Correll2018].  

Value-suppressing uncertainty pallets (VSUP) were developed as a method that would allow the signal and the noise to be interpreted together such that insights gained by the viewers of a plot are appropriately *suppressed* by the uncertainty. Hence the name of the pallet. @fig-maps depicted this map colouring approach and several other extensions on the typical choropleth map that differ in where in the visualisation process they combine the noise and signal information into a single concept of "valid signal". The first and most basic map is a simple choropleth map where each value (the colouring of each local government area) has no associated "uncertainty". The next map (which embodies the approach taken by the semantics experiments) is the bivariate map which maps the signal to colour value and the uncertainty to colour hue. If visualisation was performing signal suppression then that would mean the two dimensional space defined by colour value and colour hue can be mentally "flattened" into a single dimension of "valid value" that can captures the signal the our brain would need to be able to flatted this two dimensional space into a single space of "signal validity".  The idea of two perceptual tasks flattening into one variable in the mind of the viewer may be wishful thinking, but it is not impossible given we are not certain on how the perceptual tasks are mapped within the human brain. @Sterzik2023 found that when a value was mapped to the textures of stippling, hatching, and triangles, and found that the difference between two points on this one dimensional texture was actually a 2D space (likely "texture business" and "light/darkness"). That being said, if we look at the visual signals presented by the bivariate map, where the contrasting light and dark areas actually has no important meaning, it is unlikely this occurs for colour value and hue (or at least it doesn't occur in a way that is useful for uncertainty visualisation). Instead of hoping that uncertainty might collapse signal values into a single dimension, we can do some of that work ourselves, by using a VSUP which collapses the colour space such that high uncertainty values cannot be extracted. It is unclear how useful VSUPs are at actually combining signal and noise and therefore suppressing plot level insights, as they have only been tested on simple value extraction tasks that require evaluating a single point [@Correll2018; @Ndlovu2023] rather than looking for spatial relationships (which is arguably what maps are for). Following along with this trend, the next way we might consider visualising uncertainty is to combine uncertainty and signal at the earlier stage so the "suppressed signal" is represented by a single variable. This statistic can then be expressed in a one dimensional colour space, which is a method adopted by the Bayesian surprise metric map [@Ndlovu2023] and the exceedance probability map [Lucchesi2017]. 
 
```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-maps
#| fig-cap: "A visualisation of a typical choropleth map, as well as three other maps that are used to display uncertainty on the choropleth map. There is a high variance and low variance example for each type of map to show how well the technique supresses signal. Each map was created using the same data with the same base palette. At first glance, the high uncertainty bivariate and VSUP maps just look like maps with a low saturation colour palette, rather than map with high uncertainty. This visualisation makes it clear that supression methods that plot uncertainty to a second axis, such as hue, make uncertainty appear as a second variable, rather than a signal supression on our estimate. "
#| fig-subcap: 
#|   - "Choropleth Map: Low Variance"
#|   - "Choropleth Map: High Variance"
#|   - "Choropleth Palette"
#|   - "Bivariate Map: Low Variance"
#|   - "Bivariate Map: High Variance"
#|   - "Bivariate Palette"
#|   - "VSUP Map: Low Variance"
#|   - "VSUP Map: High Variance"
#|   - "VSU Palette"
#|   - "Pixel Map: Low Variance"
#|   - "Pixel Map: High Variance"
#|   - "Pixel Palette"
#| layout-ncol: 3

# seed for sampling
set.seed(1997)


# Get map data
my_map_data <- get_urbn_map("counties", sf = TRUE) |>
  dplyr::filter(state_name=="Iowa")
centroids <- as_tibble(rgeos::gCentroid(as(my_map_data$geometry, "Spatial"), byid = TRUE))
my_map_data$cent_long <- centroids$x
my_map_data$cent_lat <- centroids$y
n <- dim(my_map_data)[1]

# Make standard Palette
# set Base colours
basecols <- brewer.pal(8, name = "YlOrRd")
#breaks <- seq(0,41, length.out = 9)
breaks <- 21:29 #c(0,15,24,25,26,27,30,35,41)
names(basecols) <- seq(8)
limits <- c(10,41)

my_map_data <- my_map_data |>
  dplyr::mutate(temp = 28 - ((scale(cent_long))^(2) + (scale(cent_lat))^(2))[,1], # trend
         notrend = rnorm(n), # no trend
         highvar = runif(n, min=2, max=4), # high variance
         lowvar = runif(n, min=0, max=2), # low variance
         ) |>
  pivot_longer(cols=highvar:lowvar, names_to = "variance_class", values_to = "variance") |>
  # add bivariate classes to data
  mutate(bitemp = cut(temp, breaks=breaks, labels=seq(8)),
         bivar = cut(variance, breaks=0:4, labels=seq(4)),
         biclass = paste(bitemp, bivar, sep="-")
  )

# Choropleth Map
p1a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = bitemp, 
              geometry = geometry)) + 
  scale_fill_manual(values = basecols) +
  #scale_fill_gradientn(colours = basecols, 
  #                     values=breaks/limits[2],
  #                     limits=limits) +
  theme_void() + 
  theme(legend.position = "none")

p1b <- p1a %+% filter(my_map_data, variance_class=="highvar")
# Bivariate Map
# Make bivariate palette
# Function to devalue by a certain amount
colsupress <- function(basecols, hue=1, sat=1, val=1) {
    X <- diag(c(hue, sat, val)) %*% rgb2hsv(col2rgb(basecols))
    hsv(pmin(X[1,], 1), pmin(X[2,], 1), pmin(X[3,], 1))
}

# recurvisely decrease value
v_val = 0.5
bivariatepal <- c(basecols,
                  colsupress(basecols, sat=v_val),
                  colsupress(colsupress(basecols, sat=v_val), sat=v_val),
                  colsupress(colsupress(colsupress(basecols, sat=v_val), sat=v_val), sat=v_val))
# establish levels of palette
names(bivariatepal) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")


# Bivariate maps
p2a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry)) + 
  scale_fill_manual(values = bivariatepal) +
  theme_void() + 
  theme(legend.position = "none")
  
p2b <- p2a %+% filter(my_map_data, variance_class=="highvar")

# VSUP
# Function to combine colours for VSUP
colourblend <- function(basecols, p_length, nblend) {
    X <- rgb2hsv(col2rgb(unique(basecols)))
    v1 <- X[,seq(1,dim(X)[2], 2)]
    v2 <- X[,seq(2,dim(X)[2], 2)]
    if("matrix" %in% class(v1)){
      # hue issue wrap around pt 1
      v3 <- (v1+v2)
      v3["h",] <- ifelse(abs(v1["h",]-v2["h",])>0.5, v3["h",]+1, v3["h",])
      v3 <- v3/2
      # hue issue wrap around pt 2
      v3["h",] <- ifelse(v3["h",]>=1 , v3["h",]-1 ,v3["h",])
      hsv(rep(v3[1,], each=nblend), rep(v3[2,], each=nblend), rep(v3[3,], each=nblend))
      } else {
        v3 <- (v1+v2)
        v3["h"] <- ifelse(abs(v1["h"]-v2["h"])>0.5, v3["h"]+1, v3["h"])
        v3 <- v3/2
        v3["h"] <- ifelse(v3["h"]>=1 , v3["h"]-1 ,v3["h"])
        rep(hsv(h=v3[1], s=v3[2], v=v3[3]), p_length)
        }
}

VSUPfunc <- function(basecols, p_length, nblend){
  colourblend(colsupress(basecols, sat=0.5), p_length, nblend)
}

# VSUP
p = length(basecols)
VSUP <- c(basecols,
          VSUPfunc(basecols, p, 2),
          VSUPfunc(VSUPfunc(basecols, p, 2), p, 4),
          VSUPfunc(VSUPfunc(VSUPfunc(basecols, p, 2), p, 4), p, 8))

names(VSUP) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# VSUP maps
p3a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry)) + 
  scale_fill_manual(values = VSUP) +
  theme_void() + 
  theme(legend.position = "none")

p3b <- p3a %+% filter(my_map_data, variance_class=="highvar")


# Pixel Map
library(Vizumap)
# Low variance map
my_map_data_a <- my_map_data %>%
  filter(variance_class == "lowvar") %>%
  mutate(my_id = seq(n),
         error = variance) 

# quantile
q_a <- my_map_data_a |> 
  as.tibble() |>
  mutate(bitemp=as.numeric(bitemp)) |>
  with(data.frame(p0.05 = qnorm(0.05, mean=bitemp, sd=sqrt(variance)),
                  p0.25 = qnorm(0.25, mean=bitemp, sd=sqrt(variance)), 
                  p0.5 = qnorm(0.5, mean=bitemp, sd=sqrt(variance)), 
                  p0.75 = qnorm(0.75, mean=bitemp, sd=sqrt(variance)), 
                  p0.95 = qnorm(0.95, mean=bitemp, sd=sqrt(variance))))|>
  apply(2, function(x) ifelse(x>8, 8, x)) |>
  apply(2, function(x) ifelse(x<0, 0, x))


pixel_1a <- my_map_data_a %>%
  as.data.frame() %>%
  select(my_id, bitemp, error) %>%
  read.uv(estimate="bitemp", error="error")

pixel_2a <- my_map_data_a %>% as("Spatial")

pix_a <- pixelate(pixel_2a, pixelSize = 3, id = "my_id")


pmap_a <- build_pmap(data = pixel_1a, distribution = "discrete", pixelGeo = pix_a, id = "my_id", border = pixel_2a, q=q_a)

p4a <- view(pmap_a) +
  scale_fill_gradientn(colours = basecols) +
  scale_colour_gradientn(colours = basecols) +
  theme(legend.position="none")

# High variance
my_map_data_b <- my_map_data %>%
  filter(variance_class == "highvar") %>%
  mutate(my_id = seq(n),
         error = variance) 

q_b <- my_map_data_b |> 
  as.tibble() |>
  mutate(bitemp=as.numeric(bitemp)) |>
  with(data.frame(p0.05 = qnorm(0.05, mean=bitemp, sd=sqrt(variance)),
                  p0.25 = qnorm(0.25, mean=bitemp, sd=sqrt(variance)), 
                  p0.5 = qnorm(0.5, mean=bitemp, sd=sqrt(variance)), 
                  p0.75 = qnorm(0.75, mean=bitemp, sd=sqrt(variance)), 
                  p0.95 = qnorm(0.95, mean=bitemp, sd=sqrt(variance))))|>
  apply(2, function(x) ifelse(x>8, 8, x)) |> #this will mess with the variance
  apply(2, function(x) ifelse(x<0, 0, x))

pixel_1b <- my_map_data_b %>%
  as.data.frame() %>%
  select(my_id, bitemp, error) %>%
  read.uv(estimate="bitemp", error="error")

pixel_2b <- my_map_data_b %>% 
  as("Spatial")

pix_b <- pixelate(pixel_2b, pixelSize = 3, id = "my_id")

  
pmap_b <- build_pmap(data = pixel_1b, distribution = "discrete", pixelGeo = pix_b, id = "my_id", border = pixel_2b, q=q_b)

p4b <- view(pmap_b) +
  scale_fill_gradientn(colours = basecols) +
  scale_colour_gradientn(colours = basecols) +
  theme(legend.position="none")

  
# Display plots and palettes
p1a
p1b
show_col(basecols, ncol=8, labels=FALSE)
p2a
p2b
show_col(bivariatepal, ncol=8, labels=FALSE)
p3a
p3b
show_col(VSUP, ncol=8, labels=FALSE)
p4a
p4b
show_col(basecols, ncol=8, labels=FALSE)
```


These maps make the importance of combining uncertainty and signal in a single visual channel clear. A choropleth map will show signal that is not valid inference because of high uncertainty. At the other end of the spectrum, the bivariate map will show signal that is not always interesting because it forces us to interpret uncertainty and signal separately. As we move through these methods, it seems that the validity of any overarching insight becomes more visible at the cost of our ability to extract particular values of signal or noise. Therefore, given that the primary goal of visualisation *is* insights [@North2006], visualisation authors should err on the side of representing suppressed signal as a single variable, rather than visualising uncertainty separately using two different channels. 


# How do we measure uncertainty?
If uncertainty is inherently different, we would expect evaluation studies of uncertainty visualisations to produce different results to similar evaluation studies on "normal" variables. This does not seem to be the case.
Uncertainty is noise, but most evaluation experiments measure it as signal.
The previous section highlights a unique and fascinating problem faced by uncertainty visualisation. If asking direct questions about uncertainty causes us to treat it as a signal, how do we evaluate uncertainty as *noise*?
 When we ask the viewer of a plot to look at data and extract a value, we are asking them to perform inference on that value. There will be noise associated with that answer and that is uncertainty. If we ask direct question about some uncertainty metric, we have turned the uncertainty into signal because that is what the participants are drawing inference on.  

Uncertainty, acting as it is intended, is transparency. There are many secondary benefits that come with this improved transparency, such as better decisions, more trust in the results and more confidence in the authors. These secondary benefits are, however, *not* the immediate goal of uncertainty. The following sections will discuss the issues and limitations in measuring uncertainty through these secondary metrics and provide suggestions as to how future studies should consider measuring uncertainty.

## Measuring uncertainty as signal
Uncertainty visualisation papers can be organised according to the *goal* of the experiment. Evaluation experiments are the standard rule for visualisations because the human brain is not as reliable as mathematical calculation. Therefore, user studies often aim to assess the limitations, biases, and heuristics of our mental calculator so that we can better understand the problems we may encounter when we plot our data. This is not to say any paper that suggests a visualisation without an evaluation experiment is completely lacking in justification, and there are many papers that suggest a novel visualisation without an evaluation study. Sometimes these papers are a preliminary step in finding a solution for common problems and intend to evaluate the visualisation in later work. The reasoning for this is obvious. There are often heuristics and biases that are not obvious to us when designing visualisation. Additionally, these heuristics and biases can change depending on the larger scope of the graphic and the population we are communicating with [@Spiegelhalter2017; @Kinkeldey2014]. 

The overarching goal of most uncertainty visualisation evaluation papers belong to one of: performance (how effectively a participant can extract information from a plot), interpretation and semantics (the ease with which a particular visualisation is associated with uncertainty), and quality of user experience (if the participants liked the plot or not)[@Hullman2019]. @Hullman2019 found that the majority of papers evaluate visualisations on performance (approximately 65% of the papers they surveyed) or interpretation and semantics (approximately 17%) and both of these evaluation goals will run into problems because of their conceptualisation of uncertainty.

### Repeating perceptual task experiments
Given the previous section, one might consider problems in uncertainty visualisation to be fixed if we could restrict evaluation experiments to cases where the information in each graphic is almost identical. Unfortunately this still often results in uninteresting or obvious results.

If two graphics are visually different but identical in the information they contain, they must differ in how that information is depicted. 
Once we have a mathematical expression of uncertainty, the visualisation of that uncertainty is theoretically identical to the visualisation process of any other variable. For simple tasks such as value extraction, there is a hierarchy to perceptual tasks where extracting visual information in some forms is easier than others. The hierarchy was originally established 40 years ago by @Cleveland1984, below is an updated version summarised by @Vanderplas2020:  
  
1) Position along a common scale. 
2) Position along a non-aligned scale. 
3) Length, direction, angle, slope
4) Area
5) Volume, density, curvature
6) Shading, colour saturation, colour hue
7) Discriminable shape
8) Indiscriminable shape

This hierarchy is a good general rule, however it can change from person to person [@Davis2022] Additionally, there are other graphical rules to consider such as gestalt principles, broader methods of extraction, and attention principles [@Vanderplas2020]. These established visualisation concepts allow us to anticipate the ease with which certain pieces of information will be extracted from a plot. We can use these concepts to understand the computational complexity of a graphic. A bizarre feature of the uncertainty visualisation literature is that it does not work to build upon these existing principles or identify the ways in which uncertainty visualisations may diverge from these rules. These building block concepts of visualisation are seldom mentioned. 

It is difficult to find examples of uncertainty visualisation experiments where the plots do contain the same information, however when they do, the results align with existing information visualisation research. Technically, a PDF and a mean with confidence intervals both have enough information to extract the mean of the distribution, however they both have a very different computational cost. To extract the mean using a PDF, a participant would need to identify the point along the x axis that splits the area under the curve in half. If a participant is provided with a mean with a confidence interval, extracting the average is a simple task of reading the position on an aligned scale. @Ibrekk1987 found that when asking participants for the "best estimate" (which they thought should be interpreted as the mean of the distribution) of a skewed distribution, participants provided the mean when given a mean with confidence intervals and the mode when given a PDF [@Ibrekk1987]. Similar results to this occur over and over again in the uncertainty visualisation literature. @Gschwandtnei2016 found that visualisations where the start time of an interval could literally be read off the plot (error bars, centred error bars, and ambiguation) performed better than the plots (accumulated probability, gradient, and violin) where the start time involved some guesswork because the drop off was gradual. @Cheong2016 found that participants were better at answering questions when they were explicitly given the relevant probability in text rather than when they needed to read it off a map. (*Cite Gap 15: Examples of replicated perceptual task experiments*). 

These results show that uncertainty is not technically different to any other variable. When trying to anticipate the results of these studies, we can use the same principles of information equivalence and difficulty of relevant mental tasks to understand which plots will outperform others. This leads us to wonder... why is uncertainty visualisation a field at all?

### Performance
There are many metrics experimenters use to evaluate the performance of a visualisation. Each metric, such as can use accuracy, decision quality, confidence, trust and others each have their own issues which prevent them from accurately capturing the effects of uncertainty in a visualisation. 

By far the most common metric used is accuracy, which is used by approximately 36% of evaluation studies [@Hullman2019]. In previous sections, we pointed out issues that arise when accuracy (of extracting probability or variance) is used as the primary metric in evaluating uncertainty visualisations, the work simply replicates known concepts in information visualisation. Subjective metrics, such as how much participants liked the aesthetic representation of a visualisation, or metrics with an unclear motivation or use, such as how memorable the information in a plot is or how much a user interacted with a plot, are ignored in this section. Below we will explain in detail why experiments that try to measure uncertainty using methods such as decision making, trust, or other types of questions, also fail to capture the effects of visualising uncertainty in a plot.

#### Decision making and risk aversion

![Visualisation of the process of transforming raw data to a visualisation insight. Summarising the data using common statistics removes information from our data. Once the data is in a format that represents the varibales of interest, organising that information in a visualisation makes it easier to extract insights from these statistics and convert them to insights. If information is removed at the statistics stage, it cannot be added back in at the visualisation stage, as it is meerly an efficient technique to organise and present information to enable a large range of insights. ](visprocess.jpeg){#fig-location width=70%}

Decision making tasks are often described by authors as a more realistic version how plots are used in practice. While we do not disagree with this statement, exactly *why* these tasks are different to value extraction tasks is never explained. While the task may be more similar to how plots are actually used, that does not necessarily mean it is a better *experimental* environment for evaluating plots. Let us consider this in more detail. A normal value extraction task involves:

1) correctly interpret the question
2) extract the specified value
3) report the value

On the other hand, a decision making task using an uncertainty visualisation involves:

1) Correctly interpret the question
2) Use some risk utility function to set a threshold for an "acceptable level of risk"
3) Extract the value for from the plot that will be compared to the threshold
4) Make a judgement based off that value

The key aspect that separates a decision making task from a value extraction task, for uncertainty visualisations, is the inclusion of a utility function. While this might seem like a small change, it actually completely warps the experiment in unexpected. 

The first issue is that individuals will all have their own individual risk utility function, that is, how much they want to avoid or engage with risks or uncertainty. Therefore decision making experiments that have an additional layer of noise that value extraction experiments do not. We cannot be sure if participants answered differently from the ground truth because a visualisation was difficult to read *or* because the participants risk utility function did not align with the one set by the authors. Several authors have offered solutions to this issue, however the problem is deeper than any of them seem to realise.

@Hullman2016 suggested providing a utility framework for each experiment, to instruct the participants in how to account for the uncertainty information. This is a method that seems to have been adopted by @Fernandes2018 who describe the following scenario to participants who are trying to maximise the coins in their experiment:
> Subjects gain coins for every minute they are able to continue an activity that is valuable to them (e.g., watching TV at home) before going to the bus stop, and gain a bonus for arriving at their intended destination early. Subjects incur a coin penalty for time spent waiting at a stop for a bus to arrive.

Any payment scheme that is incorporated into an experiment will also implicitly set up a utility framework, as it is assumed participants will try to maximise their payments. In tasks such as this, the ground truth of each question is typically selected to be the value a ration agent would select and visualisations are evaluated on the basis of how far the participants responses are from that ground truth. The problem with this, is that all the complexity of the question is in *working out* the best response, and it has little to do with the visualisation itself. The visualisation aspect of these studies becomes a value extraction experiment. 

@Cheong2016 tested multiple different visual representations of uncertainty for representing the likelihood of a house being burned down based on its location. Their payment scheme, which paid out $0.10 for a correct choice (i.e. staying when the house was not burned down or leaving when the house was burned down) and 0 for an incorrect choice (i.e.leaving when the house didn't burn down or staying when the house burned down), meant participants were incentivised to base their entire leave/stay decision on whether or not the likelihood of a fire at their house is above or below 50%. If the participants *did* correctly identify the optimum strategy and answer accordingly, the "decision making experiment" was actually *just a value extraction experiment*. The decision making aspect ceased to matter, all that mattered was identifying if the probability of a fire was greater than 50%. Interestingly, despite this very obvious and simple tactic to maximize payout from the experiment, it seems like many participants did not adopt it, instead acting much more warily as though they were considering whether or not they would *actually* evacuate in the event of a fire. This means it is likely that introducing a payment scheme or a utility function creates too much mental labour for the participants to answer "correctly", and they would rather stick with their own risk utility function that feels *natural* than go through the mental labour of adopting another. Setting up an experiment that requires participants to do laborious calculations or strenuous mental effort for a small marginal benefit is unlikely to result in successful participant responses because it is not in the "spirit" of visualisation. (*Cite Gap 16: Examples of value extraction decision making tasks*)

It is clear that audiences primarily interact with visualisations through their "System 1" brain [@daniel2017thinking]. This aligns with other authors comments that visualisation is primarily about "gists" [Spiegelhalter2017]. In this sense, asking participants a question that requires them to shift to their "System 2" is entirely contrary to *why* we use visualisation. The exact reasons we describe visualisation to be so powerful, that they are able to communicate complicated information fast and efficiently *is a product of the system 1 brain* [@daniel2017thinking], however this means displaying information using visualisations have the same weakness' as system 1 thinking, and visualisation authors need to be aware of this. Asking participants to answer questions that require complicated calculations does not reflect how visualisations are used, and will likely be ignored by the participants. While we agree with @Hullman2016 who suggested that what determines an appropriate ground truth is largely a philosophical exercise, we differ in the sense that we do not think it should be done, lest you create a needlessly complicated value extraction task.

Additionally, several papers mention "risk-aversion" in participant decisions as a negative by-product of a particular visualisation design, however that displays a failure to understand that risk aversion is *not* a mistake, but rather it *reveals the utility of decreased uncertainty*. It may be interesting to investigate why a particular visualisation elicits increased risk-aversion, for example is it due to poorer estimates or increased awareness of negative outcomes, however simply reporting this as "risk aversion" conflates these factors. The work that advocates for transparency in risk-communication gives you whiplash when it's subtext argues that transparency leads to "incorrect" conclusions. There have been other discussions on the appropriateness of discussing risk-aversion as a bias [@Vranas2000], but few have made the point that if uncertainty holds some *value* there is not technically a "right" decision at all. 

The final issue with decision making tasks is that calculating "optimum" choices is not a task we use uncertainty for, it is a task that utilises *risk*. Risk and uncertainty are slightly different, as risk is known probabilities and uncertainty is unknown probabilities, so uncertainty is what you get when you cannot accurately define risk [@Spiegelhalter2017]. Communicating risk allows people to weigh up options and make an optimum decision, uncertainty hedges information to let people know there may not even be an optimum strategy. In this sense, many "uncertainty" decision making experiments are actually just risk decision making experiments. Not only this, but the inclusion of uncertainty information should *not* impact the choices of a rational agent. A well behaved rational agent *should* ignore uncertainty information, so long as the estimate provided is still the best case. This point seems to be lost on many uncertainty visualisation authors. @Zhao2023 displayed a model's prediction and its estimated uncertainty and asked participants if they wanted to submit the model estimate as their answer, or make their own prediction. At no point did the authors seem to realise a rational agent would accept the model estimate every time, no matter how uncertain the information was, and participants were actually disqualified from the study for taking this approach to answering the questions. (*Cite Gap 17: Examples of studies where the correct choice was to ignore the uncertainty information*)

This leaves us with several issues in decision making experiments. They are noisy because of participants individual utility functions and when that is not the case they are thinly veiled and over complicated value extraction experiments. Additionally, authors don't even seem to agree how participants should incorporate uncertainty information, with some authors designing complicated utility functions, and others not realising the best response to their experiment is to ignore uncertainty information entirely. For these reasons, uncertainty visualisation experiments should either be designed with these caveats in mind and be designed to untangle why a particular visualisation will elicit a specific decision, or the field should steer away using decision quality and related metrics as a basis for evaluating visualisation performance.

#### Trust and confidence
Authors also often measure the impact of visualising uncertainty on participants trust in an estimate. Trust is a by product of displaying uncertainty rather than the goal of it, and viewing the relationship in the converse direction can lead to misguided research.

If the purpose of displaying uncertainty information is to appropriately hedge a signal with noise, then it should be assumed that trust is only related to uncertainty communication through increased transparency and honesty. Considering trust, and not transparency, as the metric of importance in uncertainty communication can lead to a questionable subtext that argues against transparency, something that has been noticed by several other authors [@Spiegelhalter2017; ONeill2018]. @Hullman2020a found that author simultaneously argued that failing to visualise uncertainty was akin to fraud, but also many avoided uncertainty visualisation because they didn't want their work to come across as "untrustworthy". These authors are optimising *trust* rather than *transparency*, which means they opt to leave out uncertainty information when it does exactly what it is supposed to, decrease certainty in conclusions.

Science communication should be primarily concerned with accuracy. Setting trust and risk-aversion as the variables of interest implicitly encourages statisticians to set trust and risk-aversion as the primary goals of communication. The issue of trust being divorced from trustworthiness has been commented on by other authors [@ONeill2018], however the issue still persists in the uncertainty visualisation literature. @Zhao2023 displayed a several variations of a visualisation of a model prediction and its uncertainty and took participants using the model prediction as a sign of trust. They reported that visualising uncertainty information caused participants to trust the model in the low variance case, but the results in the high variance case were inconclusive. The discussion made it clear the authors thought the uncertainty information should make the visualisation more trustworthy, but conflating trust and the use of a prediction implied uncertainty information should somehow influence participants to use their own prediction, even though a prediction being uncertainty does not necessarily mean it is incorrect. Despite this, the authors seemed to assume that the uncertainty information *should* have an influence on that, showing they had not deeply considered *how* uncertainty information should influence the choices of the participants. (*Cite Gap 18: examples of studies where authors measure trust*)

A similar measure to trust is using "confidence" in an extracted value or a decision. Interestingly, "confidence" is also used to try and capture the clarity of a message in a normal visualisation. Confidence cannot simultaneously be a measure of clarity of visualisation *and* a way to capture the uncertainty expressed in a visualisation.

#### Other (questionable) attempts to capture uncertainty
There is also a swath of studies that are aware a question that boils down to a value extraction experiment, or a question that should be answered by literally ignoring uncertainty information is not what we want when we consider uncertainty visualisations. These papers often try to ask a question that should utilise both the uncertainty and signal in the response, however this is rarely what actually occurs. This method typically results in in cryptic or confusing questions that create a large amount of noise on the interpretation side of the analysis [@Hullman2016].  

Some authors opt for asking slightly vague questions that imply a use of uncertainty, but compare it to a ground truth that is very specific. @Ibrekk1987 asked participants for the "best estimate" which was evaluated in accuracy by comparing it to the mean, however the "best estimate" depends on the loss function we are using, and a loss function of minimised error was not implied by the question. @Hofmann2012 showed two distributions in 20 different visualisations (a line-up protocol) using a jittered sample, a density plot, a histogram, and a box plot and asked participants. Participants were asked to report in which of the plots was "the blue group furthest to the right" The experiment set up is shown in @fig-right. The participants answers were then compared to a ground truth where the correct plot had a blue distribution with a right shifted mean. By comparing the results to a ground truth statistic and marking participants as "wrong" or "right", the error from the participants that had an alternative interpretation to the concept of "furthest right" was conflated with the error from a the visualisation choice. These papers make it unclear if the participants got the answers wrong because they misunderstood the question or because of something related to the plot. Therefore, this method leads to inconclusive results about the plot design, and is not advised.

![This shows the user interface for the experiment performed by @Hofmann2012. The question of "furthest to the right" is open to interpretation.](furthestright.png){#fig-right width=50%} 

Another method used by authors is to ask a deterministic question about a random event. @Padilla2017 provided participants with a visualisation of the cone of uncertainty and asked then to "decide which oil rig will receive more damage based on the depicted forecast of the hurricane path". The cone of uncertainty provides a 60% confidence interval for the location of the eye of a hurricane, which allows us to know the area where the eye of the storm will go, it does not given any information about the intensity of a storm, the size of a storm, or even if a location will be hit. This inclusion of determinism seems to cause the authors to stumble themselves, as they are not consistent with their assumptions. In their first experiment @Padilla2017 indicated the correct answer was to assume that the storm was equally intense no matter how far from the centre of the distribution an oil rig was, however answering their third experiment correctly hinged on assuming the intensity of the storm at a particular point (which in this experiment they phrased as damage) *does* change in intensity as you move away from the centre of the distribution. Given these conflicting assumptions, it is unclear how the participants were supposed to adjust the probabilistic path information to answer a deterministic question about which oil rig would receive the most damage. Other authors have commented on the complexity of communicating hurricane risk because the path, storm surge and wind speed are all important and cannot be ignored [@Spiegelhalter2017]. The flip side of this is asking participants for a deterministic answer to a probabilistic question. @Correll2014 asked participants "how likely is candidate B to win the election?" when the two distributions indicated voter preference. Participants were not able to answer the question about likelihood in term of probability, but were instead given seven options from 1=Outcome will be most in favour of A to 7=Outcome will be most in favour of B. The ground truth statistic for this question was a scalar multiple of Cohen’s d, indicating participants were supposed to incorporate uncertainty information using a very specific formula that was likely unknown to them but assumed to be used implicitly.  

The final method used by authors is to just explicitly ask about uncertainty and signal information separately. @Sanyal2009 mapped uncertainty to dots and signal to a 3D surface and asked participants to identify areas of high and low signal and high and low uncertainty. Participants were not asked to combine that information in any way, and the signal and the noise were treated as separate variables. @Correll2014 asked participants to separately extract the mean and variance from four uncertainty visualisations. These methods explicitly view the uncertainty and signal as two separate variables that should be extracted from a plot, and not two variables that should be interpreted together. Even viewing these questions as a routine check to make sure the signal information isn't impacted by the uncertainty is counter intuitive, because the whole point *of* the uncertainty is to impact the signal information.

These examples are a bit complete mishmash of methods, however they point to a larger issue that goes beyond decision making, trust and confidence experiments. Authors *have no idea* how to evaluate the effects of uncertainty in an uncertainty visualisation.

## Suggestions to measure uncertainty
So, the current methods of measuring or understanding the role of uncertainty in a visualisation is questionable at best, however this is not because visualisation authors are missing the mark, but rather uncertainty is *particularly* difficult to express in a visualisation. In simple estimates or verbal communication, the signal is often easy to identify because it is what we are explicitly saying. Unlike statistical models, visualisations are used in both data exploration and communication. This means what exactly is a *signal* in any particular visualisation is hard to identify, since we often let the visualisation *tell us* what the signal is. Additionally, you cannot add noise to *every single possible* signal one might take from a visualisation. Two people looking at the same visualisation might, just by chance, develop two entirely different insights and draw inference on two completely different statistics. These unique and fascinating challenges that are faced by uncertainty visualisation have been completely untouched by the literature. This section will cover some interesting research in uncertainty visualisation and suggestions for better ways to measure uncertainty.

### More specific hypothesis
Heuristic checks are useful because they look at unknown pitfalls that might exist in interpretation of current plots. Since the hypothesis for these experiments are usually quite specific, e.g. "do people perceive a an outcome that is within the bar as more likely than one outside it, even if both outcomes are the same distance from the mean?". This means they are less likely to fall into the trap of trying to answer questions that are *far* too broad to be answered with a single experiment (e.g. "is a scatter plot better at showing uncertainty than a box plot"). This work also provides useful insights for experiments by highlights pitfalls participants might fall into when they review the results of evaluation experiments [@Hullman2016]. @Newman2012 found that participants were more likely to view points within the bar as more likely than points outside of the bar in bar charts with error bars. Similar effects have been identified in other types of uncertainty displayed. @Padilla2017 found that points that were on an outcome of an ensemble display were perceived as more likely than points not on an outcome, even when the point that was not on a specific outcome of the ensemble was closer to the mean of the uncertainty distribution. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015].

In a similar vein, experiments that verify smaller aspects of plot design might be more useful to the field in the long run because it helps contribute to a larger working theory of "how do we see visualisations". Many visualisation experiments try to compare two plots with several differences, but do not seem to be interested in the mechanisms by which we extract information from visualisations. Small perceptual tasks that seek to answer small but highly relevant questions (for example, if colour hue and colour value can be perceived as a single signal suppressed variable) would be useful to the field.

### Qualitative Studies
Alternatively visualisation research could shift away from the accuracy concept all together ask questions that allow for open ended responses. This method can enlighten authors as to *how* the uncertainty information was used by the participants. @Hofmann2012 tried to capture this by asking participants why they considered a particular plot to be more "right shifted", however this qualitative assessment does not seem to have made it into the final paper. @Daradkeh2015 presented participants with ten investment alternatives and asked participants "from among available alternatives, which alternative do you prefer the most", and were asked to think aloud and consider the uncertainty in their decision making. The experimenters goal was to observe and organise the methods people use when making decisions in the face of uncertainty. This study was an excellent example in a useful experimental design. They highlighted the specific aspects of uncertainty that participants typically considered, such as the range of outcomes that are above/below a certain threshold, minimum and maximum values, the risk of a loss, etc, and mapped where in the decision making process participants made these considerations. Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and pull out a large number of adjacent observations. This experimental framework could replicate this process. 

### Just noticeable signal
It could be argued that a well done uncertainty visualisations should have an imperceptible signal unless the signal would be identified with a hypothesis test, almost like a reverse line up protocol, but this idea also has some issues that should be considered. The reject or do not reject concepts in hypothesis testing do not offer a complete image of uncertainty, and exploration of uncertainty visualisation largely stems from a desire to move away from this binary framework. 

![The visualisation showing the kind of signal that we would expect an uncertainty plot to supress](signalsupression.jpeg){#fig-sigsupress} 

Additionally, setting up an uncertainty visualisation where the participants are expected to notice the signal once the data behind the visualisation passes a hypothesis test implies the signal *is* noticeable to a human at that level. @Patrick2023 compared people ability to recognise patterns in a residual plot to typical statistical tests and found human viewers looking at a plot were less sensitive than the typical residual tests. These experiments utilised the line-up protocol which has been suggested as a method to check if perceived patterns are real or merely the result of chance [@Buja2009; @Wickham2010; @Chowdhury]. This concept bears similarity to the goal of uncertainty visualisation, but it is not quite the same. @fig-hypvs shows the conceptual difference between the line-up protocol and uncertainty visualisation. 

![A line-up protocol displays the uncertainty about the null and identifies if the true data plot is identifiable (and therefore significantly difference). This can be considered the graphical equivalent of a standard hypothesis test. Uncertainty is frequently used to descibe the area around the estimate that we think might contain the true value, and we assesses if the null of "no signal" is within this plot (e.g. if the error bars overlap with zero). Uncertainty is technically the distance between the true value and the estimate. ](hypvs.jpeg){#fig-hypvs} 


# Great Examples 
Despite the common problems detailed in the previous sections, there is some interesting work in the uncertainty visualisation space. This can come in the form of an uncertainty visualisation that attempts to visualise a typically ignored aspect of uncertainty, or an experiment that avoids the pitfalls detailed in the previous sections.

The literature focus on "quantifiable" uncertainty leaves the variance that occurs at early stages in our analysis that is difficult to quantify ignored and forgotten. Some authors have chosen to focus on these unquantifiable cases and look at expressing more complicated cases of uncertainty.  @Tierney2023 builds upon the tidy data principles to allow users to handle missing values. This includes data plots with a missing value "shadow" that allows visualisation authors to identify if the variables used in a plot have any structure in their missing values, which would contribute to uncertainty. Another example of uncertainty that is often ignored is the uncertainty resulting from human choices. Climate scenario uncertainty, shown in @fig-climatescenario, attempts to display the range of climate change outcomes that can result from a range of best and worse case human choices. 

![Notice: I will make my own Eversion of a climate scenario plot, this one is just here from a random site as a place holder. A climate scenario plot shows the forecasts of different emission scenarios depending on human choices. The model visualisation wants to show the impact of human choice on the visualisation, rather than assume it away, so five different cases are shown in the model. Each of the case studies also depicts the statistical uncertainty of the model using a confidence interval. ](climatescenario.png){#fig-climatescenario}

Another visualisation method that has a lot of potential is visualisation of samples. Visualisations that opt to express a signal as a sample rather than an estimate have the potential to suppress signal since it is not explicitly visualised, however this has yet to be shown in evaluation experiments. This is not to say that visualisations of mass would not be able to perform signal suppression, but a sample can easily be expressed using aesthetics such as colour on a map and mass visualisation often struggling with issues such as over or under smoothing. These sampling methods can show more of the messiness of the data that sits behind a model. This may not have a detrimental effect on the viewers ability to extract global statistics, as it seems they can be extracted from a visualisation of a sample with ease [Franconeri2021]. Sample visualisations have been used in maps with the pixel map, shown in @fig-samples [Lucchesi2017], but is more commonly used in animation with the HOPs plots [@Hullman2015] or similar concepts [@Blenkinsop2000]. This method has also been adopted by visualisation authors outside of academia as can be seen in the  and the New York Times class mobility figure, shown in @fig-samples, an animated version of which can be found [here](https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html). 

::: {#fig-samples layout-ncol=2}

![Pixel map](pixelmap.png){#fig-pixelmap}

![Outcome plot](classmobility.png){#fig-nyt}

Notice: Will replace both plots with my own r visualisations. The pixel map depicts a map of the different poverty rates in local government areas in the US state of Missouri. The NYT visualisation shows an screencap of an animation that depicts the economic status of white and black boys who were raised in wealthy families. By displaying a psudo-sample rather than an estimate and a variance, the visualisation does some signal suppression as the true poverty rate is masked and can only be extracted by taking the global statistic of a sample. You can also depict the signal using explicit values, as the the NYT visualisation did, but it is important that the main visual features depict the uncertainty in the estimate.
:::

A final method to perform signal suppression is simply to visualise the data if it is available and relevant to the uncertainty distribution. An example of this is shown in @fig-census for racial distributions spatially in America, an interactive version of the plot can be found [here](https://edition.cnn.com/interactive/2021/us/census-race-ethnicity-map/). This map shows the typical causes of uncertainty in a spatial model, (e.g. regions where data is sparse, ethnically diverse areas, uneven distribution of points within boundaries, etc) but it avoids the need to create a visualisation with a specific signal in mind. This is the technique typically employed by exploratory data analysis, which means it's lack of a specified signal means there is both *no* uncertainty (since we are technically not performing inference) but in the event we *do* implicitly perform inference, there is some hedging. In this sense, the best uncertainty visualisation you can get without specifying a signal you want to convey is visualising the data itself. The census dot map's addition of interactivity also allows users to zoom in and see the details that caused "uncertainty" in the form of inconsistent colours at lower resolutions when they were zoomed out. This does not mean that visualising raw data instead of implementing sampling techniques is always a valid uncertainty visualisation that will prevent insignificant signal from getting through. @Buja2009 illustrated how groups that appear linearly separable in a linear discriminant analysis (LDA) visualisation of the data can actually be the result of a LDA performed on too many variables, something that was not clear from the visualisation until the line-up protocol was implemented. However it is simple but effective option that seems to be largely overlooked by the uncertainty visualisation community.

![Census dot map](censusdotmap.png){#fig-census}


# Future work
This paper has identified gaps in the uncertainty visualisation literature that could be filled to progress the field.

*Each new development should be accompanied by a mathematical definition of the uncertainty being addressed.* Ideally, a mathematically definition of uncertainty that allows us to combine these components would be developed, but in the absence of that, authors should be more specific about what aspect of "uncertainty" they are covering with their visualisation.

*The concept of uncertainty should be formalised within the grammar of graphics.* This formalisation would allow uncertainty visualisation authors to have a clear understanding of what is or is not an uncertainty visualisation. Additionally placing uncertainty visualisation in the framework that is used to understand existing information visualisation research would help authors understand when existing methods can be used to explain their results. incorporating uncertainty into the grammar of graphics will also give a more precise concept of the information contained within a plot. Other fields of science employ marginal changes when designing experiments to ensure it is well understood *what* aspect of their experiment is contributing to their results, and a better sense of what "marginal" is in the case of uncertainty visualisation would greatly help the field. (*XXX Is data pipeline connected with the grammar of graphics? Should this be a recommendation?*)

*Experimental practices on uncertainty visualisation need to be standardised.* If we are going to consider uncertainty as noise, not signal, there needs to be a way to identify this signal suppression in an experimental design. As the literature currently exists, there is no way to combine papers to get a meaningful sense of how uncertainty information is understood by a viewer. There is also the possibility that uncertainty visualisation evaluations will need to swap to a qualitative methodology where participants are allowed to freely comment on what they notice in graphics until we establish how the existence of noise can be observed.

If an uncertainty visualisation researcher would prefer to perform experiments rather than formalise methods, there are options there too. It would be interesting to know if any perceptual tasks that can be mapped to two different visual tasks condense into a single dimension when looking for overarching signal in a plot. Alternatively, the task dependency many authors in uncertainty visualisation mention would be a useful direction to consider. It is clear that the the number of potential tasks that can be performed on a visualisation increases with with the number of observations. A single observation is limited to value extraction, two observations can be compared, multiple observations allow for shapes or global statistics to be extracted. The interaction between sample size and task is of particular interest to the uncertainty visualisation community, as uncertainty can be expressed through multiple observations using a sample, or through a single value using an error. Of course, this is limited by the fact that there also isn't a definition for what is a "task" and given the mess created by the lack of formalisation in uncertainty visualisation, it may be wise to formalise that concept before performing these experiments. @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations, which could be a good starting point. Regardless, these are directions of research would be fruitful to the uncertainty visualisation community even if it appears on the surface to be research that is only beneficial to the "normal" visualisation community. (*XXX Not sure what this paragraph is recommending?*)

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "paper.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```