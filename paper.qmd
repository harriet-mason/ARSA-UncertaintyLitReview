---
title: "Noisy Work: A Review of The Uncertainty Visualisation Literature"
author: Harriet Mason
bibliography: references.bib
date: last-modified
toc: true
number-sections: true
format: pdf
editor_options: 
  chunk_output_type: console
---
# Current Todo List
This is a list of tasks that need to be completed before I consider the paper complete. Please keep this in mind when reading.

# Background

## Introduction
From entertainment choices to news articles to insurance plans, the modern citizen is so over run with information in every aspect of their life it can be overwhelming. In this overflow of information, tools that can effectively summarise information down into simple and clear ideas become more valuable. Information visualisations remain one of the most powerful tools for fast and reliable science communication. 

There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe's quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Anscombe quartet is shown in @fig-anscombe, because describing the data is never the same as seeing it. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about our data and **how** it might diverge from what we expect. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up Anscombe's quartet. The four scatter plots are visually distinct but have the same mean, standard deviation, and correlation. The visualisation highlights the importance of plotting your data to identify interesting features that are hidden by other summary statistics."
library(tidyverse)
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```

Uncertainty visualisation is a somewhat new subfield of visualisation, with early papers that specifically reference an "uncertainty visualisation" popping up in the late 90s and early 2000s (*Cite Gap 1*). Despite the youth of the field, commonly used uncertainty visualisations such as a box plot or histogram have been around for almost as long as statistical graphics themselves (*Cite Gap 2*). Early mentions of "uncertainty visualisation" appear in computer science papers discussing uncertainty visualisation as a subfield of geospatial information visualisation, specifically for the many uncertainties that come with satelite images (*Cite Gap 1*). In the modern day, uncertainty visualisation has exploded into its own field that applies to a wide range of contexts and is no longer bound by geospatial information (*Cite Gap 1*).

## Similar work

Any field that experiences an explosion of new research will also invite a series of literature reviews that seek to combine and summaries that information. Given that uncertainty visualisation has been around for a few decades now, there is already a wealth of uncertainty visualisation literature reviews. 

Interestingly, these reviews often do not offer overarching rules for tried and tested uncertainty visualisation, but rather comment on the *difficulties* faced when trying combine the papers from this field. @Kinkeldey2014 found most experiments the methods for uncertainty visualisation evaluation to be adhoc, with no commonly agreed upon methodology or formalisation and no greater goal of describing general principals. @Hullman2016 commented on the difficulty in taking overarching themes from uncertainty visualisation, as several conflated issues make it unclear if subjects did poorly in an experiment because they misunderstood a visualisation, because the question was misinterpreted, or because they used a heuristic. @Spiegelhalter2017 commented that different plots are good for different things, and disagreed with the goal of identifying a universal "best" plot for all people and circumstances. @Griethe2006 was unable to find common themes, but instead listed the findings and opinions of each uncertainty visualisation paper.

There are a handful of reasons that are used to explain why it is so difficult to generalise the findings in the uncertainty visualisation literature. Some papers suggested or agree with the idea that visualisation typologies should move away from data types, uncertainty categories, and representation types and towards "task-centered typologies" as this will help generalise results [@Kinkeldey2014; @Hullman2016]. Some authors may not attribute the task to the *reason* the literature cannot be easily summarised, but do mention that the choice of best visualisation is highly dependent on the specific goal of that visualisation [@Griethe2006; @Spiegelhalter2017]. Others comment that visualisations are highly dependent on the audience and there is no such thing as a "best" visualisation that will be accessible to all members [@Kinkeldey2014; @Spiegelhalter2017]. These difficulties in noisy uncertainty visualisation findings arises reguardless of the direction the analysis was approached, be it looking at geospatial data [@Kinkeldey2014], or uncertainty visualisation as a whole [@Hullman2016; @Spiegelhalter2017; @Griethe2006].

This review believe all these sources of noise have a single root cause, a lack of a cohesive and encompassing definition of uncertainty. The absence of an econompasing definition of uncertainty is mentioned by every uncertainty visualisation directly [@Spiegelhalter2017; @Griethe2006] or indirectly by describing a miriad of ways it can be considered in the literature [@Kinkeldey2014; @Hullman2016], although it is never commented on as a source of the noise in the field.

In understanding the definition problem, we will understand the *purpose* of visualising uncertainty, and understand if the current visualisation tools are sufficient to achieve that purpose and if not, where there is need for improvement. In understanding this purpose, we will also consider whether or not "uncertainty visualisation" is a valid sub-field of visualisation itself, or if that distinction has only arisen as a by product of the confused definition of uncertainty. Ultimately, this paper should serve as a guide for future uncertainty visualisation authors to prevent their work from becoming noise in what is already an incredibly noisy field.

# What is Uncertainty?
## Colloquial definitions
Uncertainty visualisation is made particularly difficult by the term "uncertainty" lacking a commonly accepted definition in the literature. This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. 

Definitions presented in papers that discuss uncertainty visualisation want it to be encompassing and include everything a layperson might consider when trying to consider "uncertainty". Some consider it to be synonymous with specific terms defined in mathematics, such as probability, variance, error, or precision. Others consider it to be an encompassing umbrella term of which for these mathematically defined objects are only examples, and include concepts that are somewhat related, such as missing values (*Cite Gap 3*). This wide array of uncertainty definitions becomes a snake eating its own tail, as many authors opt to redefine uncertainty themselves[], ignore defining uncertainty entirely [], or pick a definition from a seemingly randomly previously published papers [], all of which make variance in uncertainty definitions even worse (*Cite Gap 3.5*).

While these vague definitions might be OK for polite conversation, they are *not* foundation on which to build an entire sub-field of visualisation. In order to visualise uncertainty, it needs to be quantifiable [@Griethe2006; Leland2005], in order to quantify uncertainty, it needs to be mathematically defined. The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase, (e.g. @utypo defined it as any deviation from complete determinism) but go on to *quantify* uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation (*Cite Gap 4*). This definition swap happens so subtly that it seems to go unnoticed by authors writing the papers, and the fact that "uncertainty visualisation" methods dont even notice they are exclusively focus on depictions of easily quantifiable uncertainty, such as error [], mass [], or variance [] (*Cite Gap 5*). This has left the expressions of uncertainty that are hard to quantify or visually differentiate untouched, despite many papers calling for their invention (*Cite Gap 6*). There is likely only confusion and obfuscation if the vague definitions of "uncertainty" continue to be used and authors should try to be more specific with the mathematical object they are visualising, *or* a strict definition of uncertainty needs to be defined. The current method of titling papers and defining uncertainty forces readers to comb through the method of a particular visualisation experiment to understand if the uncertainty visualised is amenable to their needs. 

Some papers have recognised these colloquial definitions are lacking and have made attempts to formalise the space with taxonomies, however these taxonomies come with their own issues. 

## Taxonomy definitions
Much in the same way that there are almost as many definitions of uncertainty as there are papers on the topic, the field is also over run with taxonomies. Taxonomies split uncertainty based on an endless stream of ever changing boundaries, such as whether the uncertainty is due to true randomness or a lack of knowledge [@Spiegelhalter2017], if the uncertainty is in the the attribute, spatial elements, or temporal element of the data [], whether the uncertainty is scientific (e.g. error) or human (e.g. disagreement among parties) [], if the uncertainty is random or systematic [], statistical or bounded [], accuracy or precision [], if the uncertainty is about the past or the future [], the stage of the data analysis pipeline it comes from [], the severity of the uncertainty [], how quantifiable the uncertainty is [@Spiegelhalter2017], etc., etc., etc. (*Cite Gap 7*). In their own way, each of these taxonomies show an apsect of uncertainty that an author felt was important to differentiate. @utypo identified many common themes among this wide array of taxonomies and used them to design an encapsulating taxonomy, depicted in @fig-taxonomy. This definition organises uncertainty based on three axis, location (where the uncertianty is coming from), level (how quantifiable it is), and nature (if it comes from true randomness or incomplete knowledge). 

![*I need to fix the taxonomy illustration because inputs and model uncertainty need to be swapped*. Depicts an illustration of the taxonomy described in @utypo. From right to left the drawing shows the location, level and nature of uncertainty with examples of that category underneath. A specific source of uncertainty from the location can be mapped to a level of ignorance that can increase or decrease (i.e. moving up or down the green line) depending on the nature of the uncertainty. Identifying the location, level and nature of your uncertainty allows you to better understand it. ](taxonomyvis.jpeg){#fig-taxonomy}

When considering the use of taxonomies to define something inherrently vague, such as the uncertainty in an uncertainty visualisation, it is helpful to consider this quote from *The Grammar of Graphics*:

> "Taxonomies are useful to scientists when they lead to new theory or stimulate insights into a problem that previous theorizing might conceal. Classification for its own sake, however, is as unproductive in design as it is in science. [@Leland2005]

Therefore, while taxonomies can be helpful to map out the space of "things that we consider to be related to uncertainty", a taxonomy is *not* a definition and do little to help statisticians untangle and estimate the uncertainty in their projects. This is obvious in practice, as not understanding how to calculate uncertainty is one of the leading reasons cited by visualisation authors when explaining why they don't include it in their visualisation [@Hullman2020a]. That being said, these taxonomies can hint towards what is important when we think about uncertainty. The location axis of the @utypo taxonomy lines up neatly with the typical data analysis pipeline and multiple authors in the uncertainty literature have commented on the need to consider quantifying and expressing uncertainty at every stage of a project [@Kinkeldey2014; @Hullman2016; @Refsgaard2007] including stages as early as conceptualising the problem [@Otsuka2023] or collecting the data [@Meng2021]. The level axis from the @utypo taxonomy also hints towards an important consideration in uncertainty, because establishing how quantifiable uncertainty is informs us how it can be communicated, something that @Spiegelhalter2017 identified in the form of "precision". It is commonly noted that each source of uncertainty in an analysis must be discussed in isolation, but combining the uncertainty from every stage is near impossible [@Spiegelhalter2017] (*Cite Gap 8*). It is clear that elements of these taxonomies are identified in many other comments on uncertainty and represent *real* considerations that need to be made when defining uncertainty, even if that definition is yet to be published. 

The task of providing an encapsulating mathematical definition of uncertainty is far beyond the scope of this work, however we will discuss an important but commonly misunderstood feature of uncertainty; its relationship to inference.

## Defining uncertainty with respect to inference
What exactly is uncertainty, then? If we were to consider making this overarching definition, what would it need in order to be "encapsulating"? Well, let us consider what might *not* be considered uncertain in order to understand this concept a little better. 

@Otsuka2023 spends the first chapter of his book discussing the place of descriptive statistics in the philosophy of the field and in doing so, highlights an interesting connection between inference and uncertainty. Descriptive statistics simply describe our sample as it is and summarises large data down into an easy to swallow format. Descriptive statistics are not seen as the primary goal of modern statistics, however this was not always the case. Around the 19th century in England, *posivitism* was the popular philosophical approach to science (positivists included famous statisticians such as Francis Galton and Karl Pearson) and practitioners of the approach believed statistics ended with descriptive statistics as science must be based on actual experience and observations, therefore anything that refers to the unobservable (such as new observations or population statistics) is not true science [@Otsuka2023]. By its very nature, descriptive statistics *cannot* be used to make inferences about the data because it simply exists to summaries the data, to use it to make statements about *new* data is incorrect useage. In order to make statements about population statistics, future values, or new observations we need to perform inference, which requires the assumption of the "uniformity of nature" (i.e. that unobserved phenomena should be similar to observed phenomena) [@Otsuka2023]. This subtle shift, from descriptive statistics to inferential statistics was historically shunned *due to the fact it introduced the unknowable*, or in other words, uncertainty. 

This philosophical understanding of statistics highlights that descriptive statistics do not have uncertainty, however some readers may disagreee with that statement. Variance and probability are typically considered stand ins for "uncertainty", it is often how we choose to measure it, and since probability and variance exist in descriptive statistics, descriptive statistics *must* have uncertainty. This is not necessarily true, and related distinction was made by @Spiegelhalter2017  commented on a differentiation between precise random events (such as the probability of a coin flip), and uncertainty (such as the estimated probability associated with a coin that might be biased). A sample variance is not unknown, and therefore it is not uncertain, rather it is a precise description of dispersion. If we were to discuss drawing a new observation, or estimating the true mean of a population *then* the the variance would become relevant in our discussions of uncertainty, but in isolation it is not uncertain.

The idea that inference is related to uncertainty pops up in most discussions of uncertainty, however, due to the now *implicit* understanding we are performing inferential statistics, it is often brought up as a *task* or *goal* dependence. This is mentioned both by authors at a specific stage of an analysis, and by authors looking at an entire field or at all stages of the analysis pipeline. @Otsuka2023 suggested that the process of observing data to perform statistics is largely dependent on our goals, because the process of boiling real world entities down into probabilistic objects (or "probabilistic kind" as he puts it) depends on the relationship we seek to identify with our data. @Meng2014 commented what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the the question we seek to answer. @Carlin2023 mentions that each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to statistical analysis that is devoid of meaning. @Wallsten1997 argue that the best method for evaluating or combining subjective probabilities depends on the uncertainty the decision maker wants to represent and why it matters. @Munzner2009 created a nested model for visualisation that highlighted how the first mistake that can be made in a visualisation is in the problem characterisation, and failling to do it well can cause downstream effects and damage the effectivness of a visualisation. @Fischhoff2014 looks at uncertainty visualisation for decision making decides that we should have different ways of communicating uncertainty based off what the user is supposed to do with it. These examples show that authors that discuss uncertainty believe it is there is an important relationship between uncertainty and what we decide to keep in our analysis and throw away, i.e. the task or goal. The task or goal is, in essence, the *statistic you are drawing inference on*. Were this not true, we would have no reason to differentiate between a prediction interval and a sampling distribution because they could both be considered "uncertainty about the mean", however they are different because their inferential goals are different. 

These examples highlight two key concepts that seem to be true about the relationship between uncertainty and inference:    
1) Uncertainty is the by-product of inference, as we seek to draw conclusions about something unknowable.    
2) Uncertainty must be defined with respect to a *specific* statistic we wish to draw inference on, there is no such thing as a *general* uncertainty that is not linked to a motivating question.   

With this understanding it becomes clear to see why uncertainty is tied to an endless string of examples in the data analysis pipeline. Uncertainty examples include imputed data, model selection, inherent randomness, biased sampling, etc, not because these things *are* uncertainty, but because they *create* uncertainty *when we perform inference*. These things are not "uncertainty" in of themselves, but rather contribute to the distance between the final estimate and the statistic we want to perform inference on. The field of uncertainty communication is in desperate need of a unifying mathematical definition that firmly identifies exactly how each of these things contribute to uncertainty. This may be a considerable task, but as of right now, the list of things that are "uncertainty" continues to grow, and the best methods of understanding and quantifying them are ad-hoc at best. @Thomson2005 suggests a mathematical formula for *examples* of uncertainty, @Meng2014 mathematically defined the variance introduced to a model by the array of model choices, information theory tries to quantify uncertainty using the idea of entropy. None of these existing methods are thorough enough for an analyst to understand what causes uncertainty, and quantify it for communication. The need for a mathematical definition of this field is put simply by Freeman Dyson in his fameous Birds and Frogs speech:

> "Rigorous theorems are the best way to give a subject intellectual depth and precision. Until you can prove rigorous theorems, you do not fully understand the meaning of your concepts." @Dyson2010

In order to visualise something, it needs to be quantifiable. In order to quanitfy something, it needs to be mathematically defined. The broad classes of what is consdiered "uncertainty" is not currently quantifiable in any way that is not ad-hoc. The only apsects of uncertaitny that are currently quantifiable are confidence intervals, prediction intervals, and related terms. Even then, these quantifications often only capture the uncertainty in our model or its assumptions, it ignores bias often introduced in earlier stages of the analysis. Broader and more complicated concepts such as the effects of assumptions, imputed missing variables, and model choices remain difficult to meaningfully quantify beyond ad-hoc methods.

## Mistakes made when we misunderstand uncertainty
The previous section established that uncertainty is a by product of the assumptions we make when we perform inference, additionally, our quantification of uncertainty is largely task dependent, therefore the uncertainty associated with *one* distribution (which we can also view as a task or goal dependence). When uncertainty visualisaiton authors directly stare at this concept, they almost unanimously agree with these concepts, however when these details are pushed the the periphery of a problem, we start to get research that confuses itself. In these cases, the authors are more confused about what they are doing than the participants. 

There are enough papers that fail to understand these basic principals of uncertainty that this entire paper could simply be a list with explanations. Instead, we have a handful of highlights. 

@Hofman2020 performed an experiment where some participants were shown a sampling distribution of the mean, and others were shown a prediction interval around the mean, for the effectiveness of a particular treatment. The authors reported that the group that was shown the sampling distribution were willing to pay more and ascribed a higher likelihood to the belief that the treatment was more effective than a control, than the group that was shown a prediction interval. A prediction interval indicates the uncertainty associated with treatment *for a particular person* and the sampling interval shown the uncertainty associated with the *average effectivness of the treatment*. It is already established practice in medicine to use a wide range of metrics to communicate different risks to best communicate the relevant information to a patient for a particular decision [@Spiegelhalter2017]. The perceived importance of this work by @Hofman2020 hinges on a misunderstanding that the prediction and confidence interval *are* interchangeable despite performing inference on *very different* statistics. This misunderstanding is so widespread that this finding, that shows the confidence and prediction intervals are *not* interchangeable, is considered groundbreaking enough to be published in CHI.

@Boukhelifa2012 tried to quantify the strength of the intuitive connection between a line attribute called "sketchiness" and uncertainty. The authors of this study were aware that there was some kind of task dependence with respect to displaying uncertainty, so they tested the connection for multiple "tasks". The authors did not seem to understand the "task" dependence is likely a dependence on the inference for a particular statistic, so the "task" dependence was interpreted as *context* dependence. @fig-sketchy depicts the six scenarios that participants were shown and asked to interpret what they believe the squiggly line indicates. The authors misunderstanding led to "sketchiness" being used to depict uncertainty in the categories of bar charts, in graphic networks, and in train lines, with little consideration or explanation as to *how* these things would be uncertainty. It seems a tad ridiculous for the creators of a rail network map to be "uncertain" about the existence of a train line, however we are unable to guess at what the authors of the paper believed the uncertainty would be. The most likely scenario is that the authors did not consider the specific estimate that the uncertainty was representing, which is what led to visualisations that do not make conceptual sense. Participants rightfully assumed the sketchiness therefore represented something else, such alternative options or simply ignored it. 


![The graphics displayed by @Boukhelifa2012 to identify if there is an intuitive connection between sketchiness and uncertainty.](sketchiness.png){#fig-sketchy}

Many authors do not understand bias *is* uncertainty (something that has been proven mathematically by @Meng2014) and whether we view uncertainty as bias or variance depends on the resolution of our problem. @Padilla2021 found that high uncertainty in the model estimates (calculated uncertainty) and low forecaster confidence (which is typically an expression of suspected bias in the model) both caused participants to have decreased confidence in their results and suggested modelers express both if they are relevant. @Kale2019 discussed the importance of communicating decisions made in the data analysis pipeline and being aware of the alternatives. While there is nothing wrong with the results of these papers, the fact that they were published shows some level of surprise by these results. This work indicates that there was a significant number of authors in the field were not already aware that choices introduced early in the data analysis pipeline create bias and therefore uncertainty in our final values. The surprise that both bias and variance contribute to final uncertainty shows that authors do not understand both will contribute to the *distance* of your estimate to the value you are drawing inference on (AKA error). 

The misunderstanding that descriptive statistics also extends to visualisation. The exploration step of an analysis, which includes descriptive statistics, exploratory data visualisaiton, and unsupervised machine learning techniques, is performed without a prior hypothesis, however misunderstandings of this fact appears frequently in the literature. @Griethe2006 commented that "if visualization is used as a means to explore a data volume or to communicate its contents the uncertainty has to be included". @Potter2010 aimed to create a summary plot that "concisely presented data with uncertainty information" to create an exploratory visualisation tool that visualised uncertainty. Exploratory visualisations often differ from descriptive statistics because the explicit statistic we are drawing inference on is less explicit. A versatile visualisation such as a scatter plot allows for a viewer to consider several hypothesis at once, each of which may have its own associated uncertainty. @Hullman2021 argue that there is no such thing as a "model-free" visualisation, therefore visualisation require robust visualisations of uncertainty as we are always performing inference. While we agree with this sentiment, it is clear that uncertainty *cannot* be defined without a specific motivating question, and therefore trying to include uncertainty in an exploratory visualisation stage (which by definition does not have a hypothesis) is not possible. 

This confusion (or indifference) also creates many imprecisely named papers. Many papers will boast a title that claims to be about uncertainty visualisaiton, but simply depicts different visual representations of a PDF, however it takes reading the methodology to find this out (*Cite Gap 9*). While this is a minor by product of the definition issue discussed earlier, it does result in a literature that is needlessly difficult to navigate.

The reality is that a lot of uncertainty visualisation authors do not seem to have an intuitive understanding of uncertainties connection to inference, when inference is being performed, and how to design experiments that capture this relationship. This misunderstanding is not due to laziness or the fault of the authors, but rather is likely caused by the absence of a strict definition of uncertainty.The imprecise and confusing set of existing definitions are creating a field in which the authors themselves do not know what they are testing.

# What is Uncertainty Visualisation?
If we simply built the idea of uncertainty visualisation up from "uncertainty" which we already know has a precarious definition we would understand the field of "uncertainty" visualisation must also be precarious. You cannot quantify something that is not defined, and you can't visualise something you cannot quantify. If we cannot quantify the full concept uncertainty in any meaningful way (other than with examples such as missing values or a PDF) then there is no such thing as an "uncertainty" visualisation. Unfortunately this problem is not quite that simple.

## Establishing misunderstood concepts
Before discussing the issues facing uncertainty visualisation due to a poor concept *of* uncertainty visualisation, we want to establish two ideas that should be considered in conjunction with the lacking definition of uncertainty. The first is the sidestepping of the "uncertainty" definition problem by suggesting "uncertainty visualisation" is inherently different. Some authors may consider "uncertainty" to be poorly defined, but "uncertainty visualisation" to be well established. We will spend the next section explaining why that is not the case by discussing several taxonomies of uncertainty visualisation. The second is a conflation of the contextual information with the visual information. This problem is not, in isolation an issue, however it does seem to be a *key* contributor to the misunderstanding that uncertainty is an attribute of data, and not a by product of inference. Both of these issues need to be understood in order for us to fully understand the issues surrounding uncertainty visualisation.

### Definitions of "uncertainty visualisation"
A large drive of the field of uncertainty visualisation, has been the idea that "uncertainty visualisation" (defined as one word) is somehow different to "uncertainty" "visualisation" (separately defined as two words). When "uncertainty visualisation" is defined as a single word we lose some nuance in our understanding of the driving force behind some issues, and problems with visualisation and problems with uncertainty become conflated. This appears in the literature as a mountain of seemingly conflicting statements about what is, or is not, an "uncertainty visualisation". For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, however at least one or both of these charts are used in most "uncertainty visualisation" experiments [@Ibrekk1987] (*Cite Gap 10*). @Wickham2011 suggests their product plot framework, which includes histograms, should have a way to measure uncertainty, but does not consider that a histogram is *already* a depiction of mass and would already be considered an uncertainty visualisation were our statistic of interest the population mean. These conflicting ideas and lacking definition of what is means for a visualisation to be an "uncertainty visualisation" is a cornerstone of some clearly questionable results within this field.

Since the concept of "uncertainty visualisation" is harder to define than "uncertainty", "uncertainty visualisation" taxonomies are typically less thorough than taxonomies for "uncertainty". They also tend to blur the line between the mathematical elements of uncertainty and the visual depiction of those mathematical objects. While we will not focus on these taxonomies, it is still important to mention them and highlight what we can learn from their success' and failures..

Some taxonomies of uncertainty highlight how confusing most authors find the concept, and how difficult it is to articulate the rules of an uncertainty visualisation when uncertainty itself if not defined. @Potter2012 organised several existing uncertainty visualisations into groups based on the dimensionality of the data (1D, 2D, 3D, and No Dimension) and the dimensionality of the uncertainty (Scalar, Vector, Tensor).  However, because the term "PDF", a statistical object that describes a random variable that is typically a one dimensional function, is used to describe both the data and the uncertainty for all dimensions, it is hard to understand how this should work. @Grewal2021 created a taxonomy that mapped uncertainty visualisations to some point in a 2D space defined by the "domain expertise" and "continuum of discreteness" (that scaled from "point estimate" to "continuous distribution"). This conceptualisation could be considered a visual extension of the law of large numbers, because as a point estimate is a single sample, as the sample size gets larger the distribution will appear to be a continuous. However, assumes every point estimate is a sample from the same distribution (a mean is not technically a sample from a prediction interval, but it is a sample from the sampling distribution) and it conflates discreteness from a too small a number of observations, discreteness from a precision problem, discreteness because that is the true nature of the variable, and discreteness as a visualisation choice. @Griethe2006 organised uncertainty visualisations into two cases (1) a hypothesis test was preformed to confirm the validity of the visualisation and (2) the visualisation has uncertainty depicted. This conceptualization conflates hypothesis testing and confidence intervals, and while they are related concepts, a hypothesis test is not necessarily uncertainty. Of course the authors of these papers did not intend for the taxonomy to be a complete description of uncertainty visualisations, however these distinctions that are often ignored are subtly important. (*Cite Gap 10*)

There are a small handful of taxonomies that highlight some interesting features in the way uncertainty is visualised. @Kinkeldey2014 categorised uncertainty according to whether or not it was:
1. Implicit (a sample) or explicit (a depiction of mass)
2. Intrinsic (alter existing symbols symbols to represent uncertainty) or extrinsic (add new objects to represent uncertainty)
3. visually integral or separable (the uncertainty can be separated from the data and read independently) 
4. coincidence (uncertainty and data are represented in the same plot) or adjacent
5. static or dynamic (animated, interactive ect)
Of the groups they identified, they actually only used (4), (2), and (5), left out (1) because most visualisations are explicit, and (3) corresponds to (1) in most cases [@Kinkeldey2014]. This taxonomy suggests interesting considerations for a visualisation of a distribution (even though it was not intended to be a visualisation of a distribution). A similar version of this taxonomy was presented by @uncertchap2022 who commented that visualisation can be organised into two categories, "graphical annotations of distributional properties" and "visual encodings of uncertainty" which functionally aline with the intrinsic/extrinsic distinction. The first four categories boil down to the question "should uncertainty and signal be conveyed together as one variable or separately as two". This consideration will become more relevant later in this review. 

![This visualisation is here as a placeholder. I am going to replace it with an R coded visualisation that shows the 5 considerations for showing uncertainty in a plot according to @Kinkeldey2014 ](paperscreenshots/collage2.jpg){#fig-location}

Most of these taxonomies are created by observing existing uncertainty visualisations, which means they suffer from many of the same conceptual issues that the field has with uncertainty in general. Additionally, there does not seem to be anything these taxonomies offer that would not be better established by separate taxonomies for uncertainty and visualisation. The difference between "uncertainty visualisations" and "data visualisations" is not technically in the visual element, it is mathematical. @Kinkeldey2014 almost acknowledges this in their own paper that discusses an uncertainty visualsiation taxonomy when they claim "future typologies should take different categories of tasks into account (1) communication tasks (2) analytical tasks (3) exploratory task", a common typology for information visualisation in general. The process of understanding and estimating uncertainty requires knowledge of the data, the statistical methods used to make an estimate, and the assumptions of a model. Visualising the statistics that represent uncertainty should be no different than depicting the statistics that represent any other element of a graphic, this is something we will show in the following sections of this paper.

### Contextual Information
One of the key components of uncertainty and inference is the noise and signal dichotomy. When @Otsuka2023 discusses the information we boil away to create a "probabilistic kind" he uses the example of a coin flip and the sex of baby births as two things that are *very* different in reality, but it is only through the lense of a Bernoulli distribution that we can view these two things as the same. The fact is that this boiling down, of a real world event into data, is not something that happens independent of the statisticians, but is just as much as product of our interests (the particular relationship we hope to draw inference on) as it is a product of the event's natural properties. This is expressed simply in the text with an extension to the coin flipping example:

> "Recall that we modeled a coin toss by a Bernoulli distribution. That was because we supposed the coin will land either heads or tails and ignored the possibility of its landing on its edge. If we are to take the latter possibility into account, we should use a multinational distribution with three categories. As this example shows, the question of which statistical model we should use is not just determined by the physical nature of the object under study, but rather depends on the modeler’s interests and intentions." @Otsuka2023

With this in mind, let us turn to the concept of "contextual" information. For the purposes of this review, we will consider contexual information to be anything that makes it easy for the audience to connect the data from a probablistic kind to its real world parallel, but is largely irrelevant in the abstract probability space. A key example of this is spatial information, such as borders, road locations, terrains, etc. Spatial information is *highly relevant* when we translate from the abstract domain back into real world for communication. It can help provide reasons for *why* there may be specific characteristics in our data, however it is not required information for the model. @fig-spatial shows the proces of boilling information down into data, visualising it, and including contextual information to improve our scientific reasoning. The concept that contextual information is not the same as statistical information is also highlighted in *The Grammar of Graphics* which states:

> "Geography is anchored in real space-time and statistics in abstract dimensions. This is a distinction along a continuum rather than a sharp break... but this difference in focus clearly means that a system optimized to handle geography will not be graceful when dealing with statistical graphics." @Leland2005

![Data pipeline with visualisations to show why we are ignoring contextual information for the time being. The visualisations that depict the data are similar to typical statistical graphics. The purpose of the spatial information is to provide context to the information we extract from the statistical aspect of the graphic.](spatialinfo.jpeg){#fig-spatial}

For the remaining of this paper, we are not going to differentiate uncertainty visualisation using contextual information, despite the fact that it is common in the field. Our reasoning for this is that organising the field according to contextual information, promotes the idea that uncertainty is a concept specific to data qualities and ignores the role of inferential statistics.

While other discussions of uncertainty may consider this distinction more relevant to their work, here we will consider this contextual information. Contextual information is important for interpretation and understanding of graphics, but it does not in of itself generate uncertainty. Its relevance to the field of "uncertainty visualisation" is through the percieved lack of available channels. For example, a choropleth map has already used colour to represent an estimate, and position to represent spatial information, which is sometimes relevant to the probabilistic kind, and other times it is simply contextual information (such as when the modelling is done by using a state as a category rather than its geographic information). These two cases, when the spatial information is model relevant and when it is contextual, are rarely separated. It can lead authors to believe the spatial information is always of the highest importance, and thus it is always given the position axis, even when it is not, especially with respect to the uncertainty in a model. A confidence interval is a confidence interval whether it came from spatial data, temporal, or cross sectional data. 

Similar to the confusing definition of uncertainty, the constant conflation of statistical and contextual information also creates a wide range of problems. @Kinkeldey2014 discussed how uncertainty  can be represented by three components attribute (what) position (where) and temporal (when) and that studied typically deal with uncertainty around attribute but rarely position and time, however it is never specified what considerations should cause attribute uncertainty to be different to position or temporal uncertainty. @Kay2016 did an experiment that showed uncertainty around bus arrival times, however the visualisation used in the experiment, shown in @fig-bustime, is indistinguishable from most work that would be considered "attribute" uncertainty. 

![The uncertainty visualisation used by @Kay2016 to show the uncertainty around a bus arrival time prediction.](bustime.png){#fig-bustime}



This paper focuses on uncertainty, specifically uncertainty that is still in the the abstract *statistics* domain. Spatial and temporal uncertainty papers limit how they communicate their uncertainty because they are considering what can be visualised inside the context provided by maps and, not what they should *try* to visualise based on the uncertainty abstraction. When we view a spatial-temporal visualisation through the lense of trying to visualise the distribution of $P(X|longitude, latitude, time)$ there is no reason this is any different from any other high dimensional visualisation problem. 

In this sense we are not proposing that the current conceptualisation of data according to its physical properties is necessarily wrong, or that it leads to conclusions that are not necessarily specific to spatial or temporal data, and it may lead us to miss relevant information that is outside our domain. This is not only the case for uncertainty data. @Slingsby2023 discussed using a gridded glyphmap for spatial modelling, however the statistical elements are identical to a fluxuation diagram depicting $f(infection|time, age, longitude, latitude)$ presented in @Wickham2011. The plot by @Slingsby2023 is simply a special case of the graphic by @Wickham2011, where a separate visualisation is made for each time point, the continuous function $infection | age$ has replaced the categorical variable $happy$, a fade is added for the population size, and a map is shown in the background. @Slingsby2023 presents other maps in the paper, however they are also special cases of the product plots framework. This is not to say this extension is uninteresting, on the contrary we believe identifying cases where particular graphics are useful is a worthwhile endevour, but these plots seem to be have been developed unaware of the existing statistical framework. When we separate the field of visualisation along lines that may not be of particular relevance, we increase the likelihood of "reinventing the wheel" in each sub-dicipline.
 
```{r}
#| output: asis
#| echo: false
img_files <- fs::dir_ls("prodplots", glob="*.png")
cat("::: {layout-ncol=2}\n",
    glue::glue("![]({img_files})\n\n\n"),
    ":::",
    sep = ""
)
```

The focus on contextual information has also created a rift in exactly *why* uncertainty visualisation is a field of interest. The two most common justifications are:
1) Uncertainty is fundamentally differently to other variables due to psychological heuristics involved in the interpretation of uncertainty (e.g. risk aversion).
2) Uncertainty is not of interest in of itself and is typically layered on at the end of a the visualisation pipeline, so uncertainty visualisation is the field of adding error information into already established graphics and *thus* uncertainty information is a high dimensional visualisation problem.  
Papers will often discuss uncertainty in relation to one of these motivating reasons, the evaluation experiments motivated by (1) often perform different visualisations of PDFs, while the papers motivated by (2) will focus on trying to impute uncertainty as error within one of the existing channels, such as colour (*Cite Gap 11*). These dual motivations that co-exist with uncertainty visualisation authors rarely mentioning the others, create confusion in the field as to its purpose. @Kinkeldey2014 also identified this issue when they highlighted that the literature makes it unclear if uncertainty is a variable in of itself, something that should be interpreted with the main variable, or if it is metadata. 

For these reasons, the implication that uncertainty is not inference related, the barriers that cause authors to ignore existing visualisation methods, and the confusion created in the motivation of uncertainty visualization as a whole, we do not believe it is fruitful to continue separating the field of uncertainty visualisation according to its contextual information. Therefore, the rest of this review will not separate uncertainty visualisations based on this information.

## Mistakes made when we misunderstand uncertainty visualisation
In the previous section, we highlighted several problematic experimental designs or pieces of research that were the result of misunderstandings around the definition of uncertainty. Here, we will discuss the issues that arise due to a misunderstanding of "uncertainty visualisation". The two primary problems are the regular comparisons of mathematically incomparable plots, and reproducing results that are already widely understood in the larger information visualisation literature.

### Information asymetry in evaluated plots
Visualisation authors are almost unanimous in commenting that the "information" in two plots must be the same in order for the visual techniques to be compared [@Cleveland1984; @Kinkeldey2014] (*Cite gap 12*). However, what makes two visualisations equal in "information" is not consistent and is regularly mentioned without it being clear exactly what this information is. This is easily illustrated by the wide array of comments made by visualisation authors when they explain why two representations are being compared in their evaluation study. @Ibrekk1987 displayed 6 PDFs in their experiment because "formally equivalent representations are often not psychologically equivalent", but do not explain why what makes a representation "formally equivalent" or why only one representation was presented for the 95% CI, box plot, and CDF. @Hofman2020 comments that "theoretically" the sampling distribution of the mean and the prediction interval of a new observation are equal "so long as one knows the sample size", but does not recognise the several assumptions and statistical knowledge that is required to compare the two. *Other examples* (*Cite Gap 13*). @Kinkeldey2014 adopts an existing definition also that suggests two graphics are informationally equivalent if all the information in one plot is inferable from the other and vice-versa, but adds that two plots are computationally equivalent if that information can be extracted from both plots with similar easy and speed. It is clear this lacking definition of "information" in a visual is reminicent of our problems with the definition of "uncertainty". 

Unlike uncertainty, the concept of the "information" in a plot is not entirely avoided and works such as *The Grammar of Graphics* attempt to outline exactly what information is contained within a plot. When creating a graphic there are several tasks that must be completed in a specific order, regardless of whether or not we are working with a uncertainty visualisation or not, this pipeline is illustrated in @fig-pipeline, and contains all the steps of the grammar of graphics [@Leland2005]. First the mathematical information in a plot is established in the "varset" setps, and then that mathematical information is visually represented, which is defined in the "graph" steps. Since two plots cannot be compared on their visual elements if they contain different information (because you will never be able to identify if the difference is because of informational asymetry or visual processing) we could understand two plots as being the same if their "varset" steps are identical. However because uncertainty is not defined mathematically, we cannot reliably add it in at the "statistic" stage. Additionally, the grammar of graphics was not designed to extend to "uncertainty visualisations" as @Leland2005 referred to uncertainty as "semantics", so concepts that are common to uncertainty mathematically, such as resampling do not quite fit within the framework. For example, if we have one set of data and two graphics, where one graphic represents the data as a series of points, and the the other depicts a smoothed density using a line. These two plots differ in *both* the statistic and geometry stages of the grammar of graphics, however as n (the number of observations in our sample) gets larger, these two plots will become visually indistinguishable (depending on the collision modifier used). This result aligns nicely with large sample theory, because as n increases the data and mass should become mathematically indistinguishable, and many uncertainty visualisation experiments notice an interaction between sample size and graphic effectiveness so this result is likely also practically true [@Kale2018; @Newburger2022; @Hofmann2012] but despite the increasing similarity in what is actually depicted in the graphic, the sample and mass visualisations would remain different visualisations within the grammar of graphics framework. Additionally, authors rarely attribute this visual version of large sample theory to results where it is clearly applicable. @Kale2018 found that users were more sensitive to the underlying trend when shown the HOPs plot over the static outcoms plot, this effect went away when the speed of the animation increased, indicating that performance difference might be due to overplotting in the static case and a smaller sample size in the static condition might produce the same result. The issue of blurred elements of the grammar of graphics in the case of distribution visualisations does not end here. Visualisations that depict mass such as a confidence interval, a boxplot, a PDF, a letterbox plot, a violin plot, a histogram, a quantile dot plot, etc, all show some version of the mass of a variable at different resolutions, but whether these graphics differ in something as low down in the grammar pipeline as the variable stage, or later at the geometry stage is unclear. 

![The grammar of graphics data analysis pipeline from @Leland2005. the "varset" stages are the mathematical components of the graphic, the mathematical components define the "information" in the graph, while the "graph" stages are the visual elements that determine the "computational" cost of the graph.](grammarofgraphicspipeline.png){#fig-pipeline}
 
Alternative information consideration within well defined frameworks may also be worthwhile to consider. @wu2023rational tried to eliminate information asymetry by esentially looking at a visualisation and identifying what information can be extracted and then using a "rational agent benchmark" to determine how a rational person would use that information for a decision making task. While this solution is interesting, it may have different results depending on who is applying it as they may use different methods or notice different elements when extracting information from a graphic. The mathematically defined concept of sufficient statistics is also scarcely mentioned likely because it is specific to a particular ground truth statistic, however uncertainty *is* specific to a particular statistic, so it could be a useful avenue for understanding when two plots differ in their ability to answer a particular question.

Ultimately, this all points to confusion as to when two visualisation contain information asymmetry, a problem that becomes more pronounced in uncertainty visualisation. @Ibrekk1987 compared a 6 visualisations of a PDF,  a box plot, a CDF and a mean with a confidence interval on the basis that all of them were "uncertainty visualisations". They found that people are better at extracting the mean from a plot when they are shown a plot that contains a mean with a confidence interval than when they are shown a PDF, a box plot, or a CDF (where the mean could not be read off the plot). @Hofman2020 and @Zhang2022 compared prediction and sampling distributions because they are both "uncertainty" that is typically depicted around the mean. They found that people are better at answering questions about a prediction interval when shown a prediction interval instead of a sampling distribution. @Hullman2015 compared the static error bars and violin plots of the marginal distributions of two variables ($A$ and $B$) to an animated plot that depicted a single outcome of the joint distribution of $A$ and $B$ in each frame of an animation, a comparison that was justified because all plots were "uncertainty visualisations". They found that the plot that depicted outcomes from $P(A, B)$ where the distance between the two outcomes was equivalent to $a-b$ was better at answering the question "What is $P(B<A)$" than violin or error bar plots depicting $P(A)$ and $P(B)$ separately. *Other examples* (*Cite Gap 14*).

This collection of examples starts to paint a pretty clear picture. Visualisations with rather shocking information asymetry are regularly compared because they are both "uncertainty visualisations". This results in a series of experiments where the visual aspects of the graphic are not even required to anticipate the experimental results. This issue likely results in the lacking mathematical definition preventing us from defining and quantifying "uncertainty" mathematically and then including this quantification in a well defined framework such as the grammar of graphics.

### Repeating perceptual task experiments
Given the previous section, one might consider problems in uncertainty visualisation to be fixed if we could restrict evaluation experiments to cases where the information in each graphic is almost identical. Unfortunately this still often results in uninteresting or obvious results.

If two graphics are visually different but identical in the information they contain, they must differ in how that information is depicted. 
Once we have a mathematical expression of uncertainty, the visualisation of that uncertainty is theoretically identical to the visualisation process of any other variable. For simple tasks such as value extraction, there is a hierarchy to perceptual tasks where extracting visual information in some forms is easier than others. The hierarchy was originally established 40 years ago by @Cleveland1984, below is an updated version summarised by @Vanderplas2020:  
  
1) Position along a common scale. 
2) Position along a non-aligned scale. 
3) Length, direction, angle, slope
4) Area
5) Volume, density, curvature
6) Shading, color saturation, color hue
7) Discriminable shape
8) Indiscriminable shape

This hierarchy is a good general rule, however it can change from person to person [@Davis2022] Additionally, there are other graphical rules to consider such as guestalt heuristics, broader methods of extraction, and attention principles [@Vanderplas2020]. These established visualisation concepts allow us to anticipate the ease with which certain pieces of information will be extracted from a plot. We can use these concepts to understand the computational complexity of a graphic. A bizarre feature of the uncertainty visualisation literature is that it does not work to build upon these existing principles or identify the ways in which uncertainty visualisations may diverge from these rules. These building block concepts of visualisation are seldom mentioned. 

It is difficult to find examples of uncertainty visualisation experiments where the plots do contain the same information, however when they do, the results align with existing information visualisation research. Technically, a PDF and a mean with confidence intervals both have enough information to extract the mean of the distribution, however they both have a very different computational cost. To extract the mean using a PDF, a participant would need to identify the point along the x axis that splits the area under the curve in half. If a participant is provided with a mean with a confidence interval, extracting the average is a simple task of reading the position on an aligned scale. @Ibrekk1987 found that when asking participants for the "best estimate" (which they thought should be interpreted as the mean of the distribution) of a skewed distribution, participants provided the mean when given a mean with confidence intervals and the mode when given a PDF [@Ibrekk1987]. Similar results to this occur over and over again in the uncertainty visualisation literature. @Gschwandtnei2016 found that visualisations where the start time of an interval could literally be read off the plot (error bars, centered error bars, and ambiguation) performed better than the plots (accumulated probability, gradient, and violin) where the start time involved some guesswork because the drop off was gradual. @Cheong2016 found that participants were better at answering questions when they were explicitly given the relevant probability in text rather than when they needed to read it off a map. *Other examples* (*Cite Gap 15*). 

These results show that uncertainty is not technically different to any other variable. When trying to anticipate the results of these studies, we can use the same principles of information equivalence and difficulty of relevant mental tasks to understand which plots will outperform others. This leads us to wonder... why is uncertainty visualisation a field at all?

It is clear that authors intuitively *feel* that uncertainty is different even though this difference has not been captured in existing uncertainty visualisation experiments. Some authors cited uncetainty was special because of its psychological qualities, but we have yet to see any evidence of that here. @Kinkeldey2014 also noticed that it is not clear whether or not uncertainty is just another variable, as many value extraction papers treats it as such in their experimental design and uncertainty may need to be in a variable class of its own. @Hullman2016 commented that it is straightforward to show a value but it is much more complex to show uncertainty, but did not explain *why* uncertainty information faces more difficulty. After all, confidence intervals, minimums, maxmimums, variance, and any other statistic that can be considered related to "uncertainty" is just that, a statisitc. If we want to visualise the relationship between a temperature estimate and our forcasted variance on that esitmate, can we not just use a scatter plot, as we would any other pair of variables? There is nothing special about these statistics that should differentiate them from a mean or median and imply their visualisation should be "special" in a way that would warrant its own field. Especially when the core take aways from uncertainty evaluation experiments do not seem to differ at all from the established results in information visualisation. So, what is it that makes authors believe uncertainty is different?

# Uncertainty is noise, not signal
Uncertainty is noise, but most evaluation experiments measure it as signal. Uncertainty acts as a form of statistical "hedging" for signals found in an analysis. Interviews with experts in statistic back up this primary motivation, as ignoring uncertainty information of often expressed as being similar to fraud or lying [@Hullman2020a; @Manski2020]. Noise and signal are not an inherrent property of variance, or meta-data or the aspects of an analysis that are usually secondary considerations, it is a property of the data that *promotes* a message we are trying to infer and the data that *supresses* it, if that message is invalid.  When we ask the viewer of a plot to look at data and extract a value, we are asking them to perform inference on that value. There will be noise associated with that answer and that is uncertainty. If we ask direct question about some uncertainty metric, we have turned the uncertainty into signal because that is what the participants are drawing inference on.  

This is likely the cause of the contradiction between uncertainty visualisation papers following identical results to normal information visualisation papers, and authors believing uncertainty is inherently *different* to normal visualisation. Each of the sections in this paper likely contribute in some way to this contradiction in conceptualisation of uncertainty visualisaiton and the results of the studies in the field. Uncertainty is poorly defined, therefore authors do not understand it is not an inherrent property of the data and is actually related to a particular inference question, this incorrect view that uncertainty is always variance or probabilities leads to the class of "uncertainty visualisations" that depict different mathematical objects or visual features with little in the sense of consistent rules, this complicated class has caused visualisation authors to miss that they are simply repeating the established evaluation results from normal information visualisations. This situation is a bit of a mess. It also highlights a unique and facinating problem faced by uncertainty visualisation. If asking direct questions about uncertainty causes us to treat it as a signal, how do we evaluate uncertainty as *noise*?

Uncertainty, acting as it is intended, is fundamentally transparency to the viewer about a statistical analysis. There are many secondary benefits that come with this improved transparency, such as better decisions, more trust in the results and more confidence in the authors. These secondary benefits are, however, *not* uncertainty. The following sections will discuss the issues and limitations in measuring uncertainty through these secondary metrics and provide suggestions as to how future studies should consider measuring uncertainty.

## Issues with the current methods of measuring uncertainty
Uncertainty visualisation papers can be organised according to the *goal* of the experiment. Evaluation experiments are the standard rule for visualisations because the human brain is not as reliable as mathematical calculation. Therefore, user studies often aim to assess the limitations, biases, and heuristics of our mental calculator so that we can better understand the problems we may encounter when we plot our data. This is not to say any paper that suggests a visualisation without an evaluation experiment is completely lacking in justification, and there are many papers that suggest a novel visualisation without an evaluation study. Sometimes these papers are a preliminary step in finding a solution for common problems and intend to evaluate the visualsiation in later work. The reasoning for this is obvious. There are often heuristics and biases that are not obvious to us when designing visualisation. Additionally, these heuristics and baises can change depending on the larger scope of the graphic and the population we are communicating with [@Spiegelhalter2017; @Kinkeldey2014]. 

The overarching goal of most uncertainty visualisation evaluation papers belong to one of: performance (how effectively a participant can extract information from a plot), interpretation and semantics (the ease with which a particular visualisation is associated with uncertainty), and quality of user experience (if the participants liked the plot or not)[@Hullman2019]. @Hullman2019 found that the majority of papers evaluate visualisations on performance (approximately 65% of the papers they surveyed) or interpretation and semantics (approximately 17%) and both of these evaluation goals will run into problems because of their conceptualisation of uncertainty.

### Performance
There are many metrics experimenters use to evaluate the performance of a visualisation. Each metric, such as can use accuracy, decision quality, confidence, trust and others each have their own issues which prevent them from accurately capturing the effects of uncertainty in a visualisation. 

By far the most common metric used is accuracy, which is used by approximately 36% of evaluation studies [@Hullman2019]. In previous sections, we pointed out issues that arise when accuracy (of extracting probability or variance) is used as the primary metric in evaluating uncertainty visualisations, the work simply replicates known concepts in information visualisation. Subjective metrics, such as how much participants liked the aesthetic representation of a visualisation, or metrics with an unclear motivation or use, such as how memorable the information in a plot is or how much a user interacted with a plot, are ignored in this section. Below we will explain in detail why experiments that try to measure uncertainty using methods such as decision making, trust, or other types of questions, also fail to capture the effects of visualising uncertainty in a plot.

#### Decision making and risk aversion
Decision making tasks are often described by authors as a more realistic version how plots are used in practice. While we do not disagree with this statement, exactly *why* these tasks are different to value extraction tasks is never explained. While the task may be more similar to how plots are actually used, that does not necessarily mean it is a better *experimental* environement for evaluating plots. Let us consider this in more detail. A normal value extraction task involves:

1) correctly interpret the question
2) extract the specified value
3) report the value

On the other hand, a decision making task using an uncertainty visualisation involves:

1) Correctly interpret the question
2) Use some risk utility function to set a threshold for an "acceptable level of risk"
3) Extract the value for from the plot that will be compared to the threshold
4) Make a judgement based off that value

The key aspect that separates a decision making task from a value extraction task, for uncertainty visualisations, is the inclusion of a utility function. While this might seem like a small change, it actually completely warps the experiment in unexpected. 

The first issue is that individuals will all have their own individual risk utility function, that is, how much they want to avoid or engage with risks or uncertainty. Therefore decision making experiments that have an additional layer of noise that value extraction experiments do not. We cannot be sure if particpants answered differently from the ground truth because a visualisation was difficult to read *or* because the participants risk utility function did not align with the one set by the authors. Several authors have offered solutions to this issue, however the problem is deeper than any of them seem to realise.

@Hullman2016 suggested providing a utility framework for each experiment, to instruct the participants in how to account for the uncertainty information. This is a method that seems to have been adopted by @Fernandes2018 who describe the following scenario to participants who are trying to maximise the coins in their experiment:
> Subjects gain coins for every minute they are able to continue an activity that is valuable to them (e.g., watching TV at home) before going to the bus stop, and gain a bonus for arriving at their intended destination early. Subjects incur a coin penalty for time spent waiting at a stop for a bus to arrive.

Any payment scheme that is incorporated into an experiment will also implicitly set up a utility framework, as it is assumed participants will try to maximise their payments. In tasks such as this, the ground truth of each question is typically selected to be the value a ration agent would select and visualisations are evaluated on the basis of how far the participants repsponses are from that ground truth. The problem with this, is that all the complexity of the question is in *working out* the best response, and it has little to do with the visualisation itself. The visualisation aspect of these studies becomes a value extraction experiment. 

@Cheong2016 tested multiple different visual representations of uncertainty for representing the likelihood of a house being burned down based on its location. Their payment scheme, which paid out $0.10 for a correct choice (i.e. staying when the house was not burned down or leaving when the house was burned down) and 0 for an incorrect choice (i.e.leaving when the house didnt burn down or staying when the house burned down), meant participants were incentivised to base their entire leave/stay decision on whether or not the likelihood of a fire at their house is above or below 50%. If the participants *did* correctly identify the optimum strategy and answer accordingly, the "decision making experiment" was actually *just a value extraction experiment*. The decision making aspect ceased to matter, all that mattered was identifying if the probability of a fire was greater than 50%. Interestingly, despite this very obvious and simple tactic to maximize payout from the experiment, it seems like many participants did not adopt it, instead acting much more warily as though they were considering whether or not they would *actually* evacuate in the event of a fire. This means it is likely that introducing a payment scheme or a utility function creates too much mental labour for the participants to answer "correctly", and they would rather stick with their own risk utility function that feels *natural* than go through the mental labour of adopting another. Setting up an experiment that requires participants to do laborious calculations or strenuous mental effort for a small marginal benefit is unlikely to result in successful participant responses because it is not in the "spirit" of visualisation. (*Cite Gap 16*)

It is clear that audiences primarily interact with visualisations through their "System 1" brain [@daniel2017thinking]. This aligns with other authors comments that visualisation is primarily about "gists" [Spiegelhalter2017]. In this sense, asking participants a question that requires them to shift to their "System 2" is entirely contrary to *why* we use visualisation. The exact reasons we describe visualisation to be so powerful, that they are able to communicate complicated information fast and efficiently *is a product of the system 1 brain* [@daniel2017thinking], however this means displaying information using visualisations have the same weakness' as system 1 thinking, and visualisation authors need to be aware of this. Asking participants to answer questions that require complicated calculations does not reflect how visualisations are used, and will liekly be ignored by the participants. While we agree with @Hullman2016 who suggested that what determines an approripate ground truth is largely a philosophical exercise, we differ in the sense that we do not think it should be done, lest you create a needlessly complicated value extraction task.

Additionally, several papers mention "risk-aversion" in participant decisions as a negative by-product of a particular visualisation design, however that displays a failure to understand that risk aversion is *not* a mistake, but rather it *reveals the utility of decreased uncertainty*. It may be interesting to investigate why a particular visualisation elicits increased risk-aversion, for example is it due to poorer estimates or increased awareness of negative outcomes, however simply reporting this as "risk aversion" conflates these factors. The work that advocates for transparency in risk-communication gives you whiplash when it's subtext argues that transparency leads to "incorrect" conclusions. There have been other discussions on the appropriateness of discussing risk-aversion as a bias [@Vranas2000], but few have made the point that if uncertainty holds some *value* there is not technically a "right" decision at all. 

The final issue with decision making tasks is that calculating "optimum" choices is not a task we use uncertainty for, it is a task that utilises *risk*. Risk and uncertainty are slightly different, as risk is known probabilities and uncertainty is unknown probabilities, so uncertainty is what you get when you cannot accurately define risk [@Spiegelhalter2017]. Communicating risk allows people to weigh up options and make an optimum decision, uncertainty hedges information to let people know there may not even be an optimum strategy. In this sense, many "uncertainty" decision making experiments are actually just risk decision making experiments. Not only this, but the inclusion of uncertainty information should *not* impact the choices of a rational agent. A well behaved rational angent *should* ignore uncertainty information, so long as the estimate provided is still the best case. This point seems to be lost on many uncertainty visualisation authors. @Zhao2023 displayed a model's prediction and its estimated uncertainty and asked participants if they wanted to submit the model estimate as their answer, or make their own prediction. At no point did the authors seem to realise a rational agent would accept the model estimate every time, no matter how uncertain the information was, and participants were actually disqualified from the study for taking this approach to answering the questions. (*Cite Gap 17*)

This leaves us with several issues in decision making experiments. They are noisy because of participants individual utility functions and when that is not the case they are thinly veiled and over complicated value extraction experiments. Additionally, authors dont even seem to agree how participants should incoreperate uncertainty information, with some authors designing complicated utility functions, and others not realising the best response to their experiment is to ignore uncertainty information entirely. For these reasons, uncertainty visualisation experiments should either be designed with these caveates in mind and be designed to untangle why a particular visualisation will elicit a specific decision, or the field should steer away using decision quality and related metrics as a basis for evaluating visualisation performance.

#### Trust and confidence
Authors also often measure the impact of visualising uncertainty on participants trust in an estimate. Trust is a by product of displaying uncertainty rather than the goal of it, and viewing the relationship in the converse direction can lead to misguided research.

If the purpose of displaying uncertainty information is to appropriately hedge a signal with noise, then it should be assumed that trust is only related to uncertainty communication through increased transparency and honesty. Considering trust, and not transparency, as the metric of importance in uncertainty communication can lead to a questionable subtext that argues against transparency, something that has been noticed by several other authors [@Spiegelhalter2017; ONeill2018]. @Hullman2020a found that author simultaneously argued that failing to visualise uncertainty was akin to fraud, but also many avoided uncertainty visualisation because they didn't want their work to come across as "untrustworthy". These authors are optimising *trust* rather than *transparency*, which means they opt to leave out uncertainty information when it does exactly what it is supposed to, decrease certainty in conclusions.

Science communication should be primarily concerned with accuracy. Setting trust and risk-aversion as the variables of interest implicitly encourages statisticians to set trust and risk-aversion as the primary goals of communication. The issue of trust being divorced from trustworthiness has been commented on by other authors [@ONeill2018], however the issue still persists in the uncertainty visualisation literature. @Zhao2023 displayed a several variations of a visualisation of a model prediction and its uncertainty and took participants using the model prediction as a sign of trust. They reported that visualising uncertainty information caused participants to trust the model in the low variance case, but the results in the high variance case were inconclusive. The discussion made it clear the authors thought the uncertainty information should make the visualisation more trustworthy, but conflating trust and the use of a prediction implied uncertaitny information should somehow influence participants to use their own prediction, even though a prediction being uncertainty does not necessarily mean it is incorrect. Despite this, the authors seemed to assume that the uncertainty information *should* have an influence on that, showing they had not deeply considered *how* uncertainty information should influence the choices of the participants. (*Cite Gap 18*)

A similar measure to trust is using "confidence" in an extracted value or a decision. Interestingly, "confidence" is also used to try and capture the clarity of a message in a normal visualisation. Confidence cannot simultaneously be a measure of clarity of visualisation *and* a way to capture the uncertainty expressed in a visualisation.

#### Other (questionable) attemps to capture uncertainty
There is also a swath of studies that are aware a question that boils down to a value extraction experiment, or a question that should be answered by literally ignoring uncertainty information is not what we want when we consider uncertainty visualisations. These papers often try to ask a question that should utilise both the uncertainty and signal in the response, however this is rarely what actually occurs. This method typically results in in cryptic or confusing questions that create a large amount of noise on the interpretation side of the analysis [@Hullman2016].  

Some authors opt for asking slightly vague questions that imply a use of uncertainty, but compare it to a ground truth that is very specific. @Ibrekk1987 asked participants for the "best estimate" which was evaluated in accuracy by comparing it to the mean, however the "best estimate" depends on the loss function we are using, and a loss function of minimised error was not implied by the question. @Hofmann2012 showed two distributions in 20 different visualisations (a lineup protocol) using a jittered sample, a density plot, a histogram, and a box plot and asked participants. Participants were asked to report in which of the plots was "the blue group furthest to the right" The experiment set up is shown in @fig-right. The participants answers were then compared to a ground truth where the correct plot had a blue distribtuon with a right shifted mean. By comparing the results to a ground truth statistic and marking participants as "wrong" or "right", the error from the participants that had an alternative interpretation to the concept of "furthest right" was conflated with the error from a the visualisation choice. These papers make it unclear if the participants got the answers wrong because they misunderstood the question or because of something related to the plot. Therefore, this method leads to inconclusive results about the plot design, and is not advised.

![This shows the user interface for the experiment performed by @Hofmann2012. The question of "furthest to the right" is open to interpretation. ](furthestright.png){#fig-right} 

Another method used by authors is to ask a deterministic question about a random event. @Padilla2017 provided participants with a visualisation of the cone of uncertainty and asked then to "decide which oil rig will receive more damage based on the depicted forecast of the hurricane path". The cone of uncertainty provides a 60% confidence interval for the location of the eye of a hurricane, which allows us to know the area where the eye of the storm will go, it does not given any information about the intensity of a storm, the size of a storm, or even if a location will be hit. This inclusion of determinism seems to cause the authors to stumble themselves, as they are not consistent with their assumptions. In their first experiment @Padilla2017 indicated the correct answer was to assume that the storm was equally intense no matter how far from the center of the distribution an oil rig was, however answering their third experiment correctly hinged on assuming the intensity of the storm at a particular point (which in this experiment they phrased as damage) *does* change in intensity as you move away from the center of the distribution. Given these conflicting assumptions, it is unclear how the participants were supposed to adjust the probabilistic path information to answer a deterministic question about which oil rig would receive the most damage. The flipside of this is asking participants for a deterministic answer to a probabilistic question. @Correll2014 asked participants "how likely is candidate B to win the election?" when the two distributions indicated voter preference. Participants were not able to answer the question about likelihood in term of probability, but were instead given seven options from 1=Outcome will be most in favor of A to 7=Outcome will be most in favor of B. The ground truth statistic for this question was a scalar multiple of Cohen’s d, indicating participants were supposed to incorperate uncertainty information using a very specific formula that was likely unknown to them but assumed to be used implicitly.  

The final method used by authors is to just explicitly ask about uncertainty and signal information separately. @Sanyal2009 mapped uncertainty to dots and signal to a 3D surface and asked participants to identify areas of high and low signal and high and low uncertainty. Participants were not asked to combine that information in any way, and the signal and the noise were treated as separate variables. @Correll2014 asked participants to separately extract the mean and variance from four uncertainty visualisations. These methods explicitly view the uncertainty and signal as two separate variables that should be extracted from a plot, and not two variables that should be interpreted together. Even viewing these questions as a routine check to make sure the signal information isn't impacted by the uncertainty is counter intuitive, because the whole point *of* the uncertainty is to impact the signal information.

These examples are a bit complete mishmash of methods, however they point to a larger issue that goes beyond decision making, trust and confidence experiments. Authors *have no idea* how to evaluate the effects of uncertainty in an uncertainty visualisation.

### Interpretation and semantics
Interpretation and semantics experiments are seeking to identify a dimension (or visual task) that uncertainty naturally maps to. These experiments inherently view uncertainty as a variable that is separate to the variable on which we have mapped our signal. For example, lets say we have a map were maximum daily temperature is presented using points where the colour (red for hot and blue for cold) of the point is associated with the temperature, and the bluriness of that point is associated with the variance *of* that temperature. It is highly likely that our brains will not flatten that into a single variable depicting noise and signal, but rather *separately* extract the temperature (colour) and uncertainty (blur) information as two independent variables. If the variables are extracted separately, there is no guarentee that the uncertainty will act as an approriate signal supressor. This problem has been noticed by others in the field, that typically use this method (specifically in the spatial uncertainty context) and a desire for representations that integrate uncertainty and signal is one of the reasons for the invention of the value-supressing uncertainty pallet [@Correll2018].  

Value-supressing uncertainty pallets (VSUP) were developed as a method that would allow the signal and the noise to be interpreted together such that insights gained by the viewers of a plot are appropriately *supressed* by the uncertainty. Hence the name of the pallet. @fig-maps depicted this map colouring approach and several other extensions on the typical choropleth map that differ in where in the visualisation process they combine the noise and signal information into a single concept of "valid signal". The first and most basic map is a simple choropleth map where each value (the colouring of each local government area) has no associated "uncertainty". The next map (which embodies the approach taken by the semantics experiments) is the bivariate map which maps the signal to colour value and the uncertainty to colour hue. If visualisation was performing signal supression then that would mean the two dimensional space defined by colour value and colour hue can be mentally "flattened" into a single dimension of "valid value" that can captures the signal the our brain would need to be able to flatted this two dimensional space into a single space of "signal validity".  The idea of two perceptual tasks flattening into one variable in the mind of the viewer may be wishful thinking, but it is not impossible given we are not certain on how the perceptual tasks are mapped within the human brain. @Sterzik2023 found that when a value was mapped to the textures of stippling, hatching, and triangles, and found that the difference between two points on this one dimensional texture was actually a 2D space (likely "texture business" and "light/darkness"). That being said, if we look at the visual signals presented by the bivariate map, where the contrasting light and dark areas actually has no important meaning, it is unlikely this occurs for colour value and hue (or at least it doesnt occur in a way that is useful for uncertainty visualisation). Instead of hoping that uncertainty might collapse signal values into a single dimension, we can do some of that work ourselves, by using a VSUP which collapses the colour space such that high uncertainty values cannot be extracted. It is unclear how useful VSUPs are at actually combining signal and noise and therefore supressing plot level insights, as they have only been tested on simple value extraction tasks that require evaluating a single point [@Correll2018; @Ndlovu2023] rather than looking for spatial relationships (which is arguably what maps are for). Following along with this trend, the next way we might consider visualising uncertainty is to combine uncertainty and signal at the earlier stage so the "supressed signal" is represented by a single variable. This statistic can then be expressed in a one dimensional colour space, which is a method addopted by the Baysian surprise metric map [@Ndlovu2023] and the excedance probability map [Lucchesi2017]. .
 
::: {#fig-maps}

![](mappals/img1.png)

![](mappals/img2.png)

![](mappals/img4.png)
![](mappals/img3.png)

![](mappals/img5.png)

*This plot is the highest priortiy to for replacing with my own R coded maps, becuase the effects of these different mapings is really not visible using these methods* Five different ways to include error information in a map. A typical chloropleth map is considered the map that ignores uncertainty and simply visualises the signal. The bivariate map visualises the uncertainty, but does so using two different visual channels, which gives the perception of two different signals (or one signal in the assumptions the colors contrast with each other). An adjustment to the bivariate map is the VSUP which is similar to the bivariate plot except it merges high uncertainty values in the palette to reduce our ability to discern values that have a high enough error to not be a valid signal. The Bayesian surprise metric is similar to the lineup protocol, where the colour value is the difference between the observers posterior and prior beliefs. The exceedance probability map is a direct visualisation of the probability of a specific hypothesis.
:::

These maps make the importance of combining uncertainty and signal in a single visual channel clear. A chloropleth map will show signal that is not valid inference because of high uncertainty. At the other end of the spectrum, the bivariate map will show signal that is not always interesting because it forces us to interpret uncertainty and signal separately. As we move through these methods, it seems that the validity of any overarching insight becomes more visible at the cost of our ability to extract particular values of signal or noise. Therefore, given that the primary goal of visualisation *is* insights [@North2006], visualisation authors should err on the side of representing supressed signal as a single variable, rather than visualising uncertainty separately using two different channels. 

## Suggestions to measure uncertainty
So, the current methods of measuring or understanding the role of uncertainty in a visualisation is quesitonable at best, however this is not because visualiation authors are missing the mark, but rather uncertainty is *particularly* difficult to express in a visualisation. In simple estimates or verbal communication, the signal is often easy to identify because it is what we are explicitly saying. Unlike statistical models, visualisations are used in both data exploration and communication. This means what exactly is a *signal* in any particular visualisation is hard to identify, since we often let the visualisation *tell us* what the signal is. Additionally, you cannot add noise to *every single possible* signal one might take from a visualisation. Two people looking at the same visualisation might, just by chance, develop two entirely different insights and draw inference on two completely different statistics. These unique and fascinating challenges that are faced by uncertainty visualisaiton have been completely untouched by the literature. This section will cover some interesting research in uncertainty visualisation and suggestions for better ways to measure uncertainty.

### More specific hypothesis
Heuristic checks are useful because they look at unknown pitfalls that might exist in interpretation of current plots. Since the hypothesis for these experiments are usually quite specific, e.g. "do people perceive a an outcome that is within the bar as more likely than one outside it, even if both outcomes are the same distance from the mean?". This means they are less likely to fall into the trap of trying to answer questions that are *far* too broad to be answered with a single experiment (e.g. "is a scatter plot better at showing uncertainty than a box plot"). This work also provides useful insights for experiments by highlights pitfalls participants might fall into when they review the results of evaluation experiments [@Hullman2016]. @Newman2012 found that participants were more likely to view points within the bar as more likely than points outside of the bar in bar charts with error bars. Similar effects have been identified in other types of uncertainty displayed. @Padilla2017 found that points that were on an outcome of an ensemble display were perceived as more likely than points not on an outcome, even when the point that was not on a specific outcome of the ensemble was closer to the mean of the uncertainty distribution. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015].

In a similar vein, experiments that verify smaller aspects of plot design might be more useful to the field in the long run because it helps contribute to a larger working theory of "how do we see visualisations". Many visualisation experiments try to compare two plots with several differences, but do not seem to be interested in the mechanisms by which we extract information from visualisations. Small perceptual tasks that seek to answer small but highly relevant questions (for example, if colour hue and colour value can be perceived as a single signal suppressed variable) would be useful to the field.

### Qualitative Studies
Alternatively visualisation research could shift away from the accuracy concept all together ask questions that allow for open ended responses. This method can enlighten authors as to *how* the uncertainty information was used by the participants. @Hofmann2012 tried to capture this by asking participants why they considered a particular plot to be more "right shifted", however this qualitative assessment does not seem to have made it into the final paper. @Daradkeh2015 presented participants with ten investment alternatives and asked participants "from among available alternatives, which alternative do you prefer the most", and were asked to think aloud and consider the uncertainty in their decision making. The experimenters goal was to observe and organise the methods people use when making decisions in the face of uncertainty. This study was an excellent example in a useful experimental design. They highlighted the specific aspects of uncertainty that participants typically considered, such as the range of outcomes that are above/below a certain threshold, minimum and maximum values, the risk of a loss, etc, and mapped where in the decision making process participants made these considerations. Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and pull out a large number of adjacent observations. This experimental framework could replicate this process. 

### Just noticiable signal
It could be argued that a well done uncertainty visualisations should have an imperceptible signal unless the signal would be identified with a hypothesis test, almost like a reverse line up protocol, but this idea also has some issues that should be considered. The reject or do not reject concepts in hypothesis testing do not offer a complete image of uncertainty, and exploration of uncertainty visualisation largely stems from a desire to move away from this binary framework. Additionally, setting up an uncertainty visualisation where the participants are expected to notice the signal once the data behind the visualisation passes a hypothesis test implies the signal *is* noticiable to a human at that level. @Patrick2023 compared people ability to recognise patterns in a residual plot to typical statistical tests and found human viewers looking at a plot were less sensitive than the typical residual tests. These experiments utilised the lineup protocol which has been suggested as a method to check if perceived patterns are real or merely the result of chance [@Buja2009; @Wickham2010; @Chowdhury]. This concept bears similarity to the goal of uncertainty visualisation, but it is not quite the same. @fig-hypyvs shows the conceptual difference between the lineup protocol and uncertainty visualisation. 

![Difference between the null plot vs the uncertainty plot. A lineup protocol displays the uncertainty about the null and identifies if the true data plot is identifiable (and therefore significiantly difference) while an uncertainty plot displays the variance of the estimated value and assesses if the null of "no signal" is within this plot (e.g. if the error bars overlap with zero). ](hypvs.jpeg){#fig-hypvs} 


# Great Examples 
Despite the common problems detailed in the previous sections, there is some interesting work in the uncertainty visualisation space. This can come in the form of an uncertainty visualisation that attempts to visualise a typically ignored aspect of uncertainty, or an experiment that avoids the pitfalls detailled in the previous sections.

The literature focus on "quantifiable" uncertainty leaves the variance that occurs at early stages in our analysis that is difficult to quantify ignored and forgotten. Some authors have chosen to focus on these unquantifiable cases and look at expressing more complicated cases of uncertainty.  @Tierney2023 builds upon the tidy data pricipals to allow users to handle missing values. This includes data plots with a missing value "shadow" that allows visualisation authors to identify if the variables used in a plot have any structure in their missing values, which would contribute to uncertainty. Another example of uncertainty that is often ignored is the uncertainty resulting from human choices. Climate scenario uncertainty, shown in @fig-climatescenario, attempts to display the range of climate change outcomes that can result from a range of best and worse case human choices. 

![Example of a I will just make my own version of a climate scenario plot, this one is just here from a random site as a place holder](climatescenario.png){fig-climatescenario}

Another visualisation method that has a lot of potential is visualisation of samples. Visualisations that opt to express a signal as a sample rather than an estimate have the potential to supress signal since it is not explicitly visualised, however this has yet to be shown in evaluation experiments. This is not to say that visualisations of mass would not be able to perform signal supression, but a sample can easily be expressed using aesthetics such as colour on a map and mass visualisation often struggling with issues such as over or under smoothing. These sampling methods can show more of the messiness of the data that sits behind a model. This may not have a detrimental effect on the viewers ability to extract global statistics, as it seems they can be extracted from a visualisation of a sample with ease [Franconeri2021]. Sample visualisations have been used in maps with the pixel map, shown in @fig-pixelmap [Lucchesi2017], but is more commonly used in animation with the HOPs plots [@Hullman2015] or similar concepts [@Blenkinsop2000]. This method has also been adopted by visualisation authors outside of academia as can be seen in the  and the New York Times class mobility figure, shown in @fig-nyt, an animated version of which can be found [here](https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html). 

![](pixelmap.png){fig-pixelmap}
![](classmobility.png){fig-nyt}

A final method to perform signal supression is simply to visualise the data if it is available and relevant to the uncertainty distribution. An example of this is shown in @fig-census for racial distributions spatially in america, an interactive version of the plot can be found [here](https://edition.cnn.com/interactive/2021/us/census-race-ethnicity-map/). This map shows the typical causes of uncertainty in a spatial model, (e.g. regions where data is sparse, ethnically diverse areas, uneven distribution of points within boundaries, etc) but it avoids the need to create a visualisation with a specific signal in mind. This is the technique typically employed by exploratory data analysis, which means it's lack of a specified signal means there is both *no* uncertainty (since we are technically not performing inference) but in the event we *do* implicitly perform inference, there is some hedging. In this sense, the best uncertainty visualisation you can get without specifying a signal you want to convey is visualising the data itself. The census dot map's addition of interactivity also allows users to zoom in and see the details that caused "uncertainty" in the form of inconsistent colours at lower resolutions when they were zoomed out. This does not mean that visualising raw data instead of implementing sampling techniques is always a valid uncertainty visualisation that will prevent insignificant signal from getting through. @Buja2009 illustrated how groups that appear linearly separable in a linear discriminant analysis (LDA) visualisation of the data can actually be the result of a LDA performed on too many variables, something that was not clear from the visualisation until the lineup protocol was implemented. However it is simple but effective option that seems to be laregly overlooked by the uncertainty visualisaiton community.

![](censusdotmap.png){fig-census}


# Future work
This paper has identified several issues in the uncertainty visualisation literature and with those issues, we have several suggestions for future work that would likely be fruitful for the field.

Our first suggestion for future work is a mathematical defintion of uncertainty. There is currently a lot of work that quantifies the uncertainty through bias and variance at various stages of the data analysis pipeline separately. Elements such as imputed data, assumptions, sampling methods, and analyst decisions all have their own independent work, quantification and discussions, but methods to brand between these concepts are few and far between. A conceptual framework that allows us to combine these results would be incredibly beneficial to the field.

The concept of uncertainty should also be formalised within the grammar of graphics. This formalisation would allow uncertainty visualisation authors to have a clear understanding of what is or is not an uncertaitny visualisation. Additionally placing uncertainty visualisation in the framework that is used to understand existing information visualisation research would help authors understand when existing methods can be used to explain their results. Incorperating uncertainty into the grammar of graphics will also give a more precise concept of the information contained within a plot. Other fields of science employ marginal changes when designing experiments to ensure it is well understood *what* aspect of their experiment is contributing to their results, and a better sense of what "marginal" is in the case of uncertainty visualisaion would greatly help the field.

Additionally the experimental practices within uncertainty visualisation need to be less ad hoc and more standardised. If we are going to consider uncertainty as noise, not signal, there needs to be a way to identify this signal supression in an experimental design. As the literature currently exists, there is no way to combine papers to get a meaningful sense of how uncertainty information is understood by a viewer. There is also the possibility that uncertainty visualisation evaluations will need to swap to a qualitative methodology where participants are allowed to freely comment on what they notice in graphics until we establish how the existence of noise can be observed.

If an uncertainty visualisation researcher would prefer to perform experiments rather than formalise methods, there are options there too. It would be interesting to know if any perceptual tasks that can be mapped to two different visual tasks condense into a single dimension when looking for overarching signal in a plot. Alternatively, the task dependency many authors in uncertainty visualisation mention would be a useful direction to consider. It is clear that the the number of potential tasks that can be performed on a visualisation increases with with the number of observations. A single observation is limited to value extraction, two observations can be compared, multiple observations allow for shapes or global statistics to be extracted. The interaction between sample size and task is of particular interest to the uncertainty visualisation community, as uncertainty can be expressed through multiple observations using a sample, or through a single value using an error. Of course, this is limited by the fact that there also isnt a definition for what is a "task" and given the mess created by the lack of formalisation in uncertainty visualisation, it may be wise to formalise that concept before performing these experiemnts. @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations, which could be a good starting point. Regardless, these are directions of research would be fruitful to the uncertainty visualisation community even if it appears on the surface to be research that is only beneficial to the "normal" visualisation community.

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "ideas/confirmation/confirmationreport.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```