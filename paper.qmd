---
title: "Noisy Work: A Review of The Uncertainty Visualisation Literature"
author: Harriet Mason
bibliography: references.bib
date: last-modified
toc: true
number-sections: true
format: pdf
editor_options: 
  chunk_output_type: console
abstract: In reviewing the literature of uncertainty visualisation
---

# Background

## Introduction
From entertainment choices to news articles to insurance plans, the modern citizen is so over run with information in every aspect of their life it can be overwhelming. In this overflow of information, tools that can effectively summarise information down into simple and clear ideas become more valuable. Information visualisations remain one of the most powerful tools for fast and reliable science communication. 

There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe's quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Anscombe quartet is shown in @fig-anscombe, because describing the data is never the same as seeing it. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about our data and **how** it might diverge from what we expect. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up Anscombe's quartet. The four scatter plots are visually distinct but have the same mean, standard deviation, and correlation. The visualisation highlights the importance of plotting your data to identify interesting features that are hidden by other summary statistics."
library(tidyverse)
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```

Uncertainty visualisation is a somewhat new subfield of visualisation, with early papers that specifically reference an "uncertainty visualisation" popping up in the late 90s and early 2000s (*Cite Gap 1*). Despite the youth of the field, commonly used uncertainty visualisations such as a box plot or histogram have been around for almost as long as statistical graphics themselves (*Cite Gap 2*). Early mentions of "uncertainty visualisation" appear in computer science papers discussing uncertainty visualisation as a subfield of geospatial information visualisation, specifically for the many uncertainties that come with satelite images (*Cite Gap 1*). In the modern day, uncertainty visualisation has exploded into its own field that applies to a wide range of contexts and is no longer bound by geospatial information (*Cite Gap 1*).

## Similar work
::: aside
I have a large backlog of uncertainty literature review summaries in my Mendely that need to be added here. There are a lot of them.
:::

Any field that experiences an explosion of new research will also invite a series of literature reviews that seek to combine and summaries that information. Given that uncertainty visualisation has been around for a few decades now, there is already a wealth of uncertainty visualisation literature reviews. 

Interestingly, these reviews often do not offer overarching rules for tried and tested uncertainty visualisation, but rather comment on the *difficulties* faced when trying combine the papers from this field. @Kinkeldey2014 found most experiments the methods for uncertainty visualisation evaluation to be adhoc, with no commonly agreed upon methodology or formalisation and no greater goal of describing general principals. @Hullman2016 commented on the difficulty in taking overarching themes from uncertainty visualisation, as several conflated issues make it unclear if subjects did poorly in an experiment because they misunderstood a visualisation, because the question was misinterpreted, or because they used a heuristic. @Spiegelhalter2017 commented that different plots are good for different things, and disagreed with the goal of identifying a universal "best" plot for all people and circumstances. @Griethe2006 was unable to find common themes, but instead listed the findings and opinions of each uncertainty visualisation paper.

There are a handful of reasons that are used to explain why it is so difficult to generalise the findings in the uncertainty visualisation literature. Some papers suggested or agree with the idea that visualisation typologies should move away from data types, uncertainty categories, and representation types and towards "task-centered typologies" as this will help generalise results [@Kinkeldey2014; @Hullman2016]. Some authors may not attribute the task to the *reason* the literature cannot be easily summarised, but do mention that the choice of best visualisation is highly dependent on the specific goal of that visualisation [@Griethe2006; @Spiegelhalter2017]. Others comment that visualisations are highly dependent on the audience and there is no such thing as a "best" visualisation that will be accessible to all members [@Kinkeldey2014; @Spiegelhalter2017]. These difficulties in noisy uncertainty visualisation findings arises reguardless of the direction the analysis was approached, be it looking at geospatial data [@Kinkeldey2014], or uncertainty visualisation as a whole [@Hullman2016; @Spiegelhalter2017; @Griethe2006].

This review believe all these sources of noise have a single root cause, a lack of a cohesive and encompassing definition of uncertainty. The absence of an econompasing definition of uncertainty is mentioned by every uncertainty visualisation directly [@Spiegelhalter2017; @Griethe2006] or indirectly by describing a miriad of ways it can be considered in the literature [@Kinkeldey2014; @Hullman2016], although it is never commented on as a source of the noise in the field.

In understanding the definition problem, we will understand the *purpose* of visualising uncertainty, and understand if the current visualisation tools are sufficient to achieve that purpose and if not, where there is need for improvement. In understanding this purpose, we will also consider whether or not "uncertainty visualisation" is a valid sub-field of visualisation itself, or if that distinction has only arisen as a by product of the confused definition of uncertainty. Ultimately, this paper should serve as a guide for future uncertainty visualisation authors to prevent their work from becoming noise in what is already an incredibly noisy field.

# What is Uncertainty?
## Colloquial definitions
Uncertainty visualisation is made particularly difficult by the term "uncertainty" lacking a commonly accepted definition in the literature. This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. 

Definitions presented in papers that discuss uncertainty visualisation want it to be encompassing and include everything a layperson might consider when trying to consider "uncertainty". Some consider it to be synonymous with specific terms defined in mathematics, such as probability, variance, error, or precision. Others consider it to be an encompassing umbrella term of which for these mathematically defined objects are only examples, and include concepts that are somewhat related, such as missing values (*Cite Gap 3*). This wide array of uncertainty definitions becomes a snake eating its own tail, as many authors opt to redefine uncertainty themselves[], ignore defining uncertainty entirely [], or pick a definition from a seemingly randomly previously published papers [], all of which make variance in uncertainty definitions even worse (*Cite Gap 3.5*).

While these vague definitions might be OK for polite conversation, they are *not* foundation on which to build an entire sub-field of visualisation. In order to visualise uncertainty, it needs to be quantifiable [@Griethe2006; Leland2005], in order to quantify uncertainty, it needs to be mathematically defined. The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase, (e.g. @utypo defined it as any deviation from complete determinism) but go on to *quantify* uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation (*Cite Gap 4*). This definition swap happens so subtly that it seems to go unnoticed by authors writing the papers, and the fact that "uncertainty visualisation" methods dont even notice they are exclusively focus on depictions of easily quantifiable uncertainty, such as error [], mass [], or variance [] (*Cite Gap 5*). This has left the expressions of uncertainty that are hard to quantify or visually differentiate untouched, despite many papers calling for their invention (*Cite Gap 6*). There is likely only confusion and obfuscation if the vague definitions of "uncertainty" continue to be used and authors should try to be more specific with the mathematical object they are visualising, *or* a strict definition of uncertainty needs to be defined. The current method of titling papers and defining uncertainty forces readers to comb through the method of a particular visualisation experiment to understand if the uncertainty visualised is amenable to their needs. 

Some papers have recognised these colloquial definitions are lacking and have made attempts to formalise the space with taxonomies, however these taxonomies come with their own issues. 

## Taxonomy definitions
Much in the same way that there are almost as many definitions of uncertainty as there are papers on the topic, the field is also over run with taxonomies. Taxonomies split uncertainty based on an endless stream of ever changing boundaries, such as whether the uncertainty is due to true randomness or a lack of knowledge [@Spiegelhalter2017], if the uncertainty is in the the attribute, spatial elements, or temporal element of the data [], whether the uncertainty is scientific (e.g. error) or human (e.g. disagreement among parties) [], if the uncertainty is random or systematic [], statistical or bounded [], accuracy or precision [], if the uncertainty is about the past or the future [], the stage of the data analysis pipeline it comes from [], the severity of the uncertainty [], how quantifiable the uncertainty is [@Spiegelhalter2017], etc., etc., etc. (*Cite Gap 7*). In their own way, each of these taxonomies show an apsect of uncertainty that an author felt was important to differentiate. @utypo identified many common themes among this wide array of taxonomies and used them to design an encapsulating taxonomy, depicted in @fig-taxonomy. This definition organises uncertainty based on three axis, location (where the uncertianty is coming from), level (how quantifiable it is), and nature (if it comes from true randomness or incomplete knowledge). 

![*I need to fix the taxonomy illustration because inputs and model uncertainty need to be swapped*. Depicts an illustration of the taxonomy described in @utypo. From right to left the drawing shows the location, level and nature of uncertainty with examples of that category underneath. A specific source of uncertainty from the location can be mapped to a level of ignorance that can increase or decrease (i.e. moving up or down the green line) depending on the nature of the uncertainty. Identifying the location, level and nature of your uncertainty allows you to better understand it. ](taxonomyvis.jpeg){#fig-taxonomy}

When considering the use of taxonomies to define something inherrently vague, such as the uncertainty in an uncertainty visualisation, it is helpful to consider this quote from *The Grammar of Graphics*:

> "Taxonomies are useful to scientists when they lead to new theory or stimulate insights into a problem that previous theorizing might conceal. Classification for its own sake, however, is as unproductive in design as it is in science. [@Leland2005]

Therefore, while taxonomies can be helpful to map out the space of "things that we consider to be related to uncertainty", a taxonomy is *not* a definition and do little to help statisticians untangle and estimate the uncertainty in their projects. That being said, these taxonomies can hint towards what is important when we think about uncertainty. The location axis of the @utypo taxonomy lines up neatly with the typical data analysis pipeline and multiple authors in the uncertainty literature have commented on the need to consider quantifying and expressing uncertainty at every stage of a project [@Kinkeldey2014; @Hullman2016; @Refsgaard2007] including stages as early as conceptualising the problem [@Otsuka2023] or collecting the data [@Meng2021]. The level axis from the @utypo taxonomy also hints towards an important consideration in uncertainty, because establishing how quantifiable uncertainty is informs us how it can be communicated, something that @Spiegelhalter2017 identified in the form of "precision". It is commonly noted that each source of uncertainty in an analysis must be discussed in isolation, but combining the uncertainty from every stage is near impossible [@Spiegelhalter2017] (*Cite Gap 8*). It is clear that elements of these taxonomies are identified in many other comments on uncertainty and represent *real* considerations that need to be made when defining uncertainty, even if that definition is yet to be published. 

The task of providing an encapsulating mathematical definition of uncertainty is far beyond the scope of this work, however we will discuss an important but commonly misunderstood feature of uncertainty; its relationship to inference.

## Defining uncertainty with respect to inference
What exactly is uncertainty, then? If we were to consider making this overarching definition, what would it need in order to be "encapsulating"? Well, let us consider what might *not* be considered uncertain in order to understand this concept a little better. 

@Otsuka2023 spends the first chapter of his book discussing the place of descriptive statistics in the philosophy of the field and in doing so, highlights an interesting connection between inference and uncertainty. Descriptive statistics simply describe our sample as it is and summarises large data down into an easy to swallow format. Descriptive statistics are not seen as the primary goal of modern statistics, however this was not always the case. Around the 19th century in England, *posivitism* was the popular philosophical approach to science (positivists included famous statisticians such as Francis Galton and Karl Pearson) and practitioners of the approach believed statistics ended with descriptive statistics as science must be based on actual experience and observations, therefore anything that refers to the unobservable (such as new observations or population statistics) is not true science [@Otsuka2023]. By its very nature, descriptive statistics *cannot* be used to make inferences about the data because it simply exists to summaries the data, to use it to make statements about *new* data is incorrect useage. In order to make statements about population statistics, future values, or new observations we need to perform inference, which requires the assumption of the "uniformity of nature" (i.e. that unobserved phenomena should be similar to observed phenomena) [@Otsuka2023]. This subtle shift, from descriptive statistics to inferential statistics was historically shunned *due to the fact it introduced the unknowable*, or in other words, uncertainty. 

This philosophical understanding of statistics highlights that descriptive statistics do not have uncertainty, however some readers may disagreee with that statement. Variance and probability are typically considered stand ins for "uncertainty", it is often how we choose to measure it, and since probability and variance exist in descriptive statistics, descriptive statistics *must* have uncertainty. This is not necessarily true, and related distinction was made by @Spiegelhalter2017  commented on a differentiation between precise random events (such as the probability of a coin flip), and uncertainty (such as the estimated probability associated with a coin that might be biased). A sample variance is not unknown, and therefore it is not uncertain, rather it is a precise description of dispersion. If we were to discuss drawing a new observation, or estimating the true mean of a population *then* the the variance would become relevant in our discussions of uncertainty, but in isolation it is not uncertain.

The idea that inference is related to uncertainty pops up in most discussions of uncertainty, however, due to the now *implicit* understanding we are performing inferential statistics, it is often brought up as a *task* or *goal* dependence. This is mentioned both by authors at a specific stage of an analysis, and by authors looking at an entire field or at all stages of the analysis pipeline. @Otsuka2023 suggested that the process of observing data to perform statistics is largely dependent on our goals, because the process of boiling real world entities down into probabilistic objects depends on the relationship we seek to identify with our data. @Meng2014 commented what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the the question we seek to answer. @Carlin2023 mentions that each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to statistical analysis that is devoid of meaning. @Wallsten1997 argue that the best method for evaluating or combining subjective probabilities depends on the uncertainty the decision maker wants to represent and why it matters. @Munzner2009 created a nested model for visualisation that highlighted how the first mistake that can be made in a visualisation is in the problem characterisation, and failling to do it well can cause downstream effects and damage the effectivness of a visualisation. @Fischhoff2014 looks at uncertainty visualisation for decision making decides that we should have different ways of communicating uncertainty based off what the user is supposed to do with it. Health have a wide range of metrics used to communicate different risks [@Spiegelhalter2017] indicating they utilise new statistics based off the specific aspect of risk that is relevant to the issue. These examples show that authors that discuss uncertainty believe it is there is an important relationship between uncertainty and what we decide to keep in our analysis and throw away, i.e. the task or goal. The task or goal is, in essence, the *statistic you are drawing inference on*. Were this not true, we would have no reason to differentiate between a prediction interval and a sampling distribution because they could both be considered "uncertainty about the mean", however they are different because their inferential goals are different. 

These examples highlight two key concepts that seem to be true about the relationship between uncertainty and inference:    
1) Uncertainty is the by-product of inference, as we seek to draw conclusions about something unknowable.    
2) Uncertainty must be defined with respect to a *specific* statistic we wish to draw inference on, there is no such thing as a *general* uncertainty that is not linked to a motivating question.    

## Mistakes when we misunderstand uncertainty
We feel the need to make this point clear because a *large* amount of uncertainty research seems to be confused by the purpose of uncertainty visualisation, what it is related to, and why we visualise it. Often, the authors are even more confused about what they are doing than the participants. There are enough papers that fail to understand these basic principals of uncertainty that this entire paper could simply be a list with explanations. Instead, we have a handful of highlights presented below.  
- @wu2023rational recognised it is difficult to understand when differences in performance stem from information asymmetry.
- @Hofman2020 asked questions about discusses the difference in showing inferential or outcome (prediction) intervals and then found that participants shown the inferential distribution performed poorly when asked questions about predictions. They found that showing the relevant distribution to the question reduced the error. This result should be astoundingly obvious if it was well understood that uncertainty is not some latent variable and is only relevant for specific ground truth statistics. This papers *percieved* importants highlights an underlying belief that uncertainty is linked to a specific variable or data set, not the variable we are drawing inference on.  
- @Padilla2021 found that high uncertainty in the model estimates (calculated uncertainty) and low forecaster confidence (which is typically an expression of suspected bias in the model) both caused participants to have decreased confidence in their results and suggested modelers express both if they are relevant. @Kale2019 discussed the importance of communicating decisions made in the data analysis pipeline and being aware of the alternatives. While there is nothing wrong with the core ideas of these papers, the fact that they were published indicates that the general population was not already aware that choices introduced early in the data analysis pipeline create bias and therefore uncertainty in our final values. This indicates a general failure to see bias and variance both as sources of uncertainty.  
- @Wickham2011 suggests their product plot framework, which includes histograms, should have a way to measure uncertainty, but does not consider that a histogram is *already* a depiction of mass and would already be considered an uncertainty visualisation were our statistic of interest the population mean. To suggest that a histogram needs uncertainty bounds is similar to suggesting a density or box plot need uncertainty bounds.  
- @Griethe2006 commented that "if visualization is used as a means to explore a data volume or to communicate its contents the uncertainty has to be included". @Potter2010 aimed to create a summary plot that "concisely presented data with uncertaninty information". These papers fail to realise the exploration step of an analysis, which includes descriptive statistics, exploratory data visualisaiton, and unsupervised machine learning techniques, is performed without a prior hypothesis. Therefore, it is performed to *suggest* a direction for future inference work or communication, which *do* contain uncertainty and does not contain uncertainty within itself. This confusion would not occur if it was understood that uncertainty does not exist without inference.  
- @Ibrekk1987 asked participants for the "best" estimate while displaying a population density function. The participants proided the mode instead of the mean, although there was no indication the mean was what was being asked about.  
- @Boukhelifa2012 tried to quantify the strength of the intuitive connection between "sketchiness" and uncertainty. Since they had seen literature that explained uncertainty was task specific, they tested the connection for multiple "tasks", what they failled to understand, is that the "task" dependency which is likely just a proxy of the evaluation experiment problem discussed in the previous section. This misunderstanding led to "sketchiness" being used to depict uncertainty in the categories of bar charts, in graphic networks, and in train lines, with little consideration or explanation as to *how* these things would be uncertainty. I imagine it is quite hard to be "uncertain" about the existence of a train line. Participants rightfully assumed the sketchiness therefore represented something else, such alternative options or simply ignored it.  

![The graphics displayed by @Boukhelifa2012 to identify if there is an intuitive connection between sketchiness and uncertainty.](sketchiness.png){#fig-sketchy}

The reality is that a lot of uncertainty visualisation authors do not seem to have an intuitive understanding of uncertainties connection to inference, when inference is being performed, and how to design experiments that capture this relationship.

This misunderstanding is not due to laziness or the fault of the authors, but rather is likely caused by the absence of a strict definition of uncertainty. The imprecise and confusing set of existing definitions are creating a field in which the authors themselves do not know what they are testing. Uncertainty examples include imputed data, model selection, inherent randomness, biased sampling, etc, not because these things *are* uncertainty, but because they *create* uncertainty *when we perform inference*. These things are not "uncertainty" in of themselves, but rather contribute to the distance between the final estimate and the statistic we want to perform inference on. The field of uncertainty communication is in desperate need of a unifying mathematical definition that firmly identifies exactly how each of these things contribute to uncertainty. This may be a considerable task, but as of right now, the list of things that are "uncertainty" continues to grow, and the best methods of uncerstanding and quantifying them are ad-hoc at best. @Thomson2005 suggests a mathematical formula for *examples* of uncertainty, @Meng2014 mathematically defined the variance introduced to a model by the array of model choices, information theory tries to quantify uncertainty using the idea of entropy. None of these existing methods are thorough enough for an analyst to understand what causes uncertainty, and quantify it for communication. Realistically, this problem is well known, and put simply by Freeman Dyson in his fameous Birds and Frogs speech:

> Rigorous theorems are the best way to give a subject intellectual depth and precision. Until you can prove rigorous theorems, you do not fully understand the meaning of your concepts. - Freeman Dyson (Birds and Frogs speech)

This limitation becomes apparent when you look at the limited scope of what the uncertainty visualisation literatrue has *actually* focused on visualising. Almost every uncertainty visualisation paper contains a broad and encapsulating definition of uncertainty in the introduction, followed by a visualisation of a normally distributed probablity density function.

> Edit notes: Need some way to show uncertainty visualisation papers provide sweeping definitions and then visualise a simple PDF everytime.

In order to visualise something, it needs to be quantifiable. In order to quanitfy something, it needs to be mathematically defined. The broad classes of what is consdiered "uncertainty" is not currently quantifiable in any way that is not ad-hoc. The only apsects of uncertaitny that are currently quantifiable are confidence intervals, prediction intervals, and related terms. Even then, these quantifications often only capture the uncertainty in our model or its assumptions, it ignores bias often introduced in earlier stages of the analysis. Broader and more complicated concepts such as the effects of assumptions, imputed missing variables, and model choices remain difficult to meaningfully quantify beyond ad-hoc methods.  This means that the current literature of uncertainty visualisation papers are doing one of two things:  
1) Evaluating multiple visual representations of a single mathematical expression of uncertainty (e.g. a PDF, confidence interval or error). These papers are purposefully vague, and would be clearer in their intention if the title was aligned with the actual mathematical object they are visualising. For example, a paper that is titled "evaluation of uncertainty visualisations" that only considers PDF visualisation should realistically be titled "evaluation of PDF visualisations".  
2) Evaluating multiple visual representations of *multiple* different mathematical expression of uncertainty. Since these experiments visualise different mathematical objects, some of which are closer to the ground truth statistic, the results of these papers can be easily predicted with an understanding of how to get the ground truth statistic from a plot and the perceptual tasks required to do so.   
 
It is clear that the vague definition is causing across the board misunderstandings of uncertainty, confusing experiment methodologies and motivations, and a literature that is difficult to navigate. This is also causing a large body of experiments that have easily predictable results that can be easily explained by the existing visualisation. This problem will be the focus of the following section.

- The process of working out whether or not a visualisation shows a piece of information, and if it does, what perceptual task it is mapped to and what assumptions or heuristics need to be used, is *incredibly* time consuming. While this method does predict the results for these experiments with remarkable accuracy, to do it for a large number of plots, especially when the ground truth statistics and visualisation are incredibly complicated is just not feasible for a reviewer to do.


# What is an Uncertainty Visualisation?
The lack of a consistent definition of uncertainty means that every statement that builds upon "uncertainty" will be ill-formed. This includes statements about uncertainty visualisation. This appears in the literature as a mountain of seemingly conflicting statements. For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, however at least one or both of these charts are used in most "uncertainty visualisation" experiments [] (*Cite Gap __*). 

Attempts to rectify these conflicts always flutter about the surface and do not seem to grasp the core of this problem. For example, @wu2023rational noticed a series of papers with poorly defined tasks and suggested comparing study participants to a "rational agent" to help authors understand and remedy this issue. @Kim2019 believed an understanding of participants prior beliefs could assist in interpreting uncertainty visualisation results that tend to be incredibly noisy. What these suggestions fail to understand is that the entire practice of identifying "true" facts about "unertainty visualisation" is futile. It is impossible to make a statement about the properties of an "uncertainty visualisation" in the same way it is impossible to comment on $\frac 0 0 = 1$. Division by zero is undefined, so the statement is ill formed, similarly, "uncertainty" is not defined, so statements *about* "uncertainty" including statements about "uncertainty visualisation" are ill formed. Ill-formed statements can neither be true nor false and cannot be part of a logical argument, so experiments and research that build upon this ill-formed concept are neither true nor false, they simply exist. Someone who is less familiar with the rules of logical argument may beleive this understanding of the problem to an insufferable nitpick that creates impossible to live up to standards, however the consequences of this definitional issue are *very* real and costly.  The studies that investigate "uncertainty visualisation" may be identifying *something*, however there is good reason to believe that *something* is neither interesting nor does can it contribute to a larger theory of visualisation. This is something we illustrate in the following this section. 

## Common misconceptions about uncertainty visualisation

### Defining "uncertainty visualisation" as one word

> Edit Notes: Similar to the "similar works" section, I think this section would be better off with a short sentance about each paper and comments on their similarities because a paragraph for each paper is too much

There are some taxonomies of visual uncertainty. These taxonomies seem to seek to define "uncertainty visualisation" and a combined term rather than as two separate words. We largely disagree with this conceptualization of uncertainty visualisations as it blurs the line between the mathematical estimations and assumptions (the uncertainty part) and the visual depiction of those mathematical objects (the visualisation part). While we will not focus on these taxonomies, it is still important to mention them and highlight the reasons they are not good practice.

There are a small handful of taxonomies (or typologies) that are specifically for uncertainty visualisations. @Kinkeldey2014 identified give categories of uncertainty based on the work of other literature reviews of uncertainty visualisation. The five categories are: (1) explicit/implicit (directly mapping or showing multiple outcomes) (2) intrinsic/extrinsic (using existing symbols e.g. colour value, or new objects e.g. grids) (3) visually integral/separable (can or cannot be separated from the data) (4) coincidence/adjacent (if data and uncertainty are in integrated or separate views (5) static/dynamic (animation/interaction). Of the groups they identified, they actually only used (4), (2), and (5), left out (1) because most visualisations are explicit, and (3) corresponds to (1) in most cases [@Kinkeldey2014]. 

@Potter2012 organised several existing uncertainty visualisations into groups based on the dimensionality of the data (1D, 2D, 3D, and No Dimension) and the dimensionality of the uncertainty (Scalar, Vector, Tensor). However, because the term "PDF", a statistical object that describes a random variable that is typically a one dimensional function, is used to describe both the data and the uncertainty for all dimensions, it is hard to understand how this dimensionality should work.

@Grewal2021 created a taxonomy that mapped uncertainty visualisations to some point in a 2D space that was defined by the axis "domain expertise" and "continum of discreteness" that scaled from "point estimate" to "continuous distribtion". 

@Thomson2005 titled their paper "A typology for visualizing uncertainty" however it is a taxonomy for the specific types of uncertainty that we may want to visualise and a way to calculate that uncertainty. While this paper is more about mathematics than visualisation, it is clearly written as a precursor to visualisation.

![Example of different uncertainty visualisation taxonomies](paperscreenshots/collage2.jpg){#fig-location}

These taxonomies are useful in the sense that they often identify areas where there is an over or under supply of uncertainty visualisations. Additionally taxonomies do highlight a concept of uncertainty dimensionality that was not directly covered in the uncertainty taxonomy, however the distinction between uncertainty as a single value versus as a PDF could be considered a sub-consideration of uncertainty that's level is statistical so it can be quantified. While uncertainty visualisation taxonomies are not the standard practice (generating a taxonomy of uncertainty and then developing methods to depict that uncertainty is) [@Kinkeldey2014], it is still important to acknowledge the existence of "uncertainty visualisation typologies" and why they may miss the mark in what is needed from the uncertainty visualisaiton literature. 

The first issue with these taxonomies is that most of these taxonomies are created by observing existing uncertainty visualisations. This means under researched gaps in the taxonomy could be due to a lack of need just as well as a lack of attention, and the benefit of trying to create visualisations that land in uncharted areas of the taxonomy are unclear. 

The second issue is that there does not seem to be anything these taxonomies offer that would not be better established by separate taxonomies for uncertainty and visualisation. These taxonomies seem to depict uncertainty visualisation as a field within itself, rather than a specific case of information visualisation that requires more attention to mathematics than usual. The difference between "uncertainty visualisations" and "data visualisations" is not technically in the visual element, it is mathematical. @Kinkeldey2014 almost acknowledges this in their own paper that discusses an uncertainty visualsiation taxonomy when they claim "future typologies should take different categories of tasks into account (1) communication tasks (2) analytical tasks (3) exploratory task", a common typology for information visualisation in general. The process of understanding and estimating uncertainty requires knowledge of the data, the statistical methods used to make an estimate, and the assumptions of a model. Visualising the statistics that represent uncertainty should be no different than depicting the statistics that represent any other element of a graphic, and therefore there seems to be no reason to have a separate taxonomy for uncertainty visualisation. 

### Misunderstanding the role of contextual information
> Edit Note: This section should probably be moved to the visualisation specific section, although that depends on the tone that I end up with after the rewrites of the earlier sections.

Some uncertainty work, specifically uncertainty visualisation, focus on the context surrounding the uncertainty. These are things like spatial data, temporal data, category data, etc. While other discussions of uncertainty may consider this distinction more relevant to their work, here we will consider this contextual information. Contextual information is important for interpretation and understanding of graphics, but it does not in of itself generate uncertainty. This is not unique to the properties of spatial information, other statistics information that is important for modelling and assumptions, such as non-linearity, class levels, etc do not contribute to uncertainty. Operations on data, inherrent randomness, and assumptions that create bias generate uncertainty, which is what we are talking about here. A confidence interval is a confidence interval whether it came from spatial data, temporal, or cross sectional data.

Papers that stress the importance of spatial or temporal contexts may be conflating contextual information with uncertainty. @fig-spatial depicts the role of contextual information in graphics, as something that helps us understand the information in the statistical graphic. When we create statistical models and quantify uncertainty, the spatial or temporal aspects are often boilled away [@Otsuka2023], and the only element of it that remains is the inexchangeability of our data. For this reason, there realistically is no such thing as "spatial" uncertainty in the same sense that there is no such thing as "photograph" uncertainty or "coin toss" uncertainty or "baby birth" uncertainty. Maps and photographs boil down to the same statistical objects in the same way that coin tosses and baby births boil down to the same statistical objects. Once we have removed the physical nature these objects are near identical when considered statistically. Of course the nature of them can change based on what we choose to observe (i.e. if we consider a coin landing on its side to be an outcome or if we consider intersex babies as an outcome in births) but this means the contextual information of our data, e.g. whether or not it is spatial or temporal, is only of importance at two stages of the data analysis pipeline. It is first important when we *boil it down* to our data through *observation* and again when we *map our statistical information back to its real world information* to *communicate* it. The only way the context uniquely contributes to uncertainty is in the "boil down" stage, however spatial or temporal uncertainty never refers to that specific cause of uncertainty. @Kinkeldey2014 discussed how uncertainty  can be represented by three components attribute (what) position (where) and temporal (when) and that studied typically deal with uncertainty around attribute but rarely position and time, however it is never specified what considerations should cause attribute uncertainty to be different to position or temporal uncertainty. @Kay2016 did an experiment that showed uncertainty around bus arrival times, however the visualisation used in the experiment, shown in @fig-bustime, is indistinguishable from most work that would be considered "attribute" uncertainty. 

![The uncertainty visualisation used by @Kay2016 to show the uncertainty around a bus arrival time prediction.](bustime.png){#fig-bustime}

It is typically in reference to measurement error, data aggregation, statistical modelling, or other steps that occur *after* the boil down stage, and are therefore not unique to spatial or temporal data. While the spatial or temporal features *may* become relevant at the final communication stage, however this has little do with quantifying uncertainty, and the difficulties in communicating a complicated statistical model through a complicated map is already well discussed in the visualisation literature, and in later sections we will discuss how this concept is related to uncertainty visualisation. 

![Data pipeline with visualisations to show why we are ignoring contextual information for the time being. The visualisations that depict the data are similar to typical statistical graphics. The purpose of the spatial information is to provide context to the information we extract from the statistical aspect of the graphic.](spatialinfo.jpeg){#fig-spatial}

This does not mean we are saying spatial and temporal information is never of importance. Spatial information is *highly relevant* when we translate from the abstract domain back into real world for communication. This position is best explained by @Leland2005 in *The Grammar of Graphics*:

> "Geography is anchored in real space-time and statistics in abstract dimensions. This is a distinction along a continuum rather than a sharp break... but this difference in focus clearly means that a system optimized to handle geography will not be graceful when dealing with statistical graphics."

This paper focuses on uncertainty, specifically uncertainty that is still in the the abstract *statistics* domain. Since we are still in the abstract domain, things that are anchored in real space-time create confusion that is evident in the literature. Spatial and temporal uncertainty papers limit how they communicate their uncertainty because they are considering what can be visualised inside the context provided by maps and, not what they should *try* to visualise based on the uncertainty abstraction. Additionally, since uncertainty is defined in the abstract domain, of the issues faced by spatial uncertainty visualisations are likely shared by other graphics that only differ in contextual information.

In this sense we are not proposing that the current conceptualisation of data according to its physical properties is necessarily wrong, or that it leads to conclusions that are not necessarily specific to spatial or temporal data, and it may lead us to miss relevant information that is outside our domain. This is not only the case for uncertainty data. @Slingsby2023 discussed using a gridded glyphmap for spatial modelling, however the statistical elements are identical to a fluxuation diagram depicting $f(infection|time, age, longitude, latitude)$ presented in @Wickham2011. The plot by @Slingsby2023 is simply a special case of the graphic by @Wickham2011, where a separate visualisation is made for each time point, the continuous function $infection | age$ has replaced the categorical variable $happy$, a fade is added for the population size, and a map is shown in the background. @Slingsby2023 presents other maps in the paper, however they are also special cases of the product plots framework. This is not to say this extension is uninteresting, on the contrary we believe identifying cases where particular graphics are useful is a worthwhile endevour, but these plots seem to be have been developed unaware of the existing statistical framework. When we separate the field of visualisation along lines that may not be of particular relevance, we increase the likelihood of "reinventing the wheel" in each sub-dicipline.
 
```{r}
#| output: asis
#| echo: false
img_files <- fs::dir_ls("prodplots", glob="*.png")
cat("::: {layout-ncol=2}\n",
    glue::glue("![]({img_files})\n\n\n"),
    ":::",
    sep = ""
)
```

## Uncertainty Visualisation is not defined

> Edit notes: The "in order to visualise uncertainty you need to quantify it and the quantification does not align with the definition" point could also work here since this section is now about value extraction.

Visual inference can be seen as a process that combines multiple fields, from mathematics to psychology, to convey meaning. Unfortunately which tasks belong to mathematics, computer science, or psychology is poorly defined. The uncertainty visualisation taxonomies make the blur between these fields. A visualisation is, in a lot of ways, a statistic, or at the very least statistics are calculated prior to the visualisation being calculated, however this may be invisible to the user. A density plot does not just "appear" from the data, usually a smoothing function generates an estimated density function, which is then depicted on a plot using a line, which is then converted into information in your head using perceptual tasks and heuristics.  These overlapping but distinct fields create a high burden of spanning expertise to understand if two visualisation are equivalent in each field. This is largely caused by the concept of "information" differing dramatically between fields. According to mathematics, a sufficient statistic for a parameter contains the same amount of information as an entire sample. Therefore, if you are trying to estimate a population mean, the sample mean and the entire data set are both equivalent, however a similar statistic, such as the median, is not and will become a worse and worse approximation for the mean the more skewed your underlying distribution in. Computer science considered two pieces of information to be equivalent if they come from the same data. This definition of information is much looser and is contained within the mathematical definition of information. Thanksfully, we can turn to the grammar of graphics to try and organise and untangle the information in our graphics.

When creating a graphic there are several tasks that must be completed in a specific order, regardless of whether or not we are working with a uncertainty visualisation or not. @Leland2005 depicts these steps as a pipeline, shown in @fig-pipeline. This pipeline shows every step from data observation to rendering of a graphic, but we are going to focus on the tail end of the pipeline specifically on statistics, geometry, coordinates, and aesthetics. 

> Edit notes: Grammar of graphics is just a placeholder, Di suggested to replace this with my own pipeline visualisation based on some data wrangling papers.

![The grammar of graphics data analysis pipeline](grammarofgraphicspipeline.png){#fig-pipeline}
 
In order to visually communicate uncertainty we must:  
1) Mathematically quantify uncertainty in some way.   
2) Visually express this quantification using some geometry, axis, aesthetic, and rendering.  
3) Interpret the visual expression to extract the information using perceptual tasks.   

Each of these steps have a large amount of research independent of the "uncertainty visualisation" subfield. There are numerous ways to depict uncertainty mathematically. Examples of mathematically defined objects that are used to express uncertainty inlude: confidence intervals, sampling distributions, bias, variance, prediction error, value ranges, quantiles etc. These definitions all exist in mathematics independent of the field of visualisation. Once we have some quantification of uncertainty, we need to express it visually. There are numerous options in combinations of geometry, axis, and aesthetics and modern graphical software will allow most of these combinations. There is also a large amount of research in information visualisation and psychology into how the choice of visual expression impacts a persons ability or ease with which they extract relevant information. In this sense, once we have a mathematical expression of uncertainty, the visualisation of that uncertainty is identical to the visualisation process of any other variable, or at least this is how it is treated in the uncertainty visualisation experiments.

There are several frameworks that the uncertainty visualsiation problem is approached from. One approaches it from the concept of risk, and tries to answer how to best express uncertainty in a visualisation such that the viewer can acurately extract and interpret the risks involved. Another approach is to consider uncertainty visualsiation as a specific sub-field of high-dimensional data visualsiation, and therefore uncertainty is just another variable that needs to be expressed.

### Uncertainty as signal

This distinction is important because lower levels of the pipeline influence higher levels. Visualisation authors are almost unanimous in commenting that the information in two plots must be the same in order for the visual techniques to be compared [@Cleveland1984; @Kinkeldey2014] *this citation has way more, check confirmation*. @Kinkeldey2014 adopts a definition from the 80s that suggests two graphics are informationally equivalent if all the information in one plot is inferable from the other and vice versa, and computationally equivalent if that information can be extracted from both plots with similar easy and speed. This definition illustrates a similar idea to the grammar of graphics pipeline, shown in @fig-pipeline. First the mathematical information in a plot is established, and then that mathematical information is visually represented. Two plots that depict mathematical information that cannot be inferred from each other, cannot be compared on their visual elements. To prevent this requirement from being *too* limiting, when using a ground truth statistic we can consider two plots to be informationally equivalent if both visualisations depict a sufficient statistic. For example, a depiction of a sample and its average are equivalent if we are trying to infer the population mean. This does not mean they are computationally equivalent. Additionally, two plots cannot be compared on their visual elements if they contain different information as well. The reason for this is obvious. If you compare two graphics and one depicts the mean and the other depicts a median and you ask viewers to extract the median value, the graphic that *actually* depicts that statistic will perform better. People cannot percieve information that is not depicted.

Computational equivalency in visualisation is also something that has been well researched. For simple tasks such as value extraction, there is a hierarchy to perceptual tasks where extracting visual information in some forms is easier than others. The hierarchy established 40 years ago by @Cleveland1984 is:  

1) position along a common scale. 
2) position along a non-aligned scale. 
3) length. 
4) angle/slope. 
5) area. 
6) volume. 
7) colour  

> Edit notes: need to add the notes from the susan lit review that has the more recent perceptual task list

This hierarchy has had additions and adjustments over the years. For example, this hierarchy is a general rule, but the hierarchy can change from individual to individual. Additionally, there are other graphical rules to consider such as guestalt heuristics, broader methods of extraction, and attention principles. These established visualisation concepts allow us to anticipate the ease with which certain pieces of information will be extracted from a plot. 

A bizarre feature of uncertainty visualisation literature is that it does not work to build upon these existing principle or identify the ways in which uncertainty visualisations may diverge from these rules, but rather it seems to exist with complete indifference to the research in information visualisation at large. The most common uncertainty visualisation evaluation study asks participants to perform some value abstraction either explicitly by asking participants to report values or vaguely by asking participants to find regions of least certainty, rank plots, report their confidence, and/or make decisions according to uncertainty information. Experimenters then compare these results based on the participants accuracy, preference, confidence, and/or speed @Hullman2016. Most commonly, these value abstraction papers compare two different visualisations of uncertainty on a set of these questions, however, since there is no rules for what is or is not an uncertainty visualisation the graphics that are compared often differ in both computation *and* information. @Hullman2018 asks participants questions that would require a prediction interval but provided a sampling distribution and argued that the two could be considered equivalent so long as participants understood the difference between a confidence interval and a prediction interval, knew the sample size, and knew the formula to convert the confidence interval to a prediction interval. The authors themselves note that this is rediculous, which begs the question. Uncertainty visualisaiton papers only seem to require that the plots depict "uncertainty" which has already been established to be poorly defined. The complete lack of formality is easy to see if we actually plot the visualisations compared across evaluation experiments according to the grammar of graphics *in the figure below*:

> Edit note: need to include a graphic that illustrates the wide spread of mathematical information and visual techniques that are visualised in these experiments.

Reading the uncertainty visalisation literature will make the importance of information and computation incredibly salient. If plots that have different information are compared, whichever directly depicts the ground truth statistic (or has information that can easily be convered to that ground truth statistic) will perform the best *cite*. If plots that have the same information are compared, the plot that requires a perceptual task that is higher on the hierarchy will perform the best *cite*. Therefore, when it comes to any task that involves value abstraction, many uncertainty visualisation evaluation studies are simple repeating already established results in visualisation.

Several uncertainty visualisation literature reviews have commented on the immense difficulty of combining results across experiments, but none have been able to pinpoint the reason. @Kinkeldey2014 suggested a task based taxonomy that organised the plots by the question or purpose of the visualisation, and even suggested that communication tasks (such as value retrival of uncertainty values) should just follow traditional cartography rules, @Spiegelhalter2017 simply ignored the existence of uncertainty visualisation as a subfield and just suggested we follow established rules within uncertainty visualisation, @Hullman2016 said that commented on the large amount of noise generated from uncertainty visualisation evaluation methods that led to a difficulty in combining results, @Griethe2006 did not meaningfully combine the work and instead listed findings in the field, but also did not comment on the lack of generalisable results. While we largely agree with the difficulties of these authors, we do not agree that the solution is a task based taxonomy or better questions. While we agree the lack of attention to tasks and appropriate questions has generated problems in the field, there is a much more pressing issue, which is that uncertainty visualisation authors don't seem to understand *why* uncertainty visualisation different to typical visualisations.  

The class of "uncertainty visualisation" does not seem to have any rules for what belongs in the group and what makes it different to a "typical" visualisation is rarely discussed. Instead one of two general motivations for the field are provided in the subtext.  
1) Uncertainty is fundamentally differently to other variables due to psychological heuristics involved in the interpretation of uncertainty (e.g. risk aversion).
2) Uncertainty is not of interest in of itself and is typically layered on at the end of a the visualisation pipeline, so uncertainty visualisation is the field of adding error information into already established graphics.  
Neither of these motivations are remotely relevant when we are evaluating the simple task of value extraction. The psychological effects of uncertainty are only relevant for decision making and have nothing to do with value abstraction, and the information overload issue is not specific to uncertainty and is simply an example of a larger dimensionality issue.
Therefore, these explanations for the difference fall short of explaining *why* uncertainty should be treated any differently than any other variable. 

There is also the common view that uncertainty as a dimensionality problem. This perspective is not quite right and the literature seems to be about 50/50 on whether uncertainty is a psychological or dimensionality problem. @Kinkeldey2014 also identified this issue when they highlighted that the literature makes it unclear if uncertainty is a variable in of itself, something that should be interpreted with the main variable, or if it is metadata. It is important to keep in mind that just because information is in a graphic, that does not mean it will be "seen". The phenomena of inattentional blindness shows that there is no perception without attention and it is powerful enough that participants can fail to be aware of random objects appearing on a screen or a gorilla walking through a basketball game [@simons1999gorillas; @mack2003inattentional]. Therefore if information is mapped to graphical elements that are so low on hierarchy they can be ignored, they might as well not be there at all. The people who don't plot uncertainty because they think its unimportant and the people who relegate uncertainty to the lowest ranking aesthetic on the list are both saying the same thing, "I think the uncertainty is unimportant". Including uncertainty is worth very little if no attention is left to see it. This does not mean we cannot put a *large* amount of information in a graphic. Glyph maps can be used to depict the multi-dimensional information in spatial-temporal data by mapping line plots to map locations, but we still need to decide what information is important. The map can either present trends in the global or local variance depending on whether or not the line plot is scaled globally or locally [@Wickham2012], however smaller details are almost impossible to convey. No matter how much information we try and put in a plot, there will always be only a handful of key takeaways.

This issue has come up in many different forms in the uncertainty visualisation literature. Typically as a "task dependence" on the performance of uncertainty visualisations. It is repeatedly identified as a problem in previous reviews of the uncertainty visualisation literature [@Kinkeldey2014; @Hullman2016] as well as across many sub domains and applications [@Wallsten1997; @Fischhoff2014; @Meng2021; @Amar2005]. The fact that this conclusion is repeatedly reached shows both the importance and the lack of acknowledgement this concept receives. @Fischhoff2014  takes this approach by considerting how decisions with different levels of flexibility should have different uncertainty visualisations. If the user has no choice they should simply be made aware of the risk, if they have several options, the user needs to see the uncertainty in each, if they need to come up with the options themselves, they need to see how things work and how the system fails. @Kinkeldey2014 suggested that the nature of the task plays an important role and believed the task dependence for uncertainty visualisation could explain some of the inconsistency in outcomes. 

Despite these results, it is clear that authors intuitively *feel* that uncertainty is different even though this difference has not been apparent in existing uncertainty visualisation experiments. @Kinkeldey2014 also noticed that it is not clear whether or not uncertainty is just another variable, as many value extraction papers treats it as such in their experimental design and uncertainty may need to be in a variable class of its own. @Hullman2016 commented that it is straightforward to show a value but it is much more complex to show uncertainty, but did not explain *why* uncertainty information faces more difficulty. After all, confidence intervals, minimums, maxmimums, variance, and any other statistic that can be considered related to "uncertainty" is just that, a statisitc. There is nothing special about these statistics that should differentiate them from a mean or median and imply their visualisation should be "special" in a way that would warrant its own field. Especially because it seems as though (at least in the case of visual inference) they *are* no different from any other value. So, if we cannot see any difference to uncertainty information when it comes to value abstraction, *when* is uncertainty information different?

# Noise and Signal
## Uncertainty versus Risk
Uncertainty is noise. This may be an obvious statement, but what is not obvious is that the existing literature sees noise as a signal. When we directly ask "What is the varinance of this variable" the variance *becomes* the signal. Noise and signal are not an inherrent property of variance, or meta-data or the aspects of an analysis that are usually secondary considerations, it is a property of the data that *promotes* a message we are trying to infer and the data that *supresses* it. The replication problem in uncertainty visualisation, where experiments are simply repeating established visualisation work, comes from the misunderstandings established in previous sections. Uncertainty is poorly defined, therefore authors do not understand it is not an inherrent property of the data and is actually related to a particular inference question, this incorrect view that uncertainty is always variance or probabilities leads to the class of "uncertainty visualisations" that depict different mathematical objects or visual features with little in the sense of consistent rules, this complicated class has caused visualisation authors to miss that they are simply repeating the established evaluation results from normal information visualisations. This situation is a bit of a mess. It also highlights a unique and facinating problem faced by uncertainty visualisation. If asking direct questions about uncertainty causes us to treat it as a signal, how do we evaluate uncertainty as *noise*?

Uncertainty evaluation studies are not all value extraction. There are cases where authors *know* the uncertainty information is relevant, but are not quite sure how to capture it. These studies most commonly evaluate a visualiation on its ability to convey information for a decision or how well it induces trust. The motivation behind these experiments are obvious, if we cannot measure uncertainty *directy* maybe we can measure its secondary affects. The issue is that in order to say if some plots are *better* or *worse* than others, is to ascribe value to uncertainty information. 

> Edit note: this section needs a huge rewrite. The idea of this section blends way too much with the measurement discussions in the "noise and signal" section. I have already taken paragraphs from here and just pasted it in the noise section so it might not be coherrent. Additionaly this section goes on SEVERAL long rambles that realistically should have been a sentance or not included at all. 

It would be impossible to discuss uncertainty visualisation without mentioning the broader conversation around uncertainty communication. The conversation around uncertainty communication is often centered around the psychology is communicating risks.



> Edit note: Currently these notes are just from spiegelhater because this is the only paper I have moved notes over from. Other sources will be used here lol. Working out what exactly they are communicating is a bit of a pain though.

Risk and uncertainty are not only different in how they are communicated, but also why they are communicated. Risk communication focuses on communicating probability around events. These events are typically a binary event that is known to be random, so a version of communication that does not mention risks (e.g. telling someone they will or will not get cancer instead of communicating their risks about it) inherently carries with it a lack of transparency. Additionally risk communication is often communicated to avoid or information about an unfavorable outcome, so framing can be important if we want to influence decisions in a certainty way. These motivations do not extend to the communication of uncertainty information more broadly. While uncertainty information *is* often used to improve decisions, it seems the main motivation in its communication is to offer transparency to the viewer about a statistical analysis. That is, it acts as a form of statistical "hedging" for signals found in an analysis. Interviews with experts in statistic back up this primary motivation, as ignoring uncertainty information of often expressed as being similar to fraud or lying and the goal of "improving decisions" is only seen as a secondary outcome of an appropriately hedged signal [@Hullman2020a; @Manski2020]. This concept of uncertainty means that the work on communicating risk does not cover the field of uncertainty communication as a whole. It covers the specific case where the signal itself *is* uncertain, but this work does not naturally extend to the goal of hedging a signal (i.e. when we want to communicate uncertainty as noise). 

Unfortunately understanding the relevant noise seems to be its own problem, as statisticians and visualisation authors seem to be unaware or confused in how to calculate the uncertainty related to an analysis [@Hullman2020a]. Additionally, communicating uncertainty such that it conveys the appropriate singal supression involves combining uncertainty from multiple different sources, something that is considered to be difficult by even experts in uncertainty communication [@Spiegelhalter2017].

There are some elements of risk communication that seem to translate to uncertaitny communication more broadly. Using words to communicate risk is discouraged because people can misinterpret the actual risk involved, so when possible numerical estimates of risk should be communicated, however this can make information harder to understand for those with poor numeracy skills [@Spiegelhalter2017]. People performing better and preferring with numerical estimates of risk translates to uncertainty communication more broadly *inc (citations for this)*. Risk communication also needs to have clear objectives, use plain language, limit information to only what is necessary, and segment the audience to allow for differences in interest and knowledge [@Spiegelhalter2017]. These general concepts of communication also extend to communication of uncertainty. On the other hand, it is not sensible to assume that probability specific concepts such as framing, intuitiveness of probabilities, or reference classes are still relevant when we consider uncertainty communication as signal supression, not as a synonym of risk or probability communication. In this same vein, most uncertainty communication papers very often discuss risk-aversion as something that needs to be considered despite it being unclear exactly how risk-aversion is related to uncertainty communication *(citations for the papers that do this)*.

In the same sense that risk and uncertainty are different, risk-aversion and amiguity-aversion (which can be considered the uncertainty version of risk aversion) are also not the same. Risk-aversion is the tendency of people to prefer outcomes of low uncertainty to those outcomes with high uncertainty despite the outcomes with high uncertainty having a higher or equal expected outcome. In the case of communicating risk, the only reason this heuristic would be relevant was if risk-aversion was a mistake that needed to somehow be corrected for, rather than a heuristic that *reveals the utility of decreased uncertainty*. Risk-aversion is often treated as a mistake or something that should be avoided by the uncertainty visualisation literature even though that is not necessarily true, depending on what is causing the risk-aversion (e.g. is it due to poorer estimates or increased awareness of negative outcomes). If a particular communication method leads people to place a greater value on certainty or make worse estimates, that is the aspect of importance, not risk-aversion which conflates these factors. The work that advocates for transparency in risk-communication gives you whiplash when it's subtext argues that transparency leads to "incorrect" conclusions. Risk-aversion has a large number of confounding factors that make it difficult to understand if it is something to be considered in risk communication, ambiguity-aversion has a similar problem. Ambiguity-aversion is the tendency of people to prefer to take on certain risks rather than unknown risks. It is essentially risk-aversion applied to risk. For this reason discussions of ambiguity-aversion have the same issues as the conversations about risk-aversion, where they do not consider ambiguity-aversion to be an estimate of the utility of decreased uncertainty, it contains a subtext that argues against transparency, and it is considered a valuable insight in of itself despite conflating many factors.

The idea that risk and ambiguity aversion reveal the utility of decreased variance seems to seldom be considered in the literature, therefore it should be mentioned here, otherwise this issue might continue. However, considering risk aversion is at its core, a trade off between bias and variance, it is bizzare to assume it to be a mistake when statisticians perform these trade offs all the time. If risk-aversion is truly "irrational" behaviour, every statistical textbook that discusses the use of a biased but consistent estimator in a finite sample case (a common example would be specific cases of the maximum likelihood estimator) should be tossed out. Discussing risk and ambiguity aversion as though it is some diversion from a perfectly rational, mathematical choice, implies there is *no value to a decrease in variance which is not even true within mathematics*. There have been other discussions on the appropriateness of discussing risk-aversion as a bias [@Vranas2000], but few have made the point that if uncertainty holds some *value* there is not technically a "right" decision at all. The concept of risk aversion treats risk as "noise", as though it is something that only exists to be ignored, but if the common place use of biased MLE's is an indication of anything, it is that *ignoring uncertainty* is the irrational choice.

While this is a focus on uncertainty visualisation, these issues in uncertainty communication need to be acknowledged because they bleed into the uncertainty visualisation literature. Uncertainty visualisation are compared on their ability to communicate explicit risk, elicit trust, and prevent risk or ambiguity avoidance, continuing these issues in uncertainty communication at large. While uncertainty visualisation can make it easier to communicate the complicated details of uncertainty information, it carries some challenges that are unique to the visualisaition, specifically with respect to the "signal supression" concept. In simple estimates or verbal communication, the signal is often easy to identify because it is what we are explicitly saying. Visualisations are used in both data exploration and communication. This means what exactly is a *signal* in any particular visualisation is hard to identify, since we often let the visualisation *tell us* what the signal is. Additionally, you cannot add noise to *every single possible* signal one might take from a visualisation. Two people looking at the same visualisation might, just by chance, develop two entirely different insights. These unique and facinating challenges that are faced by viewing uncertainty visualisaiton through the lense of "signal supression" have been almost completely untouched by the literature. 

What is meant by "uncertainty" may seem obvious to some, but when you attempt to quantify or visualise it you will quickly find yourself asking, "uncertainty about... what?". Do you mean uncertainty on an estimate? On a forecast? How many steps ahead is this forecast? Are we only considering the uncertainty in the estimate or in the parameters or are we considering the possibility of measurement error or biased inputs? Signal and noise can only be untangled in the presence of a motivating question.

The previous sections of this paper established several issues in uncertainty communication at large. In this section, we will discuss the current literature on uncertainty visualisation, how the uncertainty communication issues more broadly manifest in the specific case of visualisation, and what general lessons we can take from the existing work.

## Decision Making

Decision making questions that typically involve uncertainty discuss the concept of "risk-aversion" as though it is a mistake. The irony of an uncertainty visualisation pper expressing the importance of uncertainty visualisation, but setting up a ground truth or optimum strategy in which participants *should* ignore the uncertainty information is not lose on us. This perspective does not make sense because it implies participants should make decisions entirely based on the average. A possible soltion for this is to provide a utility framework for a particular experiment [@Hullman2016], however it is unclear how easily participants could adopt a utility framework that is different to their own. @Cheong2016 introduced a very simple optimum strategy into their experiment on bushfire uncertainty visualisations, where participants should choose to evacuate a house as soon as its likelihood of catching fire is 50% or higher. Interestingly, despite this very obvious and simple tactic to maximize payout from the experiment, it seems like many participants did not adopt it, instead acting much more warily as though they were considering whether or not they would *actually* evacuate in the event of a fire. These papers also very rarely consider what the optimum decision would be in a given scenario, and a difficulty with anticipating the best strategy is conflated with the failings of the uncertainty visualisation. Ultimately, we agree with @Hullman2016 who suggested that what determines an approripate ground truth is largely a philosophical exercise.

## Trust
Authors also occasionally use trust as a proxy for uncertainty. Similar to measuring uncertainty through decision quality, this method also has its own series of issues. If the purpose of displaying uncertainty information is to appropriately hedge a signal with noise, then it should be assumed that trust is only related to uncertainty communication through increased transparency and honesty. It does seem to be the case that uncertainty communication increases trust because it is a proxy for transparency *(add citations)*. Unfortunately a large proportion of the literature discusses trust as something that is directly related to uncertainty visualisation rather than as an observable by product. In viewing uncertainty communication as directly related to trust, not related through the proxy of transparency, several unobserved variables are conflated. The amount of uncertainty that is present in the information, whether or not the information is trustworthy or makes sense, prior beliefs of the participants, and the trustworthiness of the source are a few examples of variables that are conflated when authors directly consider trust to be of direct interest. Considering trust, and not transparency, as the metric of importance in uncertainty communication leads to a questionable subtext that argues against transparency. @Hullman2020a found that author simultaneously argued that failing to visualise uncertainty was akin to fraud, but also many avoided uncertainty visualisation because they didn't want their work to come across as "untrustworthy".Being concerned about audiences perception of trust, without first establishing if what we are communicating is *trustworthy*, leads to the implication that details that result in information not being considered trustworthy should be avoided. Authors that imply that a decrease in trust or an increase in ambiguity or risk aversion are metrics of importance in of themselves do not understand the purpose of uncertainty communication as methods to increase transparency. Science communication should be primarily concerned with accuracy, setting trust and risk-aversion as the variables of interest implicitly encourages statisticians to set trust and risk-aversion as the primary goals of communication. The issue of trust being divorced from trustworthiness has been commented on by other authors [@ONeill2018], however the issue still persists in the uncertainty visualisation literature [@Zhao2023]. 

Additionally, some studies use a ground truth in these trust experiments, which makes for a very confusing read on exactly how participants are supposed to use the information. 
- @Zhao2023 displayed a model prediction with uncertainty and took participants using the model prediction as a sign of trust. They reported that visualising uncertainty information caused participants to trust the model in the low variance case, but the results in the high variance case were inconclusive. The discussion made it clear the authors thought the uncertainty information should make the visualisation more trustworthy, but conflating trust and the use of a prediction implied uncertaitny information should somehow influence participants to use their own prediction. If the Beuro of Meterology was uncertain about their rain prediction, that would not be sufficient conditions for me to decide to make my own rain predictions. Despite this, the authors seemed to assume that the uncertainty information *should* have an influence on that, showing they had not deeply considered how uncertainty inforormation should infludence the choices of the participants. 

A similar measure to trust is using "confidence" in an extracted value or a decision. Interestingly, "confidence" is also used to try and capture the clarity of a message in a normal visualisation. Confidence cannot simultaneously be a measure of clarity of visualisation *and* a way to capture the uncertainty expressed in a visualisation.

## Other attemps to capture uncertainty
There are other studies that try to include uncertainty information without directly asking for it but do not go down the route of asking for a decision or levels of trust. These papers often try to ask a question that should utilise both the uncertainty and signal in the response. This is an interesting approach, but these papers are often still looking for a "signal" which is specified in the form of a "ground truth" that is used to evaluate the plot. Unfortunately this usually comes in the form of slightly cryptic questions that create a large amount of noise on the interpretation side [@Hullman2016].  
- @Hofmann2012 showed two distributions in 20 different visualisations (a lineup protocol) using a jittered sample, a density plot, a histogram, and a box plot. Participants were asked to report in which of the plots was "the blue group furthest to the right" and to provide reasoning from a multiple choice list and grade their certainty. The experiment set up is shown in @fig-right. The participants answers were then compared to a ground truth where the correct plot had a right shifted mean. By comparing the results to a ground truth statistic and marking participants as "wrong" or "right", the error from the participants that had an alternative interpretation to the concept of "furthest right" was conflated with the error from a the visualisation choice.  
- @Padilla2017 made several confusing assumption when determining the ground truth of their experiments that were assessing storm visualisations. Experiment 2 assumed that the participants should provide responses that indicated they perceived the intensity of the storm to be independent of the uncertainty distribution of its location. This assumption seems counterintuitive as best, however experiment 3 asked participants to "decide which oil rig will receive more damage based on the depicted forecast of the hurricane path" for which they *were* supposed to incorperate uncertainty information and not assume they were independent. The authors seemed to be unaware of these confounding mistakes in their assumptions of how the participants were supposed to utilise the uncertainty information.  
- @Sanyal2009 mapped uncertainty to dots and signal to a 3D surface and asked participants to identify areas of high and low signal and high and low uncertainty. Participants were not asked to combine that information in any way, and the signal and the noise were treated as separate variables.  
- @Correll2014 asked participants to extract the mean and variance from four uncertainty visualisations, bar charts with error bars, box plots, gradient plots, and violin plots (that were adjusted to make sure the mathematical information they were showing was the same) from two side by side distributions. Participants were also asked to answer a question that was supposed to incorperate both the signal and the noise, such as "How likely is candidate B to win the election?" when the two distributions indicated voter preference. Participants were not able to answer the question about likelihood in term of probability, but were instead given seven options from 1=Outcome will be most in favor of A to 7=Outcome will be most in favor of B. The ground truth statistic for this question was a scalar multiple of Cohen’s d, indicating participants were supposed to incorperate uncertainty information using a very specific formula that was likely unknown to them but assumed to be used implicitly. 
- @Cheong2016 tested multiple different visual representations of uncertainty for representing the likelihood of a house being burned down based on its location. Their payment scheme, which paid out $0.10 for a correct choice (i.e. staying when the house was not burned down or leaving when the house was burned down) and 0 for an incorrect choice (i.e.leaving when the house didnt burn down or staying when the house burned down), meant participants were incentivised to base their entire leave/stay decision on whether or not the likelihood of a fire at their house is above or below 50%. The authors failled to recognise that this "complicated decision making task" boilled down to a simple value extraction problem, which the text made easiest to extract. The results from the experiment indicated that many of the participants also failled to recognise the obvious and simply strategy. 

- @Blenkinsop2000 tailored their questions according to the visualisation shown, which implies an understanding that different visualisations contain different information. Despite this the authors still compared the visualisations according to which were "useful for uncertainty visualisation". 

- @Blenkinsop2000 did try to include uncertainty as noise, asking participants to search for a specific outcome (the land classified as grass) in a random . However the participants failing to identify the signal (something that would be expected of an uncertainty plot) was seen as a sign the task was too complicated. While the task in this experiment *was* certainty far too complicated (and poorly communicated) this discussion indicates the authors did not consider the signal suppression to be the goal of the visualisation, and rather an annoying distraction. 


- @Ibrekk1987 asked participants for the "best estimate" which was provided in terms of most probable value (mode), the mean and the median, the "ground truth" statistic used was the mean, despite the "best estimate" largely depending on the loss function of the model and the methods used.

- @Ibrekk1987 used several displays of a probability density functions, since “formally equivalent representations are often not psychologically equivalent”.

- @Ibrekk1987 highlights that in the face of a vague question, participants will use the plot to decide what the authors mean. The authors take this as a 

- @Boone2018 organised responses from participants by the approach they used and tried to account for participants prior beliefs (although the assessment of those beliefs used the word "likely" which is open to interpretation). 

There is so little understanding of information in uncertainty graphics that participants are frequently provided with information that is insufficient to answer the question at all, an issue that is regularly missed by the experimenters. In order to test whether or not people incorrectly interpreted the cone of uncertainty that is used to communicate cone tasks, experimenters regularly do not give participants enough information to answer the questions [@Padilla2017; @Boone2018]. The cone of uncertainty provides a 95% confidence interval for the *eye* of a hurricane, which allows us to know *where the eye of the storm will likely go*, this does not, give us enough information to answer if the storm will *hit* a particular point, because this requires information about the size and intensity of the storm, *and* an assumption about whether or not the size and intensity will change over time. Since these experiments are often trying to *test* for common misreadings of the plot, such as only thinking the area inside the cone will be damaged or believing the cone represents size and the storm is getting bigger. The authors often fail to recognize that the exact information that has been witheld for testing is necessary for answering their questions, and considering that information is necessary to mark participants as correct or incorrect, it is hard to understand exactly how they evaluated the experiment participants. @Ibrekk1987 expected participants to calculate the mean using a pie chart that had numerical bins combined (e.g. segment one was 0-2 inches of snow, and segment two was 2-4 inches of snow) so it could not be done by calculation. The authors also expected participants to use a CDF to calculate the mean, which needs to be converted *back* into a PDF to do so mathematically, so it was unclear how participants were supposed to do this calculation visually unless they were to just make a blind guess. The authors interpreted this as the participants misinterpreting how the CDF works, not seeming to realise they gave them an impossible task.
In light of this, the main take away from these papers seems to be "if you do not give people the necessary information to answer a question, they will be unable to answer it", which does not seem interesting enough to justify several papers.




![This shows the user interface for the experiment performed by @Hofmann2012. The question of "furthest to the right" is open to interpretation. ](furthestright.png){#fig-right} 

## Uncertainty is not hypothesis testing
It is unclear how participants are supposed to incorperate uncertainty information into their responses when uncertainty visualisation authors themselves are unsure. Other authors interpret this difficulty as a combination of the uncertainty and task complexity [@Kinkeldey2014], and not as a general confusion. All these examples sever to illustrate that *we dont know how to measure uncertainty* and if we don't know how to measure it, *we cannot evaluate the quality of a visualisation*. It could be argued that a well done uncertainty visualisations should have an imperceptible signal unless the signal would be identified with a hypothesis, but this idea also has a miriad of issues that come with it.  There is already a series of issues with the reject/do not reject concepts in hypothesis testing, so it is already questionable to set the results of a hypothesis test as the ground truth in an uncertainty visualisation experiment, however these may be additional problems that go beyond the binary nature of the issue. @Patrick2023 compared people ability to recognise patterns in a residual plot to typical statistical tests and found human viewers looking at a plot were less sensitive than the typical residual tests. This increased sensisitivity could be the result of another aspect of uncertainty that is ignored. Statstical tests are typically built upon a series of assumptions, and it is difficult to identify *which* assumption failure caused the data to fail the statistical tests. Lineup plots allow us to see *how* our data is different to the assumed distribution, and better understand *why* our data may have failled a test ^[Did Patricks paper compare statistical tests to lineup protocols in the event an earlier assumption of the tests, such as, independent observation? I think he did check some. I wonder if you can quantify the information difference you get from a rejected statistical test vs a rejected line plot]. These experiments utilised the lineup protocol which has been suggested as a method to check if perceived patterns are real or merely the result of chance [@Buja2009; @Wickham2010; @Chowdhury]. This concept bears similarity to the goal of uncertainty visualisation, but it is not quite the same. @fig-hypyvs shows the conceptual difference between the lineup protocol and uncertainty visualisation. A lineup protocol displays the uncertainty about the null and identifies if the true data plot is identifiable (and therefore significiantly difference) while an uncertainty plot displays the variance of the estimated value and assesses if the null of "no signal" is within this plot (e.g. if the error bars overlap with zero). ^[Does this flip have mathematical implications in how the plots should be assessed?] This implies a visible signal in an uncertainty plot should indicate some divergence from the null.

![Difference between the null plot vs the uncertainty plot.](hypvs.jpeg){#fig-hypvs} 

The problem of trying to assess uncertainty without knowing *why* we are depiting uncertainty has been noticed by other authors. @Spiegelhalter2017 noted we "cannot assess the quality of risk communication unless the objectives are clear".

## Suggestions for measuring uncertainty
### Types of Uncertainty Visualisation Papers

> Edit notes: Originally I was planning to have the discussion following this section relate to these "types" of papers, but that didn't end up happening. Depending on what I do with later sections, this might need a rewrite or to just be removed, since I'm not sure how relevant this is to the main ideas of the paper.

Uncertainty visualisation papers can be organised according to the *goal* of the experiment. Evaluation experiments are the standard rule for visualisations because the human brain is not as reliable as mathematical calculation. Therefore, user studies often aim to assess the limitations, biases, and heuristics of our mental calculator so that we can better understand the problems we may encounter when we plot our data. This is not to say any paper that suggests a visualisation without an evaluation experiment is completely lacking in justification, and there are many papers that suggest a novel visualisation without an evaluation study. Sometimes these papers are a preliminary step in finding a solution for common problems and intend to evaluate the visualsiation in later work. Until that later work is done, it is often difficult to accept one particular representation. While justification could come in the form of discussing established concepts in visualisation, such as the hierarchy of perceptual tasks, even these may need an evaluation study to back up their claims.

The reasoning for this is obvious. There are a large number heuristics and biases that are not obvious to us when designing visualisation. Additionally, these heuristics and baises can change depending on the larger scope of the graphic and the population we are communicating with. Additionally, since there is often a miriad of ways to visualise any particular mathematical object, to adopt specific visualisation that is not already common practice needs reasonable justification, lest we run into a range of unforseen heuristic pitfalls.

The remainder of section 4 will focus on the results and methods of uncertainty visualisation evaluation experiments. There are six main types of evaluation experiments in the uncertainty visualisation literature, they are:  
- Association perceptual tasks: These experiments identify perceptual tasks that are/could be associated with uncertainty, and test its association with uncertainty, check it's number of distinguishable levels, and/or check how quickly/accurately people can draw the uncertainty information from the graphic.  
- Value perceptual tasks: Participants are shown some depiction of uncertainty and are asked to extract a value, make a comparison, or do something simple to show that the graphic is readable.  
- Heuristic Checks: These experiments are checking if a perceptual bias that has been identified through public use of a particular plot, other evaluation experiments, or gut feeling identified through public use of a plot or in a different.  
-  Plot Comparisons: These papers describe an uncertainty visualisation evaluation experiment, where they compare two or more uncertainty visualisaitons on a specific set of questions. These questions can be about specific statistics (e.g. What is the probability that A>B?) or about a secondary goal such as trust or decision making (e.g. How much do you trust this estimate?).  
- Qualitative analysis: These experiments show participants an uncertainty visualisation for an open ended or specific task, and asks them to describe how the visualisation was used to come to a conclusion, or they are asked to describe what they see in the visualisation. 
- Method papers: These papers discuss issues with the evaluation methods of uncertainty visualisaiton and suggest changes or improvements, typically with results indicating the benefit of the change.  
  
These experiments are not mutually exclusive within a paper, and a single study will often contain several experiments, usually from several of these categories. These distinctions are important because each have their own issues in how they relate to uncertainty more broadly. Below we will go through the five key issues in uncertainty visualisation, describe how they mainfest in the uncertainty visualisation literature, and offer some tentative solutions to these problems.

We will also comment on some of the uncertainty visualisation papers that suggest a visualisation without an evaluation experiment. Some of these plots may offer a solution to the unique problems we mention in this paper and these visualisations will be discussed in "Part 6: Great Examples". We will also consider these papers to illustrate the issues that visualisations are aware of and working to fix, and discuss whether or not these directions of research are likely to bear fruit, and why.

# Great Examples

> Edit notes: this section seems to go off a bit on dimensionality of visual tasks and stuff that might be better put in the noise and signal section about measuring uncertainty.

Of course we do not believe every uncertainty visualisation is unusable and every experiment is build upon misguided assumptions. Despite the common problems detialled in the previous sections, there is some interesting work in the uncertainty visualisation space. This can come in the form of an uncertainty visualisation that identifies or fixes pitfalls in the previous sections (without identifying them) or does an experiment that identifies useful information that avoids these pitfalls.

Uncertainty that is created by steps that are further down the pipeline are often ignored by the uncertainty visualisation literature, however there are visualisations that make these considerations. If uncertainty had a more encapsulating mathematical definition, the uncertainty created at earlier stages might be able to be visualised with the model uncertainty that is typically expressed through confidence intervals. That being said, even with a definition that allows us to combine these uncertainties into a single value, it still might be desirable to see which stage of the analysis is creating uncertainty in the visualisaiton. One example of a visualisation that includes that information is visualisations of climate scenario uncertainty, as shown in @fig-climatescenario. Uncertainty is introduced into this model by the choices of individuals which is difficult to model or predict and is also unlikely to be constant over time. Therefore specific scenarios are modelled and shown, each with their own model uncertainty. This highlights a method that can be used to visualise uncertainty from earlier in the pipeline that is often forgotten or ignored via assumptions.

There are also some interesting qualitative studies that take an investigative stance to the use of uncertainty in decision making rather than an evaluation stance. @Daradkeh2015 presented participants with ten investment alternatives and asked participants "from among available alternatives, which alternative do you prefer the most", and were asked to think aloud and consider the uncertainty in their decision making. This study was qualitative and rather than setting a "correct" answer that would have forced a value of risk on the participants, the experimenters goal was to observe and organise the methods people use when making decisions in the face of uncertainty. This study was an excellent example in a useful experimental design. They highlighted the specific aspects of uncertainty that participants typically considered, such as the range of outcomes that are above/below a certain threshold, minimum and maximum values, the risk of a loss, etc, and mapped where in the decision making process participants made these considerations. 

![I will just make my own version of a climate scenario plot, this one is just here from a random site as a place holder](climatescenario.png){fig-climatescenario}


A lot of work that identifies heuristics or biases in plots do not hinge on the previous assumptions. This work also provides useful insights for experiments by highlights pitfalls participants might fall into when they review the results of evaluation experiments [@Hullman2016]. @Newman2012 found that participants were more likely to view points within the bar as more likely than points outside of the bar in bar charts with error bars. Similar effects have been identified in other types of uncertainty displayed. @Padilla2017 found that points that were on an outcome of an ensemble display were perceived as more likely than points not on an outcome, even when the point that was not on a specific outcome of the ensemble was closer to the mean of the uncertainty distribution. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015]

There are a number of visualisation methods that do take the approach of uncertainty as signal suppression. These visualisation examples try to establish uncertainty as something that should be interpreted *with* the signal and take several approaches to do so. @fig-maps depicts the array of approaches that have been used to visualise uncertainty in a map. 

The first approach to embedding noise in the signal is to adjust the signal visualisation so that it includes the noise. This method has been particularly popular with map visualisations. To visualise uncertainty such that it does its job of signal supression the uncertainty and the signal need to be interpreted in conjunction with each other. There are several suggested ways to go about this, most of which assume the "no uncertianty" version of the visualisation is a simple choropleth map. There are two methods to include uncertainty in a map. The first is to include uncertainty as error and map our signal and noise to a two dimensional *colour* space, which is the method adopted by the bivariate map and the value supressing uncertainty pallet [@Correll2018]. These methods rely on the assumption that colour hue and value exist mentally as a two dimensional colour space and that distance in this colour space is linear (or at least appears linear using our scale). However, for uncertainty to be interpreted in the visualisation *with* the signal, our brain would need to be able to flatted this two dimensional space into a single space of "signal validity", and there is simply not evidence that we can do this. This visualisation method brings up an interesting concept though. If there are two aesthetics we can use that are flattened into a single channel in our mind, then plotting measurement and error to each of those aesthetics is a viable method to visualise uncertainty. The idea of two perceptual tasks flattening into one variable in the mind of the viewer may be wishful thinking, but it is not impossible given we are not certain on how the perceptual tasks are mapped within the human brain. @Sterzik2023 found that when a value was mapped to the textures of stippling, hatching, and triangles, and found that the difference between two points on this one dimensional texture was actually a 2D space (likely "business" and light/darkness). This idea of two aesthetics flattening into one cohesive variable in the perceptual space is particularly interesting for uncertainty visualisation. 

An alternative method to visualising uncertainty in a map is to combine uncertainty and signal at the earlier stage so the "supressed signal" is represented by a single variable. This statistic can then be expressed in a one dimensional colour space, which is a method addopted by the Baysian surprise metric map [@Ndlovu2023] and the excedance probability map [Lucchesi2017]. These methods do not require us to compress colour hue and value into a single variable to interpret signal and noise as a single variable, it is done explicitly for us before the visualisaiton. These methods require more specificity in *how* the uncertainty contains important information.


::: {#fig-maps}

![](mappals/img1.png)

![](mappals/img2.png)

![](mappals/img4.png)
![](mappals/img3.png)

![](mappals/img5.png)

*I am going to replace this with my own R coded maps so they are all on the same data and signal/noise.* Four different ways to include error information in a map. A typical chloropleth map is considered the map that ignores uncertainty and simply visualises the signal. The bivariate map visualises the uncertainty, but does so using two different visual channels, which gives the perception of two different signals (or one signal in the assumptions the colors contrast with each other). An adjustment to the bivariate map is the VSUP which is similar to the bivariate plot except it merges high uncertainty values in the palette to reduce our ability to discern values that have a high enough error to not be a valid signal. The Bayesian surprise metric is similar to the lineup protocol, where the colour value is the difference between the observers posterior and prior beliefs. The exceedance probability map is a direct visualisation of the probability of a specific hypothesis.
:::

These maps make the importance of combining uncertainty and signal in a single visual channel clear. A chloropleth map will show signal that is not valid inference because of high uncertainty. At the other end of the spectrum, the bivariate map will show signal that is not always interesting because it forces us to interpret uncertainty and signal differently. The value supressing uncertainty pallet was clearly designed with hypothesis testing in mind, because it combines highly uncertain values. The main difference between the VSUP and the bivariate colour map is actually the subtle combination of signal and noise through discernability. In this sense, the uncertainty visualisation maps that combine uncertainty and signal before it is conveyed visually simplify the visualisation and make it easier to interpret value supressed signal. This is what is done by the exceedence probability map and the Bayesian surprise metric map. They are more explicit in how the uncertainty should supress the signal since they make the specific hypothesis explicit. 

Another method that has been used is to visualise samples from the distribution we are trying to draw inference on instead of estimates along with a variance or error. This has been used in maps with the pixel map [Lucchesi2017], but is more commonly used in animation with the HOPs plots [@Hullman2015] or similar concepts [@Blenkinsop2000] . The difficulty with resampling methods is that they are typically conflated with animation even though this does not necessarily need to be true, as can be seen in the @fig-pixelmap and the New York Times class mobility figure, shown in @fig-nyt, an animated version of which can be found [here](https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html). Even though @Kale2018 found that users were more sensitive to the underlying trend when shown the HOPs plot over the static outcoms plot, this effect went away when the speed of the animation increased, indicating that performance difference might be due to overplotting in the static case and a smaller sample size in the static condition would produce the same result. Therefore, so long as the sample size of the number of outcomes is kept to a level where individual samples are still visible, there is no evidence that an outcome plot *needs* to be animated. Visualisations that opt to express a signal as a sample rather than an estimate have the potential to supress signal since it is not explicitly visualised, however this has yet to be shown in evaluation experiments. This is not to say that visualisations of mass would not be able to perform signal supression, but a sample can easily be expressed using aesthetics such as colour on a map and mass visualisation often struggling with issues such as over or under smoothing. These sampling methods can show the messiness of the data that sits behind a model.

![](pixelmap.png){fig-pixelmap}
![](classmobility.png){fig-nyt}

A final method to perform signal supression is simply to visualise the data if it is available. An example of this is shown in @fig-census for racial distributions spatially in america, an interactive version of the plot can be found [here](https://edition.cnn.com/interactive/2021/us/census-race-ethnicity-map/). This map shows the typical causes of uncertainty in a spatial model, i.e. regions where data is sparse, ethnically diverse areas, uneven distribution of points within boundaries, etc, but it avoids the need to create a visualisation with a specific signal in mind. This is the technique typically employed by exploratory data analysis, which means it's lack of a specified signal means there is both *no* uncertainty (since we are technically not performing inference) but in the event we *do* implicitly perform inference, there is some hedging. In this sense, the best uncertainty visualisation you can get without specifying a signal you want to convey is visualising the data itself. The census dot map's addition of interactivity also allows users to zoom in and see the details that caused "uncertainty" in the form of inconsistent colours at lower resolutions when they were zoomed out. The raw data can also be used in a similar method to the HOPs plot, where a statistic of importance is implied by the visualisation, but the signal is supressed using the data itself instead of resampling methods. This does not mean that visualising raw data instead of implementing sampling techniques is always a valid uncertainty visualisation that will prevent insignificant signal from getting through. @Buja2009 illustrated how groups that appear linearly separable in a linear discriminant analysis (LDA) visualisation of the data can actually be the result of a LDA performed on too many variables, something that was not clear from the visualisation until the lineup protocol was implemented.

![](censusdotmap.png){fig-census}

These great examples show there are visualisations that are being made that have a good conceptualisation of uncertainty and express it as signal supression, however this work is not being done formally within visualisation. These industry visualisation are likely made with a large amount of back and forth between different levels of an organisation, something that is important for good visual communication [@Spiegelhalter2017]. 

# Future work
This paper has identified several issues in the uncertainty visualisation literature and with those issues, we have several suggestions for future work that would likely be fruitful for the field.

Our first suggestion for future work is a mathematical defintion of uncertainty. Specifically, a definition that has a place for all the concepts that currently sit under the "uncertainty" umbrella, explains how these concepts relate to uncertianty, and also allows us to quantify them in a way where they can be combined or visualised. This work would not only benefit uncertainty visualisaiton, but also statistics more broadly. There is currently a lot of work that quantifies the uncertainty through bias and variance at various stages of the data analysis pipeline separately. Elements such as imputed data, assumptions, sampling methods, and analyst decisions all have their own independent work, quantification and discussions, but methods to brand between these concepts are few and far between. A conceptual framework that allows us to combine these results would be incredibly beneficial to the field.

The concept of uncertainty should also be formalised within the grammar of graphics. @Leland2005 referred to uncertainty as "semantics", however the depth of integration required to understand problems in uncertainty visualisation suggest they *need* to be formally placed within the framework. Displaying a distribution is not as simple as displaying the data. For example, the distinction between a sample and a smothed density function is not cut and dry. One may view them both as the same data but one is represented as a point, the other is binned according to some function and then represented as a line. However, this distinction blurs as n gets larger and larger and, depending on the collision modifier used, depictions of the sample will appear identical to depictions of the mass. This is obviously true due to large sample theory, the backbone of statistics. The issue of blurred elements of the grammar of graphics in the case of distribution visualisations does not end here. Visualisations that depict mass such as a confidence interval, a boxplot, a PDF, a letterbox plot, a violin plot, a histogram, a quantile dot plot, etc, all show some version of the mass of a variable at different resolutions, but whether these veriables differ in something as low down in the grammar pipeline as the variable stage, or later at the geometry stage is unclear. The lack of formalisation in this way allows authors to compare graphics that *seem* to be similar mathematically but are actually conveying very different information. Not only would a formalisation of uncertainty visualisation within the grammar of graphics allow us to iron out some of these confusions, but they would make it easier to understand when existing visualisation methods apply and can be used to explain our results.

```{r}
#| include: false

# put a visualisation of a dot plot with increasing sample size here to show my point
```

If we are going to consider uncertainty as noise, not signal, there needs to be a way to identify this signal supression in an experimental design. It is clear that value retrival and questions with a specific ground truth are not appropriate, as uncertainty visualisation will simply follow the existing visualisation rules. Testing a secondary feature of a plot, that is, how strong a signal is becomes a little more complicated. Line up protocols have been used in adjacent work that look at the strength of the signal in a plot, and this idea of identifying if plots have some "barely noticiable differences" could be utilised. There is also the possibility that uncertainty visualisation evaluations will need to swap to a qualitative methodology where participants are allowed to freely comment on what they notice in graphics until we establish how the existence of noise can be observed.

Additionally, information visualisation needs a more precise concept of the difference between two plots, and that framework should be utilised in experiments. Other fields of science employ marginal changes when designing experiments to ensure it is well understood *what* aspect of their experiment is contributing to their results. Visualisation has the grammar of graphics, however we have already discussed how this can break down in the case of distribution visualisation. 

If a visualisation researcher would prefer to perform experiments rather than formalise methods, the task dependency many authors in uncertainty visualisation mention would be a useful direction for research. Unfortunately the current visualisation evaluation literature is *far* too noisy to allow us to identify what might be the cause of this task dependence. It is unclear if it is a conflation between task and the ground truth/displayed statistics concepts discussed earlier, or an actual difference in the way we percieve graphics. A good starting place for this idea would be to identify if there is task dependency in perceptual tasks, since all perceptual task research to date has been done on value retrival. That is, an experiment that compares things like area, position, colour and other perceptual tasks on value exaction, comparisons, and other simple tasks. It is also clear that the the number of potential tasks that can be performed on a visualisation increases with with the number of observations. A single observation is limited to value extraction, two observations can be compared, multiple observations allow for shapes or global statistics to be extracted. The interaction between sample size and task is of particular interest to the uncertainty visualisation community, as uncertainty can be expressed through multiple observations using a sample, or through a single value using an error. Of course, this is limited by the fact that there also isnt a definition for what is a "task" and given the mess created by the lack of formalisation in uncertainty visualisation, it may be wise to formalise that concept before performing these experiemnts. @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations, which could be a good starting point. Reguardless, these are directions of research would be fruitful to the uncertainty visualisation community even if it appears on the surface to be research that is only beneficial to the "normal" visualisation community.

# Conclusion
In this paper we have highlighted a series of misunderstandings, confusions, and methodical errors in the uncertainty literature. 

Many of these issues are not problems in isolation, so long as the authors themselves are aware of it and state these assumptions in their papers. Our problem with this body is work is that the authors themselves seem to be oblivious to this vast array of problems, considering not a single one of the problems discussed here are mentioned explicitly in a single visualisation paper.

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "ideas/confirmation/confirmationreport.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```