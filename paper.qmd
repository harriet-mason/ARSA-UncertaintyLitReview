---
title: "The Noisy Work of Uncertainty Visualisation Research: A Review"
author: Harriet Mason
bibliography: references.bib
date: last-modified
toc: true
number-sections: true
format: pdf
fig-valign: bottom
cap-location: bottom
editor_options: 
  chunk_output_type: console
---

<!-- TODO 
- convert to American spelling
- May not need to reference the same paper again and again
- examples that aren't maps? Sure are a lot of maps.
-->

```{r}
#| echo: false
#| message: false
#| warning: false


# load Libraries
library(tidyverse)
library(biscale)
library(RColorBrewer)
library(scales)
library(sf)
library(ggrepel)
library(urbnmapr)
```

{{< pagebreak >}}

# Background

From entertainment choices to news articles to insurance plans, the modern citizen is so over run with information in every aspect of their life it can be overwhelming. In this overflow of information, tools that can effectively summarize information down into simple and clear ideas become more valuable. Information visualisations remain one of the most powerful tools for fast and reliable science communication. 

Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. Datasets such as Anscombe's quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg] highlight this power in visualisation. Additionally, visualisations allow for efficient and memorable communication. Even something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those measures [@Hullman2018; @Goldstein2014]. 

Uncertainty visualisation is a relatively new field in research. Early papers that specifically reference "uncertainty visualisation" appear in the late 80s [@Ibrekk1987], with geospatial information visualisation literature in the early 90s declaring this to be essential aspect of information display [@MacEachren1992; @Carr1992]. These early experiments typically involved showing participants a distribution, such as those depicted in @fig-ibrekk, and asking the viewers to extract a probability or average. Despite the new terminology visualisation of uncertainty has been present since the earliest times. For example, box plots or histograms can be considered to be displaying uncertainty in the sense of variability in observations sampled from a population distribution. Today, there is an abundance of publications on the topic which makes it is timely to construct a review of the field. In fact, there have already been several reviews published. 

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-ibrekk
#| fig-cap: "A replication of the the uncertainty visualisations shown by @Ibrekk1987 in one of the earliest uncertainty visualisation experiments. This early experiment is a good example of many of the issues that are still common in uncertainty visualisation today. For example, the '95% confidence interval' is more accurately a '95% prediction interval'. Additionally graphics that depict different mathematical objects that are also different on their visual components are compared because of a percieved relation to uncertainty. So visualisations of the mean, PDF and CDF, are all discussed as though they all contain the relevant statistical information. The axis also have different scales, and visualisation methods that are now unpopular for displaying proportions, such as a pie chart, are used."
#| fig-subcap: 
#|   - "Picture 1"
#|   - "Picture 2"
#|   - "Picture 3"
#|   - "Picture 4"
#|   - "Picture 5"
#|   - "Picture 6"
#|   - "Picture 7"
#|   - "Picture 8"
#|   - "Picture 9"
#| layout-ncol: 3

# Generate data
set.seed(1)
x=rnorm(1000, 8, 4)
ib_data <- tibble(x=ifelse(x<0, -x, x))

# Picture 1
p1 <- ib_data %>%
  summarise(avg = mean(x),
          conf_95a = quantile(x, probs=c(0.025)),
          conf_95b = quantile(x, probs=c(0.975))) %>%
  ggplot(aes(y="NA")) +
  geom_point(aes(x=avg)) +
  geom_errorbar(aes(xmin = conf_95a, xmax = conf_95b), width = 0.1) +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,19),
                     labels= ggplot2:::interleave(as.character(c(seq(0,18, 2), 19)), rep("", 11))[c(0:19, 21)],
                     limits=c(0,19)) +
  theme_classic() +
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=1/10)

  
# Picture 2
p2 <- ib_data |>
  mutate(x = ifelse(x>18, 18, x),
         binx = cut(x, breaks=seq(0,18,2))) |>
  group_by(binx) |>
  summarise(n = n()) |>
  mutate(Probability = n / sum(n)) |>
  ggplot(aes(x=binx, y=Probability)) +
  geom_col(fill="black", colour="white") +
  scale_x_discrete(name = "INCHES OF SNOW",
                   labels= paste0(seq(0,16,2), sep = "-", seq(2,18,2))) +
  scale_y_continuous(breaks = seq(0.00, 0.25, 0.05)) +
  theme_classic() + 
  theme(aspect.ratio=0.33)

# Picture 3
# library(ggpattern)
p3 <- ib_data |>
  mutate(x = ifelse(x>18, 18, x),
         binx = cut(x, 
                     breaks=seq(0,18,2), 
                     labels= paste0(seq(0,16,2), sep = "-", seq(2,18,2)))) |>
  group_by(binx) |>
  summarise(n = n()) |>
  mutate(Probability = n / sum(n),
         csum = rev(cumsum(rev(Probability))), 
         pos = Probability/2 + lead(csum, 1),
         pos = if_else(is.na(pos), Probability/2, pos)) |>
  ggplot(aes(x="", y=Probability, fill=binx)) +
  geom_bar(stat="identity", width=1) +
  geom_text_repel(aes(y = pos, label = paste0(round(Probability*100), sep="", "%")),
                   size = 3, nudge_x = 0.6, show.legend = FALSE, segment.color = 'transparent') +
  #geom_label(aes(label = paste0(round(Probability*100), sep="", "%")),
  #          position = position_stack(vjust = 0.5)) + 
  scale_fill_grey() +
  coord_polar("y", start=0) +
  labs(fill = "INCHES OF SNOW") + 
  theme_void() + 
  theme(aspect.ratio=1)

# Picture 4
p4 <- ib_data |>
  ggplot(aes(x=x)) +
  geom_density() + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2))) +
  scale_y_continuous(name = "Probability density",
                     breaks = seq(0.00, 0.20, 0.02)) +
  theme_classic() + 
  theme(aspect.ratio=0.33)



# Picture 5
p5 <- ib_data |>
  ggplot(aes(y="", x=x)) +
  geom_violin() + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.4)

# Picture 6
set.seed(1)
x=rnorm(5000, 8, 4)
ib_data2 <- tibble(x=ifelse(x<0, -x, x)) |>
  mutate(x=ifelse(x>=18, 18-rexp(5000,rate=0), x))
p6 <- ib_data2 |>
  ggplot(aes(y="", x=x)) +
  geom_jitter(size=0.05) + 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.1)


# Picture 7
p7 <- ib_data2 |>
  arrange(x) |>
  mutate(group = rep(1:50, each=100))|>
  group_by(group) |>
  summarise(x = max(x, na.rm=TRUE)) |>
  add_row(group=c(0,51), x = c(0,20)) |>
  ggplot(aes(x=x)) +
  geom_linerange(ymin = 0.1, ymax = 1) + 
  geom_linerange(y=1, xmin = -0.03, xmax = 20.03)+ 
  geom_linerange(y=0.1, xmin = -0.03, xmax = 20.03)+ 
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks = seq(0,20,2),
                     labels= paste0(seq(0,20,2)),
                     limits=c(0,20)) +
  scale_y_continuous(limits=c(0,1)) + 
  theme_classic() + 
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=0.1)

# Picture 8
p8 <- ib_data %>%
  reframe(x = quantile(x, probs=c(0.25, 0.50, 0.75)))|>
  add_row(x = c(0,20)) |>
  arrange(x) |>
  mutate(quantile = c("min", "q1", "med", "q3", "max")) |>
  pivot_wider(names_from = quantile, values_from = x) |>
  ggplot(aes(y="")) +
  #geom_point(aes(x=med)) +
  geom_errorbar(aes(y="", xmin = min, xmax = max), width = 0.2) +
  geom_crossbar(aes(y="", x=med, xmin = q1, xmax = q3), width = 0.5) +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,20),
                     labels= ggplot2:::interleave(as.character(c(seq(0,20, 2))), rep("", 11))[1:21],
                     limits=c(0,20)) +
  theme_classic() +
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=1/10)


# Picture 9
p9 <- ib_data |>
  ggplot(aes(x)) +
  stat_ecdf(geom = "step") +
  scale_x_continuous(name = "INCHES OF SNOW",
                     breaks=seq(0,20),
                     labels= ggplot2:::interleave(as.character(c(seq(0,20, 2))), rep("", 11))[1:21],
                     limits=c(0,20)) +
  scale_y_continuous(name = "Cumulative probability",
                     breaks=seq(0,1,0.1),
                     labels= seq(0,1,0.1),
                     limits=c(0,1)) +
  theme_classic() +
  theme(aspect.ratio=4/10)

# Display Plots
p1
p2
p3
p4
p5
p6
p7
p8
p9
```


Reviews on uncertainty visualisation rarely offer tried and tested rules for effective uncertainty visualisation, but rather comment on the *difficulties* faced when trying to summarize the field. @Kinkeldey2014 found most experiments on the methods for uncertainty visualisation evaluation to be ad hoc, with no commonly agreed upon methodology or formalisation and no greater goal of describing general principals. @Hullman2016 commented on the difficulty in taking overarching themes from uncertainty visualisation, as several conflated issues make it unclear if subjects did poorly in an experiment because they misunderstood a visualisation, because the question was misinterpreted, or because they used a specific heuristic. @Spiegelhalter2017 commented that different plots are good for different things, and disagreed with the goal of identifying a universal "best" plot for all people and circumstances. @Griethe2006 was unable to find common themes, but instead listed the findings and opinions of a collection of papers. @uncertchap2022 summarized several cognitive effects that have repeatedly arised in uncertainty literature, however these effects were each discussed in isolation as a list of considerations an author might make. While these reviews are thorough in scope, none discuss how the existing literature contribute to the commonly state goal of uncertainty visualisation, scientific transparency. The problem faced by the literature is easily summarized with a famous quote by Henri Poincaré.

> "Science is built up of facts, as a house is built of stones; but an accumulation of facts is no more a science than a heap of stones is a house." - Henri Poincaré (1905)

That is to say, despite the wealth of reviews, the field of uncertainty visualisation remains a heap of stones. This review attempts to address this issue by offering a novel perspective on the uncertainty visualisation problem, and hopefully laying the foundations on which we can build a house. 
This review is broken into several parts that each reflect a different approach to uncertainty visualisation. First we look at graphics that ignore uncertainty entirely and discuss why uncertainty should be included at all. Second, we look at methods that consider uncertainty to be just another variable and discuss the characteristics of uncertainty that make it a unique visualisation problem. Third, we look at methods that explicitly combine our estimate and its uncertainty and discuss if the visualisations created by these transformations are still "uncertainty visualisations". Fourth, we will discuss methods that implicitly include uncertainty by depicting a sample or original data in place of an estimate. Finally, we discuss how uncertainty visualisations can be effectively evaluated. When discussing each of these methods, we consider the *purpose* of uncertainty visualisation and comment on how effective each visualisation is at fulfilling that purpose.

Due to the fields origins and focus in geospatial information visualisation, there have been a large number of suggested variations on the choropleth map that allow authors to include uncertainty. We will use these maps to provide simple examples for each approach that can be easily compared to the "no uncertainty" choropleth map to better understand the costs and benefits of each approach. Despite the example of each method focusing on variations of the choropleth map, it is important to understand that the approaches we are discussing are universal and are not unique to maps.

{{< pagebreak >}}

# Ignoring uncertainty
A good place to start might be at deceptively straight forward question, why should we include uncertainty at all? 

## The choropleth map
@fig-choropleth depicts a choropleth map of the counties of Iowa. Each of these counties are colored according to an estimate of average daily temperature that was simulated so that the values followed a clear spatial trend (hot in the middle of the map, and cold on the outside). The variance of these estimates were simulated such that a hypothesis test would indicate the existence of a spatial trend in the low variance map, while the trend in the high variance map should be indistinguishable from noise. Is this distinction in validity of the spatial trend clear in in the map? Is the validity of the trend communicated through the visualisation?

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-choropleth
#| fig-cap: "Two choropleth maps that depict the counties of Iowa where each country coloured acording to a simulated average temperature. Both maps depict a spatial trend, where counties closer to the center of the map are hotter than counties on the edge of the map. This trend is only technically there in the low variance condition, i.e. if we performed a hypothesis test we would conclude the trend exists. The high variance condition displays a spatial trend that could simply be the result of noise, which means the plot is displaying a false conclusion."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "Choropleth Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"
#| cap-location: "bottom"

# seed for sampling
set.seed(1997)


# Get map data
my_map_data <- get_urbn_map("counties", sf = TRUE) |>
  dplyr::filter(state_name=="Iowa")
centroids <- as_tibble(rgeos::gCentroid(as(my_map_data$geometry, "Spatial"), byid = TRUE))
my_map_data$cent_long <- centroids$x
my_map_data$cent_lat <- centroids$y
n <- dim(my_map_data)[1]

# Make standard Palette
# set Base colours
basecols <- brewer.pal(8, name = "YlOrRd")
#breaks <- seq(0,41, length.out = 9)
breaks <- 21:29 #c(0,15,24,25,26,27,30,35,41)
names(basecols) <- seq(8)
limits <- c(10,41)

my_map_data <- my_map_data |>
  dplyr::mutate(temp = 28 - ((scale(cent_long))^(2) + (scale(cent_lat))^(2))[,1], # trend
         notrend = rnorm(n), # no trend
         highvar = runif(n, min=2, max=4), # high variance
         lowvar = runif(n, min=0, max=2), # low variance
         ) |>
  pivot_longer(cols=highvar:lowvar, names_to = "variance_class", values_to = "variance") |>
  # add bivariate classes to data
  mutate(bitemp = cut(temp, breaks=breaks, labels=seq(8)),
         bivar = cut(variance, breaks=0:4, labels=seq(4)),
         biclass = paste(bitemp, bivar, sep="-")
  )

# Choropleth Map
p1a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = bitemp, 
              geometry = geometry), colour=NA) + 
  scale_fill_manual(values = basecols) +
  #scale_fill_gradientn(colours = basecols, 
  #                     values=breaks/limits[2],
  #                     limits=limits) +
  theme_void() + 
  theme(legend.position = "none")

p1b <- p1a %+% filter(my_map_data, variance_class=="highvar")

show_pal <- function (colours, borders = NULL, cex_label = 1, ncol = NULL, myxlab, breaks) {
  # Set dimensions of palette
  n <- length(colours)
  ncol <- ncol %||% ceiling(sqrt(length(colours)))
  nrow <- ceiling(n/ncol)
  # make matrix with null values (if not full)
  colours <- c(colours, rep(NA, nrow * ncol - length(colours)))
  colours <- matrix(colours, ncol = ncol, byrow = TRUE)
  # set graphical parameters (?)
  old <- par(pty = "s", mar = c(0, 0, 0, 0))
  on.exit(par(old))
  size <- max(dim(colours))
  plot(c(0, size), c(0, -size), type = "n", xlab = "", ylab = "", 
      axes = FALSE)
  rect(col(colours) - 1, -row(colours) + 1, col(colours), -row(colours), 
       col = colours, border = borders)
  text(c(0,col(colours)) + c(0.2, 0.1, 0,0,0,0,0,-0.1,-0.2), -c(1,row(colours))-0.25, breaks, 
              cex = 1, col = "black")
  text(4, -1.75, myxlab ,cex = 1.5, col = "black")
}

p1a
p1b

show_pal(basecols, ncol=8, borders=NA, myxlab = "Temperature", breaks = 21:29)

```

## Signal-supression
The two choropleth maps appearing to be identical in @fig-choropleth highlights the need for uncertainty visualisations. Uncertainty visualisation is is required for transparency and this sentiment has been repeated.  Some authors suggest uncertainty is important to include as it communicates the legitimacy (or illegitimacy) of the conclusion drawn from visual inference [@Correll2014; @Kale2018; @Griethe2006]. Some authors have said that uncertainty should be included to degree of confidence or trust in the data [@Boukhelifa2012; @Zhao2023]. Some authors directly connect uncertainty visualisation to hypothesis testing as it ensures the "validity" of a statement [@Hullman2020a; @Griethe2006], but allows for a proportional level of trust that is more detailed than the binary results of a hypothesis test [@Correll2014; @Correll2018]. Some authors even go so far as to claim that failing to include uncertainty is akin to fraud or lying [@Hullman2020a; @Manski2020].

This consensus leads us to understand that uncertainty visualisation is motivated by the need for a sort of "visual hypothesis test". A successful uncertainty visualisation would act as a "statistical hedge" for any inference me make using the graphic. Additionally, since the purpose of a visualisation is to give a quick "gist" of the information [@Spiegelhalter2017], this hedging needs to be communicated visually without any need for computation from the viewer. Additionally, @Ndlovu2023 found that participants applied the same methods they used for simple choropleth maps to complicated uncertainty maps even if that take away was invalid. Therefore, this hedging effect needs to be communicated simply through the visualisation. If we refer to the conclusion we draw from a graphic to be its "signal" and the variance that makes this signal harder to identify as the "noise", we can summaries this information into three key requirements. A good uncertainty visualisation needs to:

  1) Reinforce justified signals to encourage confidence in results
  2) Hide signals that are just noise to prevent unjustified conclusions
  3) Perform tasks 1) and 2) in a way that is proportional to the level of confidence in those conclusions.

As @fig-choropleth showed, visualisations that are unconcerned with uncertainty have no issue showing justified signals, but struggle with the display of unjustified signals. Therefore, we coin this approach to uncertainty visualisation as "signal-suppression" since it primarily differentiates itself from from the "noiseless" visualisation approach through criteria (2). That is, the main difference between an uncertainty visualisation and a "normal" visualisation is that an uncertainty visualisation should prevent us from drawing unjustified conclusions. 

{{< pagebreak >}}

## Uncertainty as a signal
Uncertainty visualisation is not only motivated by signal-suppression, and we would be remiss to ignore these alternative motivations. Some authors claim the purpose of uncertainty is to improve decision making [@Ibrekk1987; @uncertchap2022; @Hullman2016; @Cheong2016; @Boone2018; @Padilla2017]. Other authors do not describe uncertainty as important for decision making, but rather explicitly state it as variable of importance in of itself [@Blenkinsop2000]. While uncertainty can provide useful information in decision making, it is important to recognize the "uncertainty" in these cases is not acting as "uncertainty" at all. It is acting as signal. 

This is obvious for the cases where we are explicitly interested in the variance or error, as we are literally trying to draw conclusions about an "uncertainty" statistic. The same is true for "decision making" experiments, but it is less overt. This is easiest to understand with an example. Imagine you are trying to decide if you want to bring an umbrella with you to work. An umbrella is annoying to bring with you, so you only want to pack it if the chance of rain is greater than 10%. Unfortunately, your weather prediction app only provides you with the predicted daily rainfall. Therefore, your decision will be improved with the inclusion of uncertainty, *not* because uncertainty is important for your decision, but because it gives you the tools required to calculate the *actual* statistic you are basing your decision on. In this sense, uncertainty is no more "special" to decision making than weight is in a BMI calculation.

Uncertainty visualisation's made for these purposes should simply display the uncertainty statistic we are interested in, such as the variance, or probability of an event. This is precisely what we observe. @fig-exceed depicts an exceedance probability map that has was designed as an alternative to the choropleth map to improve decision making under uncertainty [@Kuhnert2018; @Lucchesi2021]. A keen viewer may notice that the "exceedance probability map" is actually just a choropleth map, only the statistic being displayed has changed. We do not believe this graphic be considered an uncertainty visualisation.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-exceed
#| fig-cap: "An exceedance probability map that depict the counties of Iowa where each country coloured acording to the probability that the average temperature exceeds 27. This map is a choropleth map where the variable of interest is a probability."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "Exceedance Probability Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# quantile
prob_breaks <- seq(-0.1,1.1, length.out=9)
exeed_data <- my_map_data |> 
  as_tibble() |>
  mutate(xprob = 1- pnorm(27, mean=temp, sd=sqrt(variance))) |>
  mutate(xprob = cut(xprob, breaks=prob_breaks, labels=seq(8)))
             
# Exceed Prob Map
p2a <- exeed_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = xprob, 
              geometry = geometry), colour=NA) + 
  scale_fill_manual(values = basecols) +
  theme_void() + 
  theme(legend.position = "none")

p2b <- p2a %+% filter(exeed_data, variance_class=="highvar")

p2a
p2b
show_pal(basecols, ncol=8, borders=NA, myxlab = "Probability Temp>27", breaks = c(0, prob_breaks[2:8], 1))
```

There seem to be two different definitions of "uncertainty visualisation" floating around in the literature. The first considers *any* visualisation of error, variance, or probability to be an uncertainty visualisation. The second believes an "uncertainty visualisation" is the output of a function that takes a normal visualisation as an input, and transforms it to include the uncertainty information. The former group believe the purpose of uncertainty visualisation to provide signal about a distribution, while the later believe it should act as noise to obfuscate a signal. The lack of explicit distinction between these two motivations leaves the literature muddled and reviewers struggle to understand if uncertainty should be treated as a variable, as metadata, or as something else entirely [@Kinkeldey2014]. This disagreement creates constant contradictions in what the literature considers to be an "uncertainty visualisation". For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, and @Wickham2011 suggests their product plot framework, which includes histograms and bar charts, should be extended to include uncertainty however at least one or both of these charts are used in a significant number of uncertainty visualisation experiments [@Ibrekk1987; @Olston2002; @Zhao2023; @Hofmann2012]. If you view uncertainty as a function applied to an existing graphic, then you would believe a pie chart and bar chart are not uncertainty visualisations, as they are yet to have the "uncertainty visualisation function" applied to them. If you view uncertainty as any graphic that depicts an "uncertainty statistic" then there are no limitations on which graphics can or cannot be uncertainty visualisations. 

When we use "uncertainty visualization" to refer to graphics that simply communicate a variance or probability, we are classifying  visualisations by the data they display, not their visual features. Graphics, just like statistics, are not defined by their input data. A scatter plot that compares mean and a scatter plot that compares variances are both scatter plots. Given that there is no special class of visualisation for *other* statistics (such the median or maximum) there is no reason to assume visualisations that simply depict a variance, error, or probability to be special. Some authors implicitly suggest that that visualisations of variance or probability are differentiated due to the psychological heuristics involved in interpreting uncertainty [@Hullman2019]. While it is true that heuristics lead people to avoid uncertainty [@Spiegelhalter2017] there is no evidence that this psychological effect translates to issues with the visual representation of uncertainty. Again, given that we do not make these same visual considerations for other variables that elicit distaste or irrational behavior, there is no reason to assume this is what makes uncertainty visualisation so special. 

This leads us to the conclusion that the visualisations made for the purpose of displaying information about uncertainty statistics are not uncertainty visualisations. These graphics are just normal information visualisations, and authors can follow existing principles of graphical design. We will focus on the perspective that uncertainty visualisation serves to obfuscate signal, and an uncertainty visualisation is a variation on an existing graphic that gives it the ability to suppress false signals. 

Of course, we do not believe there is anything wrong with explicitly visualizing variance, error, bias, or any other statistic used to depict uncertainty as a signal. Just like any other statistic, these metrics provide important and useful information for analysis and decisions. However, there is no interesting visualisation challenge associated with these graphics, and they do not require any special visualisation. The uncertainty in these graphics are acting as a signal variable, and they should be treated as such. 


{{< pagebreak >}}

# Visualising uncertainty as a variable
Upon hearing that uncertainty needs to be included for transparency, the solutions may seem obvious. You may think "well, I will just add a dimension to my plot that includes uncertainty". This makes sense, as this is the simplest way to add uncertainty to an existing graphic is to simply map uncertainty to an unused visual channel. 

## The bivariate map
@fig-bivariate a variation of the choropleth map, where we have a two dimensional color palette. Not only is temperature mapped to hue, but variance is also mapped to saturation. While these two maps *do* look visually different (which was not the case in the choropleth map) the spatial trend is still clearly visible in both graphics. This means the uncertainty *is* being communicated, however the primary take away in the graphic is the spatial trend (that does not exist). The graphic no longer has a transparency issue as the uncertainty *is* technically included, but is this a good uncertainty visualisation? And if not, why?

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-bivariate
#| fig-cap: "A bivariate map that depict the counties of Iowa where each county is coloured acording to it's average daily temperature and the variance in temperature. This map is a choropleth map with a two dimensional colour palette where temperature is represented by colour hue, and variance is represented by colour saturation. Even though uncertainty has been added to the graphic the spatial trend is still clearly visible in the case where the spatial trend could be attributed to noise."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "Bivariate Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# Bivariate Map
# Make bivariate palette
# Function to devalue by a certain amount
colsupress <- function(basecols, hue=1, sat=1, val=1) {
    X <- diag(c(hue, sat, val)) %*% rgb2hsv(col2rgb(basecols))
    hsv(pmin(X[1,], 1), pmin(X[2,], 1), pmin(X[3,], 1))
}

# recurvisely decrease value
v_val = 0.5
bivariatepal <- c(basecols,
                  colsupress(basecols, sat=v_val),
                  colsupress(colsupress(basecols, sat=v_val), sat=v_val),
                  colsupress(colsupress(colsupress(basecols, sat=v_val), sat=v_val), sat=v_val))
# establish levels of palette
names(bivariatepal) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# Bivariate maps
p2a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = bivariatepal) +
  theme_void() + 
  theme(legend.position = "none")
  
p2b <- p2a %+% filter(my_map_data, variance_class=="highvar")

show_pal2 <- function (colours, borders = NULL, cex_label = 1, ncol = NULL, myxlab, myylab, breaks, breaks2) {
  # Set dimensions of palette
  n <- length(colours)
  ncol <- ncol %||% ceiling(sqrt(length(colours)))
  nrow <- ceiling(n/ncol)
  # make matrix with null values (if not full)
  colours <- c(colours, rep(NA, nrow * ncol - length(colours)))
  colours <- matrix(colours, ncol = ncol, byrow = TRUE)
  # set graphical parameters (?)
  old <- par(pty = "s", mar = c(0, 0, 0, 0))
  on.exit(par(old))
  size <- max(dim(colours))
  plot(c(-1.5, size), c(0, -size), type = "n", xlab = "", ylab = "", 
      axes = FALSE)
  rect(col(colours) - 1, -row(colours) + 1, col(colours), -row(colours), 
       col = colours, border = borders)
  text(c(0,col(colours)[nrow,]) + c(0.2, 0.1, 0,0,0,0,0,-0.1,-0.2) , -4.5, 
       breaks, cex = 1, col = "black")
  text(-0.25, -c(0,row(colours)[,ncol]) + c(-0.2, -0.1, 0, 0.1, 0.2),
       breaks2, cex = 1, col = "black")
  text(4, -5.5, myxlab ,cex = 1.5, col = "black")
  text(x=-1.25,y=-2, myylab, srt=270, cex = 1.5, col = "black")
}

p2a
p2b
show_pal2(colours = bivariatepal, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)
```

## What makes uncertainty visualisation difficult?
Uncertainty visualisation is a uniquely difficult problem. @Hullman2016 commented that it is straightforward to show a value but it is much more complex to show uncertainty. This difficulty is often described as a high-dimensional visualisation problem, as the difficulty in uncertainty visualisation is in working out how to add uncertainty into already existing graphics [@moritz2017trust; @Griethe2006]. The problem with this approach to uncertainty visualisation is that it treats uncertainty the same as we would any other variable, but we are not interested in visualising uncertainty as though it were any other variable.

Typically, when making visualisations, we want the visual channels to be separable, that is, we don't want the data represented through one visual channel to interfere with the others [@Smart2019]. This is not the goal of uncertainty visualisation. The goal of uncertainty visualisations is to display signal and uncertainty together as a "single integrated uncertain value" [@Kinkeldey2014].  Separability allows for the signal and the uncertainty to be read separately which allows for the uncertainty information to simply be ignored when reading the signal, which is a pervasive issue in current uncertainty visualisation methods [@uncertchap2022]. We can see this problem in @fig-bivariate, as it sends the message "this data has a spatial trend and the estimates have a large variance" as we read the signal and the uncertainty separately.

The need for interference from the uncertainty channel does not necessarily mean high-dimensional visualisation techniques would be useless for uncertainty visualisation. High-dimensional visualisation methods could work if the uncertainty and the estimates were mapped to integrable channels. That is, the visual channels would need to be separately manipulable, but read as a single channel by the human brain. While most visual aesthetics *are* separable, there are some variables that have been shown to be integrable, such as color hue and brightness [@Vanderplas2020]. When visualising uncertainty using its own visual channel, we can also consider visual semiotics and make sure to map uncertainty to intuitive visual channels, such as mapping more uncertain values to lighter colors [@Maceachren2012].

```{r}
#| echo: false
#| message: false
#| warning: false
#| eval: false
#| label: fig-bivariate2
#| layout-ncol: 3
#| layout-valign: "bottom"

#fig-cap: "A bivariate map that depict the counties of Iowa where each county is coloured acording to it's average daily temperature and the variance in temperature. This map is a variation on the previous bivariate map where instead of variance being mapped to colour saturation, it is mapped to colour value. Even though colour value is intergrable with colour hue, and colour value is naturally accociated with uncertainty, the spatial trend is still visible in the map."

# Bivariate Map
# Make bivariate palette
# Function to devalue by a certain amount
colsupress2 <- function(basecols, hue=1, sat=1, val=1) {
    X <- diag(c(hue, sat, val)) %*% rgb2hsv(col2rgb(basecols))
    hsv(pmin(X[1,], 1), pmin(X[2,], 1), pmin(X[3,], 1))
}

# recurvisely decrease value
v_val = 1.1
bivariatepal2 <- c(basecols,
                   colsupress2(basecols, val=v_val), 
                   colsupress2(colsupress2(basecols, val=v_val), val=v_val),
                   colsupress2(colsupress2(colsupress2(basecols, val=v_val), val=v_val), val=v_val)
                   )

# establish levels of palette
names(bivariatepal2) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# Bivariate maps 2
p2a1 <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = bivariatepal2) +
  theme_void() + 
  theme(legend.position = "none")
  
p2b2 <- p2a1 %+% filter(my_map_data, variance_class=="highvar")


p2a1
p2b2
#show_pal2(colours = bivariatepal2, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-transparency
#| fig-cap: "A bivariate map that depict the counties of Iowa where each county is coloured acording to it's average daily temperature and variance. This map is a variation on the previous bivariate map where instead of variance being mapped to colour saturation, it is mapped to transparency."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "Transparency Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# ALTERNATIVE TRANSPARENCY INSTEAD OF COLOUR VALUE
# get colour values from transparency transformation
transpal <- lapply(basecols, scales::alpha, alpha=c(0.95, 0.65, 0.35, 0.05))

transpal <- as.vector(matrix(unlist(transpal), ncol = 4, byrow = TRUE))

names(transpal) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# Transparency maps
pta <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = transpal) +
  theme_void() + 
  theme(legend.position = "none")
  
ptb <- pta %+% filter(my_map_data, variance_class=="highvar")

pta
ptb
show_pal2(colours = transpal, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)

```

@fig-transparency is an example of a variations of @fig-bivariate where uncertainty is mapped to transparency, and temperature is mapped to color hue to leverage these visualisation concepts. This method achieves signal suppression quite well. The spatial trend is clearly visible in the low variance case and that trend it becomes much harder to identify in the high variance case. While this was an effective approach for this graphic, relying on integrability may not give us the amount of control we want over our signal-supresison. Without a strong understanding of how these visual channels collapse down into a single channel, relying on integrability could create unintended consequences such as displaying phantom signals or hiding justified signals. Additionally, multi-dimensional colour palettes can make the graphics harder to read and hurt the accessibility of the plots [@Vanderplas2015]. 

There is another reason @fig-transparency is better at signal-suppression than @fig-bivariate, and it may not be due to integrability. Colour value has a second desirable quality for signal-supression, which is that the colours become harder to distinguish as the value decreases. This means high uncertainty values are harder to differentiate than low uncertainty values. This implicit feature of colour value can generalised to other aesthetics by transforming the visual feature space ourselves.

{{< pagebreak >}}


# Visualising uncertainty and signal in a new space
Instead of hoping that uncertainty might collapse signal values into a single dimension, we can do some of that work ourselves, and uncertainty visualisation authors already have.

### Value Supressing Uncertainty Palettes
The Value Suppressing Uncertainty Palette (VSUP) [@Correll2018], was designed with the intention of preventing high uncertainty values from being extracted from a map. Since the palette was designed with the extraction of individual values in mind and it has only been tested on simple value extraction tasks [@Correll2018] or search tasks [@Ndlovu2023], it is unclear how effective the method is at suppressing broader insights such as spatial trends. 

@fig-vsup is a visualisation of the Iowa temperature data using a VSUP to color the counties. The low uncertainty case still has a visible spatial trend, while the spatial trend in the high uncertainty map has functionally disappeared. This means the VSUP has successfully performed signal suppression for our data. 

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-vsup
#| fig-cap: "A map made with a VSUP. The counties of Iowa are coloured acording to it's average daily temperature and the variance in temperature, although the space they have been mapped to is slightly warped. Similar to the bivariate map, temperature is mapped to hue while variance is mapped to saturation. This map successfully reduces the visibility of the spatial trend in the high uncertainty case while maintaining the visibility of the spatial trend in the low uncertainty case."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "VSUP Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# VSUP
# Function to combine colours for VSUP
colourblend <- function(basecols, p_length, nblend) {
    X <- rgb2hsv(col2rgb(unique(basecols)))
    v1 <- X[,seq(1,dim(X)[2], 2)]
    v2 <- X[,seq(2,dim(X)[2], 2)]
    if("matrix" %in% class(v1)){
      # hue issue wrap around pt 1
      v3 <- (v1+v2)
      v3["h",] <- ifelse(abs(v1["h",]-v2["h",])>0.5, v3["h",]+1, v3["h",])
      v3 <- v3/2
      # hue issue wrap around pt 2
      v3["h",] <- ifelse(v3["h",]>=1 , v3["h",]-1 ,v3["h",])
      hsv(rep(v3[1,], each=nblend), rep(v3[2,], each=nblend), rep(v3[3,], each=nblend))
      } else {
        v3 <- (v1+v2)
        v3["h"] <- ifelse(abs(v1["h"]-v2["h"])>0.5, v3["h"]+1, v3["h"])
        v3 <- v3/2
        v3["h"] <- ifelse(v3["h"]>=1 , v3["h"]-1 ,v3["h"])
        rep(hsv(h=v3[1], s=v3[2], v=v3[3]), p_length)
        }
}

VSUPfunc <- function(basecols, p_length, nblend){
  colourblend(colsupress(basecols, sat=0.5), p_length, nblend)
}

# VSUP
p = length(basecols)
VSUP <- c(basecols,
          VSUPfunc(basecols, p, 2),
          VSUPfunc(VSUPfunc(basecols, p, 2), p, 4),
          VSUPfunc(VSUPfunc(VSUPfunc(basecols, p, 2), p, 4), p, 8))

names(VSUP) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# VSUP maps
p3a <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = VSUP) +
  theme_void() + 
  theme(legend.position = "none")

p3b <- p3a %+% filter(my_map_data, variance_class=="highvar")

p3a
p3b
show_pal2(colours = VSUP, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)

```

Now that we have successfully suppressed our invalid signal, we must return to the original criteria we had for signal supression and ask ourselves if we meet all of them. We want to make sure that in our efforts to suppress invalid signals, we have not also suppressed statistically *valid* signals in the process. 

## Can we have uncertinty visualisations for EDA?
A slight problem with uncertainty visualisation is that the uncertainty and the purpose of visualisation are somewhat at odds with one another. The VSUP highlights this problem quite well.

There are two primary motivations behind visualisation, communication and exploratory data analysis (EDA). Communication involves identifying a signal we want to communicate and designing a visualisation that best conveys that, while EDA involves creating a versatile visualisation using it to extract several signals. It is not uncommon for authors to express a desire for uncertainty visualisations that perform signal-suppression in visualisations made for EDA [@Sarma2024; @Griethe2006], however it is not clear that these uncertainty visualisations *exist at all*. 

The impossible task of making this graphic is easy to see with a simple example. Let's say we have a graphic that depicts a set of coefficients from a linear regression and the value of the coefficient is shown using a single colour. We do a series of t-tests on these estimates, and all of them fail to reject the null hypothesis that they are different from 0. This would indicate that each value needs to be visualised using a single color that is indistinguishable from the colour assigned to 0. However, we also perform an f-test on our model, and this test *is* rejected. This test indicates that the set of estimates should not be visually indistinguishable from 0. This is clearly a contradiction. 

This t-test, f-test example highlights a fundamental problem with the VSUP and even the bivariate map. When we blend colors to make them visually indistinguishable, we need to decide at what level of *uncertainty* to blend these colors together. There is no intrinsic mapping of uncertainty, so we need to make a decision as to how much signal-suppression the visualisation should perform. There is no existing rule or accepted practice in how to go about this, but it is present in every uncertainty graphic. Even though the bivariate map was not explicitly combining temperature values at certain variance levels, the mapping of variance to color saturation does this implicitly. That is, at certain saturation values the colors in a bivariate map are imperceptibly different and appear as though they are mapped to the same value, regardless of whether or not that is actually the case. 

Regardless as to whether the map colors are implicitly or explicitly indistinguishable, the point at which the colors blend together can only be validated by a single hypothesis. Therefore, using the methods discussed thus far, it would be impossible to design signal-suppression for EDA. If we are designing a graphic for communication then we can suppress the signal we were seeking to communicate, but this would rely on the distinction between EDA and communication to be somewhat clear cut. Visualization authors may design a graphic with a particular purpose, but what the visualization *is actually* used for is determined by the viewer and some authors have discussed the validity of there being any distinction between visualisation motivation's at all [@Hullman2021]. However, this problem only exists when we color each cell using a single color. If we can color a cell using multiple colors, this limitation may disappear entirely.

{{< pagebreak >}}

# Implicitly Combining Uncertainty and Signal

## Pixel maps

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pixel
#| fig-cap: "A pixel map of the counties of Iowa. In this map, each county is broken up into several smaller regions and coloured according to a potential daily temperature, given the its average daily temperature and its sampling distribution. This results in each county being represented by a sample rather than a single value."
#| fig-subcap: 
#|   - "High Variance Data"
#|   - "Low Variance Data"
#|   - "Pixel Map Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# Pixel Map
library(Vizumap)
# Low variance map
my_map_data_a <- my_map_data %>%
  filter(variance_class == "lowvar") %>%
  mutate(my_id = seq(n),
         error = variance) 

# quantile
q_a <- my_map_data_a |> 
  as.tibble() |>
  mutate(bitemp=as.numeric(bitemp)) |>
  with(data.frame(p0.05 = qnorm(0.05, mean=bitemp, sd=sqrt(variance)),
                  p0.25 = qnorm(0.25, mean=bitemp, sd=sqrt(variance)), 
                  p0.5 = qnorm(0.5, mean=bitemp, sd=sqrt(variance)), 
                  p0.75 = qnorm(0.75, mean=bitemp, sd=sqrt(variance)), 
                  p0.95 = qnorm(0.95, mean=bitemp, sd=sqrt(variance))))|>
  apply(2, function(x) ifelse(x>8, 8, x)) |>
  apply(2, function(x) ifelse(x<0, 0, x))


pixel_1a <- my_map_data_a %>%
  as.data.frame() %>%
  select(my_id, bitemp, error) %>%
  read.uv(estimate="bitemp", error="error")

pixel_2a <- my_map_data_a %>% as("Spatial")

pix_a <- pixelate(pixel_2a, pixelSize = 3, id = "my_id")


pmap_a <- build_pmap(data = pixel_1a, distribution = "discrete", pixelGeo = pix_a, id = "my_id", border = pixel_2a, q=q_a)

p4a <- view(pmap_a) +
  scale_fill_gradientn(colours = basecols) +
  scale_colour_gradientn(colours = basecols) +
  theme(legend.position="none")

# High variance
my_map_data_b <- my_map_data %>%
  filter(variance_class == "highvar") %>%
  mutate(my_id = seq(n),
         error = variance) 

q_b <- my_map_data_b |> 
  as.tibble() |>
  mutate(bitemp=as.numeric(bitemp)) |>
  with(data.frame(p0.05 = qnorm(0.05, mean=bitemp, sd=sqrt(variance)),
                  p0.25 = qnorm(0.25, mean=bitemp, sd=sqrt(variance)), 
                  p0.5 = qnorm(0.5, mean=bitemp, sd=sqrt(variance)), 
                  p0.75 = qnorm(0.75, mean=bitemp, sd=sqrt(variance)), 
                  p0.95 = qnorm(0.95, mean=bitemp, sd=sqrt(variance))))|>
  apply(2, function(x) ifelse(x>8, 8, x)) |> #this will mess with the variance
  apply(2, function(x) ifelse(x<0, 0, x))

pixel_1b <- my_map_data_b %>%
  as.data.frame() %>%
  select(my_id, bitemp, error) %>%
  read.uv(estimate="bitemp", error="error")

pixel_2b <- my_map_data_b %>% 
  as("Spatial")

pix_b <- pixelate(pixel_2b, pixelSize = 3, id = "my_id")

  
pmap_b <- build_pmap(data = pixel_1b, distribution = "discrete", pixelGeo = pix_b, id = "my_id", border = pixel_2b, q=q_b)

p4b <- view(pmap_b) +
  scale_fill_gradientn(colours = basecols) +
  scale_colour_gradientn(colours = basecols) +
  theme(legend.position="none")

  
# Display plots and palettes
p4a
p4b
show_col(basecols, ncol=8, labels=FALSE)

```

## What is uncertainty?

## Show me the data
@Kim2019 looked at how different uncertainty visualisation techniques influenced the percieved sample size

{{< pagebreak >}}

# Evaluating uncertainty visualisations

##  Current methods
@Kinkeldey2014 expressed that uncertainty visualization experiments are idiosyncratic, so it is difficult to generalise the results.

Extracting information using a graphic requires a perceptual task, so it will always be less accurate than explicitly reading the value provided in text form [@Cleveland1984]. 

## The Hermann grid illusion of visualisation experiments
A Herman grid is an optical illusion where a black and white grid appears to have grey dots, but they disappear when you look directly at them. This optical illusion is a fitting description of the issues in evaluating uncertainty visualisations.



##  Testing signal supression
If asking direct questions about uncertainty causes us to treat it as a signal, how do we evaluate uncertainty as *noise*?
 When we ask the viewer of a plot to look at data and extract a value, we are asking them to perform inference on that value. There will be noise associated with that answer and that is uncertainty. If we ask direct question about some uncertainty metric, we have turned the uncertainty into signal because that is what the participants are drawing inference on. 
 
## Current methods
### Secondary effects

- trust, confidence, etc

### Direct questions
Several authors have connected the issues around defining uncertainty to inference, however it is often discussed as a *task* or *goal* dependence. Multiple authors have commented on the need to consider quantifying and expressing uncertainty at every stage of a project as the "goal" shapes every step of the analysis [@Kinkeldey2014; @Hullman2016; @Refsgaard2007]. @Otsuka2023 suggested that the process of observing data to perform statistics is largely dependent on our goals, because the process of boiling real world entities down into probabilistic objects (or "probabilistic kind" as he puts it) depends on the relationship we seek to identify with our data. @Meng2014 commented what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the the question we seek to answer. @Kale2019 discussed how the choices we make in our analysis impact our outcomes and introduce uncertainty. @Carlin2023 mentions that each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to statistical analysis that is devoid of meaning. @Wallsten1997 argue that the best method for evaluating or combining subjective probabilities depends on the uncertainty the decision maker wants to represent and why it matters. @Fischhoff2014 looks at uncertainty visualisation for decision making decides that we should have different ways of communicating uncertainty based off what the user is supposed to do with it. The importance of inference when discussing uncertainty is never directly acknowledge, but it is always present. It is clear that the relationship between uncertainty and inference is noticed at every stage in an analysis, however combining these uncertainties into a single "uncertainty" value is near impossible [@Spiegelhalter2017].

Experiments where users are asked to make a decision based on a probability, and experiments where users need to extract a variance or some feature of the distribution both have the implicit belief that uncertainty visualisation is a "class" of visualisation, and a "good" uncertainty visualisation allows us to extract all information with ease. 

> Most studies that assess the usability of uncertainty visualisations do not contribute to this aspect since they test the retrieval of data and uncertainty separately, but from the perspective that there is nothing special about uncertainty. Traditionally, studies in this field focus on the ability to read both the map content and its uncertainty at the same time. This may be the mandatory criterion for a successful use of uncertainty, but the question that remains is whether this is sufficient to ensure that a user does not only have two separate values in mind but an integrated uncertain data value. [@Kinkeldey2014]

Uncertainty visualisation experiments often compare graphics on the basis of being "uncertainty visualisations" [@Ibrekk1987; @Hullman2015; @Hofman2020] with little consideration for how difficult the information is to extract from the chat. For example, @Ibrekk1987 found that participants were more accurate at extracting a statistic when it could be directly read off the graphic, than when it required an area estimate (which is the case if using the PDF), or when there was no visual indicator for the statistic at all (which is the case when using the CDF of an asymmetric function). @Hullman2015 found that a visualisation that allows viewers to count outcomes to estimate a probability outperformed one that required a complicated area calculation. @Hofman2020 and @Zhang2022 found that participants were better at answering questions about a prediction intervals when shown a prediction interval instead of a sampling distribution. @Gschwandtnei2016 found that graphics where the required statistic could be directly read off the plot outperformed those that involved guesswork due to a gradually decreasing line. @Cheong2016 found that participants made better decisions when they were explicitly given the relevant probability in text rather than when they needed to read it off a map.  

@Hullman2019 found that the majority of papers evaluate visualisations on performance (approximately 65% of the papers they surveyed).

By far the most common metric used is accuracy, which is used by approximately 36% of evaluation studies [@Hullman2019]. 

The final method used by authors is to just explicitly ask about uncertainty and signal information separately. @Sanyal2009 mapped uncertainty to dots and signal to a 3D surface and asked participants to identify areas of high and low signal and high and low uncertainty. Participants were not asked to combine that information in any way, and the signal and the noise were treated as separate variables. @Correll2014 asked participants to separately extract the mean and variance from four uncertainty visualisations. These methods explicitly view the uncertainty and signal as two separate variables that should be extracted from a plot, and not two variables that should be interpreted together. Even viewing these questions as a routine check to make sure the signal information isn't impacted by the uncertainty is counter intuitive, because the whole point *of* the uncertainty is to impact the signal information.


- definition of uncertainty


There are many secondary benefits that come with this improved transparency, such as better decisions, more trust in the results and more confidence in the authors. These secondary benefits are, however, *not* the immediate goal of uncertainty. The following sections will discuss the issues and limitations in measuring uncertainty through these secondary metrics and provide suggestions as to how future studies should consider measuring uncertainty.

Trust is a by product of displaying uncertainty rather than the goal of it, and viewing the relationship in the converse direction can lead to misguided research. Considering trust, and not transparency, as the metric of importance in uncertainty communication can lead to a questionable subtext that argues against transparency, something that has been noticed by several other authors [@Spiegelhalter2017; ONeill2018]. @Hullman2020a found that author simultaneously argued that failing to visualise uncertainty was akin to fraud, but also many avoided uncertainty visualisation because they didn't want their work to come across as "untrustworthy". These authors are optimising *trust* rather than *transparency*, which means they opt to leave out uncertainty information when it does exactly what it is supposed to, decrease certainty in conclusions.

Science communication should be primarily concerned with accuracy. Setting trust and risk-aversion as the variables of interest implicitly encourages statisticians to set trust and risk-aversion as the primary goals of communication. The issue of trust being divorced from trustworthiness has been commented on by other authors [@ONeill2018], however the issue still persists in the uncertainty visualisation literature. @Zhao2023 displayed a several variations of a visualisation of a model prediction and its uncertainty and took participants using the model prediction as a sign of trust. They reported that visualising uncertainty information caused participants to trust the model in the low variance case, but the results in the high variance case were inconclusive. The discussion made it clear the authors thought the uncertainty information should make the visualisation more trustworthy, but conflating trust and the use of a prediction implied uncertainty information should somehow influence participants to use their own prediction, even though a prediction being uncertainty does not necessarily mean it is incorrect. Despite this, the authors seemed to assume that the uncertainty information *should* have an influence on that, showing they had not deeply considered *how* uncertainty information should influence the choices of the participants. (*Cite Gap 18: examples of studies where authors measure trust*)


A similar measure to trust is using "confidence" in an extracted value or a decision. Interestingly, "confidence" is also used to try and capture the clarity of a message in a normal visualisation. Confidence cannot simultaneously be a measure of clarity of visualisation *and* a way to capture the uncertainty expressed in a visualisation.

{{< pagebreak >}}

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "paper.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```

# ------------------- BELOW HERE IS UNSORTED ------------------------

### What is uncertainty?
There does not seem to be a clear agreed upon definition of uncertainty which would obviously create some issues when we want to visualize it. You cant measure something you cant define, you cant quantify something you cant measure, and you cant visualise something you cant quantify.

The ramifications of this problem is seen repeatedly in the uncertainty visualisation literature. A survey of visualisation authors cited "not knowing how to calculate uncertainty" as one of the primary reasons they did not include it in visualisations [@Hullman2020a]. Some works [@Hullman2018; @Maceachren2012; @Thomson2005] ignore the problem get around this definition issue by focusing narrowly on specific terms defined mathematically, such as probability, variance, error, or precision. Others [@Griethe2006; @Pang1997; @Pham2009; @Boukhelifa2017] include broader loosely related elements, such as missing values, but ignore these concepts when translating that uncertainty to a visualisation. Existing mathematical definition of uncertainty (or related concepts) have been unable to unify this range of diverse concepts. For example, @Thomson2005 suggests a mathematical formula for *examples* of uncertainty, and information theory tries to quantify uncertainty using the idea of entropy, but the disconnect between the broad concept of uncertainty and what we can reliably quantify remains a problem.

Some authors prefer to define uncertainty in qualitative terms, often through the use of taxonomies. In these cases, uncertainty is split using an endless stream of ever changing boundaries, such as whether the uncertainty is due to true randomness or a lack of knowledge [@Spiegelhalter2017; @Hullman2016; @utypo], if the uncertainty is in the attribute, spatial elements, or temporal element of the data [@Kinkeldey2014], whether the uncertainty is scientific (e.g. error) or human (e.g. disagreement among parties) [@Benjamin2018], if the uncertainty is random or systematic [@Sanyal2009], statistical or bounded [@Gschwandtnei2016; @Olston2002], recorded as accuracy or precision [@Griethe2006; @Benjamin2018], which stage of the data analysis pipeline the uncertainty comes from [@utypo], how quantifiable the uncertainty is [@Spiegelhalter2017; @utypo], etc. In their own way, each of these approaches show an aspect of uncertainty that an author felt was important to differentiate. 

What is interesting about all these views of uncertainty, is that they all imply that it is a latent feature of our data. That is, it is something that can be revealed through exploration, and not something that needs to be explicitly calculated. The constant stream of new definitions that are unable to capture the true essence of uncertainty might be a result of this false assumption that underpins ever definition. 
Uncertainty cannot be defined without a signal because uncertainty does not exist without a signal, is a by-product of inference.

It is easy to see uncertainties relationship to inference when we consider what might *not* be considered uncertain rather than just trying to think about what *is*. 

Descriptive statistics simply describe our sample as it is and summarizes large data down into an easy to swallow format. Descriptive statistics are not seen as the primary goal of modern statistics, however, this was not always the case. Around the 19th century in England, *positivism* was the popular philosophical approach to science (positivists included famous statisticians such as Francis Galton and Karl Pearson) and practitioners of the approach believed statistics ended with descriptive statistics as science must be based on actual experience and observations, therefore anything that refers to the unobservable (such as new observations or population statistics) is not true science [@Otsuka2023]. In order to make statements about population statistics, future values, or new observations we need to perform inference, which requires the assumption of the "uniformity of nature" (i.e. that unobserved phenomena should be similar to observed phenomena) [@Otsuka2023]. This subtle shift, from descriptive statistics to inferential statistics was shunned during the positivism era *due to the fact it introduced the unknowable*, or in other words, uncertainty. 

This approach to uncertainty is embedded in the idea that descriptive statistics do not have uncertainty, which some readers may disagree with. Specifically, because it means uncertainty is *not* a latent attribute of data, but rather an attribute of a specific hypothesis or estimate. Deniers of this fact follow a consistent logical path and it is easy to identify the common mistake. We know that variance and probability are typically considered types of "uncertainty" *and* descriptive statistics can have variance and probabilities *therefore* descriptive statistics must have uncertainty. The flaw in this logic comes from the first step, assuming that the tools with which we measure uncertainty *are* uncertainty in of themselves. This confusion is common and there are many papers that spend a great deal of time clarifying the difference. @Begg2014 highlight that uncertainty is related to not knowing a specific value, while variability refers to the range of values a quantity can take at different locations, times or instances. @Spiegelhalter2017 made sure to comment on the difference between precise random events (such as the probability associated with a coin flip), and uncertainty (such as the estimated probability associated with a coin that might be biased). The variance of a sample variance can be calculated and know, therefore it is not uncertain but rather it a precise description of dispersion. If we were to discuss drawing a new observation, or estimating the true mean of a population *then* the variance would become relevant in our discussions of uncertainty. 


With this understanding it becomes clear to see why uncertainty is tied to an endless string of examples in the data analysis pipeline. Uncertainty examples include imputed data, model selection, inherent randomness, biased sampling, etc, not because these things *are* uncertainty, but because they *create* uncertainty when we perform inference. Whether or not these elements are relevant is highly dependent on what statistic you are trying to draw inference on, and by extension, the purpose of your visualisation.

As we move through these methods, it seems that the validity of any overarching insight becomes more visible at the cost of our ability to extract particular values of signal or noise. Therefore, given that the primary goal of visualisation *is* insights [@North2006], visualisation authors should err on the side of representing suppressed signal as a single variable, rather than visualising uncertainty separately using two different channels. 


The concept of uncertainty as a by-product of inference, not a latent feature of data, extends naturally to visualisation. 
There are two primary reasons for making a visualisation, to perform exploratory data analysis (EDA) or communication. Communication involves identifying a signal we want to communicate and designing the visualisation that best conveys that, while EDA involves creating a versatile visualisation without an explicit purpose and using it to extract several signals.

 Since EDA is the visual parallel to descriptive statistics, it is performed without an explicit hypothesis which means there *is* no uncertainty in the visualisation. Similarly to our issues with descritive statistics, this is not well understood by the uncertainty visualisation community. Some authors simply confuse varianc for uncertainty. For example @Potter2010 aimed to create a summary plot that "concisely presented data with uncertainty information" to create an exploratory visualisation tool that visualised uncertainty. Other authors regonise inference will occur (in some shape or form) and believe uncertainty *should* be visualised but do not recognise *how* uncertainty would be visualised. @Hullman2021 argued that there is no such thing as a "model-free" visualisation, therefore visualisation require robust visualisations of uncertainty as we are always performing inference.  @Griethe2006 commented that "if visualization is used as a means to explore a data volume or to communicate its contents the uncertainty has to be included". While we agree people cannot prevent themselves from performing inference, that does not mean it is possible for uncertainty to be included in an EDA visualisation. A versatile visualisation such as a scatter plot allows for a viewer to consider several hypothesis at once, each of which will be a different inferential statistic with a different distribution that depicts its uncertainty. It is likely impossible to suppress all possible signals at once.


# How do we evaluate uncertainty visualisations?
 

## Current attempts to measure uncertainty
There is also a swath of studies that are aware a question that boils down to a value extraction experiment, or a question that should be answered by literally ignoring uncertainty information is not what we want when we consider uncertainty visualisations. These papers often try to ask a question that should utilise both the uncertainty and signal in the response, however this is rarely what actually occurs. This method typically results in in cryptic or confusing questions that create a large amount of noise on the interpretation side of the analysis [@Hullman2016].  

Some authors opt for asking slightly vague questions that imply a use of uncertainty, but compare it to a ground truth that is very specific. @Ibrekk1987 asked participants for the "best estimate" which was evaluated in accuracy by comparing it to the mean, however the "best estimate" depends on the loss function we are using, and a loss function of minimised error was not implied by the question. @Hofmann2012 showed two distributions in 20 different visualisations (a line-up protocol) using a jittered sample, a density plot, a histogram, and a box plot and asked participants. Participants were asked to report in which of the plots was "the blue group furthest to the right" The experiment set up is shown in fig-right. The participants answers were then compared to a ground truth where the correct plot had a blue distribution with a right shifted mean. By comparing the results to a ground truth statistic and marking participants as "wrong" or "right", the error from the participants that had an alternative interpretation to the concept of "furthest right" was conflated with the error from a the visualisation choice. These papers make it unclear if the participants got the answers wrong because they misunderstood the question or because of something related to the plot. Therefore, this method leads to inconclusive results about the plot design, and is not advised.


Another method used by authors is to ask a deterministic question about a random event. @Padilla2017 provided participants with a visualisation of the cone of uncertainty and asked then to "decide which oil rig will receive more damage based on the depicted forecast of the hurricane path". The cone of uncertainty provides a 60% confidence interval for the location of the eye of a hurricane, which allows us to know the area where the eye of the storm will go, it does not given any information about the intensity of a storm, the size of a storm, or even if a location will be hit. This inclusion of determinism seems to cause the authors to stumble themselves, as they are not consistent with their assumptions. In their first experiment @Padilla2017 indicated the correct answer was to assume that the storm was equally intense no matter how far from the centre of the distribution an oil rig was, however answering their third experiment correctly hinged on assuming the intensity of the storm at a particular point (which in this experiment they phrased as damage) *does* change in intensity as you move away from the centre of the distribution. Given these conflicting assumptions, it is unclear how the participants were supposed to adjust the probabilistic path information to answer a deterministic question about which oil rig would receive the most damage. Other authors have commented on the complexity of communicating hurricane risk because the path, storm surge and wind speed are all important and cannot be ignored [@Spiegelhalter2017]. The flip side of this is asking participants for a deterministic answer to a probabilistic question. @Correll2014 asked participants "how likely is candidate B to win the election?" when the two distributions indicated voter preference. Participants were not able to answer the question about likelihood in term of probability, but were instead given seven options from 1=Outcome will be most in favour of A to 7=Outcome will be most in favour of B. The ground truth statistic for this question was a scalar multiple of Cohen’s d, indicating participants were supposed to incorporate uncertainty information using a very specific formula that was likely unknown to them but assumed to be used implicitly.  

These examples are a bit complete mishmash of methods, however they point to a larger issue that goes beyond decision making, trust and confidence experiments. Authors *have no idea* how to evaluate the effects of uncertainty in an uncertainty visualisation.



## How to test uncertainty as noise
So, the current methods of measuring or understanding the role of uncertainty in a visualisation is questionable at best, however this is not because visualisation authors are missing the mark, but rather uncertainty is *particularly* difficult to express in a visualisation. In simple estimates or verbal communication, the signal is often easy to identify because it is what we are explicitly saying. Unlike statistical models, visualisations are used in both data exploration and communication. This means what exactly is a *signal* in any particular visualisation is hard to identify, since we often let the visualisation *tell us* what the signal is. Additionally, you cannot add noise to *every single possible* signal one might take from a visualisation. Two people looking at the same visualisation might, just by chance, develop two entirely different insights and draw inference on two completely different statistics. These unique and fascinating challenges that are faced by uncertainty visualisation have been completely untouched by the literature. This section will cover some interesting research in uncertainty visualisation and suggestions for better ways to measure uncertainty.

### Qualitative Studies
Alternatively visualisation research could shift away from the accuracy concept all together ask questions that allow for open ended responses. This method can enlighten authors as to *how* the uncertainty information was used by the participants. @Hofmann2012 tried to capture this by asking participants why they considered a particular plot to be more "right shifted", however this qualitative assessment does not seem to have made it into the final paper. @Daradkeh2015 presented participants with ten investment alternatives and asked participants "from among available alternatives, which alternative do you prefer the most", and were asked to think aloud and consider the uncertainty in their decision making. The experimenters goal was to observe and organise the methods people use when making decisions in the face of uncertainty. This study was an excellent example in a useful experimental design. They highlighted the specific aspects of uncertainty that participants typically considered, such as the range of outcomes that are above/below a certain threshold, minimum and maximum values, the risk of a loss, etc, and mapped where in the decision making process participants made these considerations. Data visualisation is commonly utilised as a tool in data exploration, so it is not uncommon for a data analyst to make a plot with only a vague goal and pull out a large number of adjacent observations. This experimental framework could replicate this process. 

### Just noticeable signal
It could be argued that a well done uncertainty visualisations should have an imperceptible signal unless the signal would be identified with a hypothesis test, almost like a reverse line up protocol, but this idea also has some issues that should be considered. The reject or do not reject concepts in hypothesis testing do not offer a complete image of uncertainty, and exploration of uncertainty visualisation largely stems from a desire to move away from this binary framework. 


@Patrick2023 found that human viewers were less sensitive to deviations from the null hypothesis than the typical statistical tests.

What this all means, is that there will always be some trade off when we are designing a visualisation for signal-suppression. There will be a trade off in *which* signal we suppress, as well as *how much* we suppress it. This is true for any graphic that is comparable in some way to a statistical test or process. A good example is the line-up protocol, which is a visualisation tool that used to check if perceived patterns are real or merely the result of chance [@Buja2009; @Wickham2010]. This motivation is similar to our signal-suppression goal. @Patrick2023 compared standard statistical tests to the lineup-protocol, and evaluated the visualisations using the power curves that are typical for hypothesis testing. In a similar vein, @Kim2019 investigated how different uncertainty visualisation methods influenced user's prior beliefs, and evaluated the graphics by comparing their results to those from Bayesian inference. While these methods apply different statistical philosophies, they both investigated the sensitivity of visualisation methods relative to existing evaluation methods. This approach may prove useful when considering 

## Other elements of uncertainty visualisations
- Things that are true for both normal visualisation and uncertainty, but of particular interest for uncertainty


### Heuristics
Additionally, these heuristics and biases can change depending on the larger scope of the graphic and the population we are communicating with [@Spiegelhalter2017; @Kinkeldey2014]. 

Heuristic checks are useful because they look at unknown pitfalls that might exist in interpretation of current plots. Since the hypothesis for these experiments are usually quite specific, e.g. "do people perceive a an outcome that is within the bar as more likely than one outside it, even if both outcomes are the same distance from the mean?". This means they are less likely to fall into the trap of trying to answer questions that are *far* too broad to be answered with a single experiment (e.g. "is a scatter plot better at showing uncertainty than a box plot"). This work also provides useful insights for experiments by highlights pitfalls participants might fall into when they review the results of evaluation experiments [@Hullman2016]. @Newman2012 found that participants were more likely to view points within the bar as more likely than points outside of the bar in bar charts with error bars. Similar effects have been identified in other types of uncertainty displayed. @Padilla2017 found that points that were on an outcome of an ensemble display were perceived as more likely than points not on an outcome, even when the point that was not on a specific outcome of the ensemble was closer to the mean of the uncertainty distribution. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015].

##
In a similar vein, experiments that verify smaller aspects of plot design might be more useful to the field in the long run because it helps contribute to a larger working theory of "how do we see visualisations". Many visualisation experiments try to compare two plots with several differences, but do not seem to be interested in the mechanisms by which we extract information from visualisations. Small perceptual tasks that seek to answer small but highly relevant questions (for example, if colour hue and colour value can be perceived as a single signal suppressed variable) would be useful to the field.


# Future work
This paper has identified gaps in the uncertainty visualisation literature that could be filled to progress the field.

*Each new development should be accompanied by a mathematical definition of the uncertainty being addressed.* Ideally, a mathematically definition of uncertainty that allows us to combine these components would be developed, but in the absence of that, authors should be more specific about what aspect of "uncertainty" they are covering with their visualisation.

*The concept of uncertainty should be formalised within the grammar of graphics.* This formalisation would allow uncertainty visualisation authors to have a clear understanding of what is or is not an uncertainty visualisation. Additionally placing uncertainty visualisation in the framework that is used to understand existing information visualisation research would help authors understand when existing methods can be used to explain their results. incorporating uncertainty into the grammar of graphics will also give a more precise concept of the information contained within a plot. Other fields of science employ marginal changes when designing experiments to ensure it is well understood *what* aspect of their experiment is contributing to their results, and a better sense of what "marginal" is in the case of uncertainty visualisation would greatly help the field. (*XXX Is data pipeline connected with the grammar of graphics? Should this be a recommendation?*)

*Experimental practices on uncertainty visualisation need to be standardised.* If we are going to consider uncertainty as noise, not signal, there needs to be a way to identify this signal suppression in an experimental design. As the literature currently exists, there is no way to combine papers to get a meaningful sense of how uncertainty information is understood by a viewer. There is also the possibility that uncertainty visualisation evaluations will need to swap to a qualitative methodology where participants are allowed to freely comment on what they notice in graphics until we establish how the existence of noise can be observed.

If an uncertainty visualisation researcher would prefer to perform experiments rather than formalise methods, there are options there too. It would be interesting to know if any perceptual tasks that can be mapped to two different visual tasks condense into a single dimension when looking for overarching signal in a plot. Alternatively, the task dependency many authors in uncertainty visualisation mention would be a useful direction to consider. It is clear that the the number of potential tasks that can be performed on a visualisation increases with with the number of observations. A single observation is limited to value extraction, two observations can be compared, multiple observations allow for shapes or global statistics to be extracted. The interaction between sample size and task is of particular interest to the uncertainty visualisation community, as uncertainty can be expressed through multiple observations using a sample, or through a single value using an error. Of course, this is limited by the fact that there also isn't a definition for what is a "task" and given the mess created by the lack of formalisation in uncertainty visualisation, it may be wise to formalise that concept before performing these experiments. @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations, which could be a good starting point. Regardless, these are directions of research would be fruitful to the uncertainty visualisation community even if it appears on the surface to be research that is only beneficial to the "normal" visualisation community. (*XXX Not sure what this paragraph is recommending?*)


