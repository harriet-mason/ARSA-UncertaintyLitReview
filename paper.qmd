---
title: "Noisy Work: A Review of The Uncertainty Visualisation Literature"
author: Harriet Mason
bibliography: references.bib
date: last-modified
format: pdf
editor_options: 
  chunk_output_type: console
---

# 1. Introduction
From entertainment choices to news articles to insurance plans, the modern citizen is so over run with information in every aspect of their life it can be overwhelming. So overwhelming in fact, that it is not uncommon for people to become so overwhelmed by the information they refuse to engage with it at all. In this overflow of information, tools that can effectively summarise information down into simple and clear ideas become more valuable. While websites that summarise insurance plans or hotel options down into a clear table make a living off it, few tools are as powerful or effective as a visualisation. Information visualisations remain one of the most powerful tools for fast and reliable science communication. Effectively leveraging visualisations to make them more effective is therefore key in facilitating simple and clear communication of science and other ideas. 

Think back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? Now think about the last time you expressed some form of uncertainty. Was it a set of numerical confidence intervals? Maybe they were expressed as a set of values in a table. Did you consider visualising your uncertainty instead? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe's quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Anscombe quartet is shown in @fig-anscombe, because describing the data is never the same as seeing it. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about our data. This powerful aspect of data visualisation is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty.

- @Suh2023 found that the main issue with deployment of AI was communication between data science and subject matter experts, since subject matter experts didn't want technical presentations that didn't explain exactly when and how a model would fail.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up Anscombe's quartet. The four scatter plots are visually distinct but have the same mean, standard deviation, and correlation. The visualisation highlights the importance of plotting your data to identify interesting features that are hidden by other summary statistics."
library(tidyverse)
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```


- industrial accidents have a "tollerable" risk chart based on risk and number of fatalities @Spiegelhalter2017

# 2. Defining Uncertainty
Uncertainty visualisation is not made any easier by the fact that the term "uncertainty" lacks a commonly accepted definition in the literature. @Spiegelhalter2017 even commented that the definition of uncertainty essentially "depends on who you ask". The disarray of organisation and definitions within the uncertainty literature is a common sentiment expressed by reviewers[@Spiegelhalter2017; @Kinkeldey2014]. This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. In this section we will establish exactly what we mean when we talk about uncertainty.

## 2.1 Coloquial Definitions
The most common definition of uncertainty used in papers that discuss visualising it are colloquial. Most definitions make some nod towards variance, precision, trust, error, missing values or another related concepts. Since *uncertainty*, unlike *probability*, does not have a strict definition that is established in mathematics, there is large variety in its usage. Some consider it to be synonymous with probabiltiy, some consider it to be an encompassing umbrella term of which probability is only an element, and some consider it to by something else entirely. This lack of a definition leads to uncertainty being swapped out freely with other vague words such as ambiguity and risk. Definitions presented in papers that discuss uncertainty visualisation want it to be encompassing and include everything a layperson might consider when trying to consider "uncertainty". The issue with these broad definitions is is that in order to visualize something, it needs to be quantified, and in order to quantify something, we need to know exactly what it is we are counting. This discrepancy appears regularly in the uncertainty visualisation literature. Often a vague definition such as "uncertainty is anything that isn't deterministic", or even a refusal to define uncertainty at all in an experiments introduction will be accompanied by a highly specific quantification of uncertainty as a probability density distribution of a forecast when providing the visualisation. Ironically it seems that many of the papers that refer to themselves as "uncertainty" papers are just suffering from a form of uncertainty, a lack of precision.[^1]

This lack of precision has caused several pernicious issues in the literature, including different representation of uncertainty being compared despite not being realistic substitutes (with no acknowledgement of this from the authors); a lack of understanding as to *why* uncertainty should be included in a visualisation; a large swath of literature focused on a minute aspect of the entire uncertainty space with little attention to creative solutions to real world visualisation problems; and a swath of newly designed visualisations that are disconnected from the problems that they hope to solve. These problems means the literature either needs to transition away from the broad umbrella term "uncertainty", or consider the full range of sources of uncertainty and how they can be communicated when designing an experiment.[^2]

The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system"**. This definition encapsulates many concepts adjacent to randomness such as predictions, probability mass distributions (PMF), estimate error, and any data that is not a set of deterministic outcomes. This is the definition of uncertainty I will use for the rest of this report.

[^1]: Flesh out note - I think several of the sentences in this paragraph need examples (or quatified evidence) from the spreadsheet definition part
[^2]: I am not sure if I should mention this here because I go into depth in the literature review and these comments might come across as unjustified if they are left in this section. Maybe this is more of a conclusion paragraph. 

## 2.2 Taxonomy Definitions
### 2.2.1 Taxonomies of Uncertainty
While colloquial definitions of uncertainty are useful when trying to communicate with laypeople who don't understand strict statistical definitions, these definitions are not helpful when we are trying to work out exactly what should be shown on a plot. When authors try to make a considerable effort to define what we mean when we talk about uncertainty, they often express it in the form of a taxonomy. 

When reading any literature about uncertainty or an adjacent topic you will be overrun with more ways to organise uncertainty than you will know what to do with. The problem is so pervasive that it is almost impossible for two randomly selected papers that both claim to be about uncertainty will use the same definition. What is worse is that the reason for using one taxonomy over another, or why a new taxonomy was established in the first place is rarely discussed. This leaves new authors in the field with a wide array of choices and no information that would allow them to determine which is most effective for them. The complete mess of this literature makes it more enticing for authors to just define their own definition of uncertainty, which only adds to this problem. This is not done in ignorance of the issue either, many papers that comment on the unending list of uncertainty definitions will still provide a new taxonomy of their own design. I would almost find this situation comical if I were not one of the people forced to sort through this mess.

Examples of Taxonomies used to untanlge the features of uncertainty:

- aleatory uncertainty (unavoidable randomness), epistemic uncertatiny (uncertainty about structure, still quantified), ontological uncertainty (uncertain about entire modelling process). @Spiegelhalter2017
- measurement (uncertainty on a single varible), spatial (uncertainty in spatial data), temporal (uncertainty in a time data)
- aleatory (inherrent randomness in a forecast) vs epistemic (uncertainty in structure and parameters of statistical models) vs ontological (uncertainty about the entire modelling process as a description of reality)
- scientific methods (variance, distribution) vs human judgments (disagreement among parties)
- Random vs systematic uncertainty
- statistical (can provide a continuous pdf) vs bounded (know values will fall in a certain range) uncertainty
- accuracy vs precision
- Uncertainty from discovery, assessment and execution
- risk vs uncertainty
- "known knowns, known unknowns, and unknown unknowns", (risk, uncertainty, ignorance)
- Source of uncertainty (model structure, parameters, forcing functions, intial state, model operation) and diagnostic (past/current) vs prognostic (forecasting)
- uncertainty is organised according to Satience (legal moral, societal, institutional, proprietary, situational) and level of severity (high, medium, low)
- Level of precision in epistemic uncertainty: 1) numbers given to appropriate levels of precision 2) a distribution range 3) a measure of statistical significance 4) verbal quantifiers to numbers 5) refusal to give a number unless the evidence is good enough @Spiegelhalter2017

![Example of different uncertainty taxonomies](paperscreenshots/collage1.jpg){#fig-location}

This taxonomy issue creates several problems in the literature. Ideally, if we wanted to combine the literature on uncertainty visualisation, each paper would state the same definition of uncertainty and explain which aspect of uncertainty they are visualising, so the experiments would shift around the space while the definition of uncertainty and what it contains would remains stationary. This would make it easy for the typical reader to use the literature to find an appropriate visualisation for the particular category or case of uncertainty. Instead, one has to comb through the method of a particular design to understand if the uncertainty you are working with is amenable to this method. Another issue this creates is that the "space" of uncertainty visualisation is unclear. This means that several areas of uncertainty visualisation that have a real need for creative visualisation tools are left untouched, while the visualisations that are considered "uncertainty visualisations" simply because they are an expression of a probability mass function dominate the literature. Finally the taxonomies do little to help data analysts or statisticians untangle, estimate and understand the uncertainty that may be in their own projects. The endless set of constantly changing definitions implies their *is no* set meaning to the term "uncertainty" and something that cannot be defined certainly cannot be quantified for communication or estimation. This leads people to be less likely to visualise uncertainty due to its imprecision and difficulty to commincate.

This does not mean there is nothing to be learned from these taxonomies, each in their own way show what apsect of uncertainty is important to certain cases. There are a handful of overlapping features that indicate which elements of uncertainty are of interest to statistics and related applied fields. @utypo identified these similarities and used them to design a taxonomy that encapsulates a large proportion of the ad hoc definitions that currently overrun the literature, and is the definition of uncertainty that we will work with for the remainder of this paper. 

@fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each "uncertainty" we encounter through the modelling process. First, we need to consider the source of the uncertainty. Is this uncertainty coming from the data (for example, from an inaccurate measurement), from the array of model choices we have, or from the assumptions on a parameter? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is a measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (aleatory) or is it due to imperfect information and could be improved (epistemic). This is the *nature* of your uncertainty. @utypo then goes on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we should to consider when we are trying to quantify uncertainty. Location asks "where and at what stage in the modelling process is this uncertainty coming from", level asks "how well can we quantify this uncertainty", and nature asks "can we reduce this uncertainty with improves knowledge?". While this taxonomy specifically applies to the uncertainty in the model, it is easy to fit most of the uncertainty we would consider quantifying into this format.

*I need to fix the taxonomy illustration because inputs and model uncertainty need to be swapped*

![Depicts an illustration of the taxonomy described in @utypo. From right to left the drawing shows the location, level and nature of uncertainty with examples of that category underneath. A specific source of uncertainty from the location can be mapped to a level of ignorance that can increase or decrease (i.e. moving up or down the green line) depending on the nature of the uncertainty. Identifying the location, level and nature of your uncertainty allows you to better understand it. ](taxonomyvis.jpeg){#fig-taxonomy}

*The taxonomy* we will use in this paper may seem complicated and difficult to grasp at first glance, but it essentially boils down to three dimensions to consider. Uncertainty is a broad and complicated topic and while oversimplified taxonomies might help us hold the concept of uncertainty in our head, they do little to help with the practical question of how to quantify, visualise, or communicate uncertainty. Organising uncertainty according to its location, level and nature makes allows us to consider uncertainty in the miriad of ways it can arise in a data analysis project and better understand *how* this uncertainty should be communicated. With a detailed understanding of the sources of uncertainty and the ways it can be quantified, we can better understand the gaps in the current set of visualisation tools and assess if these gaps need to be filled.

*The location* axis of the taxonomy lines up neatly with the typical data analysis pipeline. Multiple authors in the uncertainty literature have commented on the need to consider quantifying and expressing uncertainty at every stage of a project [@Kinkeldey2014; @Refsgaard2007]. @fig-location shows the connection between the typical data analysis pipeline and the location element of the uncertainty taxonomy, along with examples of how uncertainty could arise from each of these steps. The relevance of every stage of the uncertainty taxonomy is backed up by both theoretical and practical work in statistics and adjacent fields. @Munzner2009, in her nested model for visualisation, comments on how even something as low level as poor problem abstraction can cause down stream effects and become impossible to ignore. @Otsuka2023 highlights that the choices made in the ontological process of boiling real world events down into statistical objects can influence later stages of analysis due to what was considred signal and what was considered noise. @Meng2021 stresses the importance of treating data as an input from another process with noise and variance, rather than as a neutral input. High level stages such as model selection and estimation have obvious introductions of uncertainty through the choice of models and nature of estimation [^I'm not sure if I need a citation here because this feels obvious]. Location is also acknowledged as an important element of consideration in the taxonomies of several visualsiation papers [^ I will probably need a citation for which ones].

![Illustration of the connection between the data generating process and the location of the uncertainty analysis. Several examples are collected from utypo and Munzner2009. ](location.png){#fig-location}

*The level* axis forces us to identify how quantifiable the uncertainty is. This element is important because establishing how quantifiable uncertainty is informs us how it can be communicated, something that @Spiegelhalter2017 also identified in the form of "precision". Statistical uncertainty can be quantified, so it can be expressed using a PDF, a variance, an error estimate, or something similar while outcome uncertainty cant be quantified but each scenario may be simulated. Acknowledged ignorance can only be stated in the form of an assumption. Visualisations of climate change scenario uncertainty typically combine the outcome uncertainty of carbon emissions with the and statistical uncertainty of the noisy measurement and modelling system to create a cohesive illustration of the uncertainty surrounding climate change. It is commonly noted that each uncertainty must be discussed in isolation, but combining the uncertainty from every stage is near impossible [@Spiegelhalter2017]. The level axis and climate change visualisations provide tools for how to consider combining these uncertainties. Uncertainties that are the same level but from multiple locations can likely be combined to establish an overall model uncertainty. While uncertainties at different levels cannot be combined, they can still be visualised simultaneously as shown in the climate scenario uncertainty visualisations.

*The nature axis* allows us to easily communicate if the uncertainty can be reduced or if it needs to be accepted. While this level if not of particular importance to the visualisation of uncertainty, that is not the only purpose of the taxonomy. The nature of the uncertainty is an important element in expressing uncertainty, especially to stakeholders, and connecting the nature to the location makes it easier to understand *how* uncertainty can be reduced.

Mapping uncertainty into the 3D space defined by location, level, and nature can expand beyond the narrow case of statistical modelling it was defined for. *@___* included "consensus" as a taxonomy element, however it was not included in the final definition, but it can easily be added as a location that would align with the communication stage of the data pipeline. Several authors [*@_+_@Spiegelhalter2017*] considered the method by which uncertainty was quantified (e.g. by a single value, a function, simulated outcomes, etc), which can be be thought of as a more precise categorisation of the level axis. This taxonomy also ignores contextual information about the data, such as spatial or temporal considerations. This flexibility means that the use of this framework is not limited by a specific case and should be used going forward to define what aspect of the uncertainty space a contribution exists in.

The taxonomies that cannot be contained within the @utypo taxonomy are not a new uncertainty dimensions, but rather they are conflating adjacent information with uncertainty. A good example of this is the distinction between spatial, temporal, and estimated uncertainty. When we create statistical models and quantify uncertainty, the spatial or temporal aspects are often boilled away [@Otsuka2023], and the only element of it that remains is the inexchangeability of our data. For this reason, there realistically is no such thing as "spatial" uncertainty in the same sense that there is no such thing as "photograph" uncertainty or "coin toss" uncertainty or "baby birth" uncertainty. Maps and photographs boil down to the same statistical objects in the same way that coin tosses and baby births boil down to the same statistical objects. Once we have removed the physical nature these objects are near identical when considered statistically. Of course the nature of them can change based on what we choose to observe (i.e. if we consider a coin landing on its side to be an outcome or if we consider intersex babies as an outcome in births) but this means the contextual information of our data, e.g. whether or not it is spatial or temporal, is only of importance at two stages of the data analysis pipeline. It is first important when we *boil it down* to our data through *observation* and again when we *map our statistical information back to its real world information* to *communicate* it. The only way the context uniquely contributes to uncertainty is in the "boil down" stage, however spatial or temporal uncertainty never refers to that specific cause of uncertainty. It is typically in reference to measurement error, data aggregation, statistical modelling, or other steps that occur *after* the boil down stage, and are therefore not unique to spatial or temporal data. While the spatial or temporal features *may* become relevant at the final communication stage, however this has little do with quantifying uncertainty, and the difficulties in communicating a complicated statistical model through a complicated map is already well discussed in the visualisation literature, and in later sections we will discuss how this concept is related to uncertainty visualisation. 

In this sense we are not proposing that the current conceptualisation of data according to its physical properties is necessarily wrong, or that it leads to incorrect conclusions, we are merely suggesting for a different way of slicing the metaphysical cake of uncertainty that better aligns with what is *actually happening* when we perform a statistical analysis. This metaphysical cake is illustrated below as @fig-cake. In changing  what the actual boundaries are when we work with these problems, we improve our understanding of what we are actually doing when we conceptualise uncertainty, and in the process allow for more creative solutions creativity (e.g. by borrowing concepts from spatial visualisation to visualise uncertainty in photographs generated). However, since uncertainty is almost synonymous with the concept of "statistics", this adjustment in how we conceptualise and organise statistical methods would likely benefit other domains within the field. 

![The metaphysical cake we are slicing differently](metaphysicalcake.jpg){#fig-cake}

It is important to note that this taxonomy is the closest we can get to mathematical definition of uncertainty. While individual *expressions of uncertainty*, such as a confidence interval or a PDF are mathematically defined, the term *uncertainty* cannot be defined mathematically because it is a layperson word that has been adopted by statistics to describe any deviation from complete determinism, whether mathematically defined or not. While the broader field of uncertainty visualisation does need to be more closely inspected, specifically through the scope of mathematical definitions, this issue cannot be solved with a more specific definition of uncertainty through mathematics. Instead or establishing a new definition of uncertainty that is strict enough to prevent the current array of misunderstandings, we should focus on inspecting how the current literature *deviates* from established mathematical definitions, why these deviations occur, and understand how these deviations can lead to larger problems in uncertainty communication.

### 2.2.2 Taxonomies of Visual Uncertainty
There are some taxonomies of visual uncertainty. These taxonomies seem to seek to define "uncertainty visualisation" and a combined term rather than as two separate words. We largely disagree with this conceptualization of uncertainty visualisations as it blurs the line between the mathematical estimations and assumptions (the uncertainty part) and the visual depiction of those mathematical objects (the visualisation part). While we will not focus on these taxonomies, it is still important to mention them and highlight the reasons they are not good practice.

There are a small handful of taxonomies (or typologies) that are specifically for uncertainty visualisations. @Kinkeldey2014 identified give categories of uncertainty based on the work of other literature reviews of uncertainty visualisation. The five categories are: (1) explicit/implicit (directly mapping or showing multiple outcomes) (2) intrinsic/extrinsic (using existing symbols e.g. colour value, or new objects e.g. grids) (3) visually integral/separable (can or cannot be separated from the data) (4) coincidence/adjacent (if data and uncertainty are in integrated or separate views (5) static/dynamic (animation/interaction). Of the groups they identified, they actually only used (4), (2), and (5), left out (1) because most visualisations are explicit, and (3) corresponds to (1) in most cases [@Kinkeldey2014]. 

@Potter2012 organised several existing uncertainty visualisations into groups based on the dimensionality of the data (1D, 2D, 3D, and No Dimension) and the dimensionality of the uncertainty (Scalar, Vector, Tensor). However, because the term "PDF", a statistical object that describes a random variable that is typically a one dimensional function, is used to describe both the data and the uncertainty for all dimensions, it is hard to understand how this dimensionality should work.

@Grewal2021 created a taxonomy that mapped uncertainty visualisations to some point in a 2D space that was defined by the axis "domain expertise" and "continum of discreteness" that scaled from "point estimate" to "continuous distribtion". 

@Thomson2005 titled their paper "A typology for visualizing uncertainty" however it is a taxonomy for the specific types of uncertainty that we may want to visualise and a way to calculate that uncertainty. While this paper is more about mathematics than visualisation, it is clearly written as a precursor to visualisation.

![Example of different uncertainty visualisation taxonomies](paperscreenshots/collage2.jpg){#fig-location}

These taxonomies are useful in the sense that they often identify areas where there is an over or under supply of uncertainty visualisations. Additionally taxonomies do highlight a concept of uncertainty dimensionality that was not directly covered in the uncertainty taxonomy, however the distinction between uncertainty as a single value versus as a PDF could be considered a sub-consideration of uncertainty that's level is statistical so it can be quantified. While uncertainty visualisation taxonomies are not the standard practice (generating a taxonomy of uncertainty and then developing methods to depict that uncertainty is) [@Kinkeldey2014], it is still important to acknowledge the existence of "uncertainty visualisation typologies" and why they may miss the mark in what is needed from the uncertainty visualisaiton literature. 

The first issue with these taxonomies is that most of these taxonomies are created by observing existing uncertainty visualisations. This means under researched gaps in the taxonomy could be due to a lack of need just as well as a lack of attention, and the benefit of trying to create visualisations that land in uncharted areas of the taxonomy are unclear. 

The second issue is that there does not seem to be anything these taxonomies offer that would not be better established by separate taxonomies for uncertainty and visualisation. These taxonomies seem to depict uncertainty visualisation as a field within itself, rather than a specific case of information visualisation that requires more attention to mathematics than usual. The difference between "uncertainty visualisations" and "data visualisations" is not technically in the visual element, it is mathematical. @Kinkeldey2014 almost acknowledges this in their own paper that discusses an uncertainty visualsiation taxonomy when they claim "future typologies should take different categories of tasks into account (1) communication tasks (2) analytical tasks (3) exploratory task", a common typology for information visualisation in general. The process of understanding and estimating uncertainty requires knowledge of the data, the statistical methods used to make an estimate, and the assumptions of a model. Visualising the statistics that represent uncertainty should be no different than depicting the statistics that represent any other element of a graphic, and therefore there seems to be no reason to have a separate taxonomy for uncertainty visualisation. 


# 3.General Attitude Towards Uncertainty (Uncertainty Communication Boradly)
It would be impossible to discuss uncertainty visualisation without mentioning the broader conversation around uncertainty communication. The conversation around uncertainty communication is often centered around the psychology is communicating risks.

Most research into general uncertainty communication seems to focus specifically on how we communicate risks. Risk and uncertainty are not perfect synonyms of one another. @Spiegelhalter2017 makes the distinction clear by specifying *risk* as *a precise random event* (such as a coin flip) while *uncertainty* (also sometimes referred to as abiguity) is imprecision (such as the estimated probability associated with a coin that might be biased). For this reason, risk is often seen as a specific case of uncertainty and that well defined sub area has a lot of detailled research. Some markers of probability that have common uses, probabilities such as 0 (impossible), 0.5 (a fair coin flip), 1 (certain) are easy for people to have an internal sense of [@Spiegelhalter2017]. It is incredibly hard to communicate small risks, because people cannot differentiate between 1/100 an 1/1000, and communicating low probability but high impact risks are particularly challenging [@Spiegelhalter2017]. This issue can be somewhat alleviated by communicating the likelihood of an event in relation to another event on a similar scale (such as being struck by lightning) [@Spiegelhalter2017]. The framing of how we communicate risks matters due to the affect heuristic, so there will be a difference in peoples behaviours if, for example, you tell people they have a 90% chance of survival vs a 10% chance of death, and when expressing a frequency, a larger numerator communicates a larger risk [@Spiegelhalter2017]. The reference class of a particular risk should be explicitly stated to avoid confusion, for example if we are communicating the chance of rain, the time period over which we are expecting this chance of rain (i.e. 50% chance today or this week) needs to be specified [@Spiegelhalter2017]. The problem with a risk focus is that it is only a small piece of the uncertainty pie, and very few of these findings can be reasonably assumed to go beyond risk communication. Even less can be assumed to translate to uncertainty visualisation.

*this is all currently from the spieglehatter paper because that is the only paper I have moved notes over from mendeley before I started doing the summary write up. this will not all be from one paper, dont worry.*

Risk and uncertainty are not only different in how they are communicated, but also why they are communicated. Risk communication focuses on communicating probability around events. These events are typically a binary event that is known to be random, so a version of communication that does not mention risks (e.g. telling someone they will or will not get cancer instead of communicating their risks about it) inherently carries with it a lack of transparency. Additionally risk communication is often communicated to avoid or informa about an unfavorable outcome, so framing can be important if we want to influence decisions in a certainty way. These motivations do not extend to the communication of uncertainty information more broadly. While uncertainty information *is* often used to improve decisions, it seems the main motivation in its communication is to offer transparency to the viewer about a statistical analysis. That is, it acts as a form of statistical "hedging" for signals found in an analysis. Interviews with experts in statistic back up this primary motivation, as ignoring uncertainty information of often expressed as being similar to fraud or lying and the goal of "improving decisions" is only seen as a secondary outcome of an appropriately hedged signal [@Hullman2020a; @Manski2020]. This concept of uncertainty means that the work on communicating risk does not cover the field of uncertainty communication as a whole. It covers the specific case where the signal itself *is* uncertain, but this work does not naturally extend to the goal of hedging a signal (i.e. when we want to communicate uncertainty as noise). Unfortunately understanding the relevant noise seems to be its own problem, as statisticians and visualisation authors seem to be unaware or confused in how to calculate the uncertainty related to an analysis [@Hullman2020a]. Additionally, communicating uncertainty such that it conveys the appropriate singal supression involves combining uncertainty from multiple different sources, something that is considered to be difficult by even experts in uncertainty communication [@Spiegelhalter2017].

There are some elements of risk communication that seem to translate to uncertaitny communication more broadly. Using words to communicate risk is discouraged because people can misinterpret the actual risk involved, so when possible numerical estimates of risk should be communicated, however this can make information harder to understand for those with poor numeracy skills [@Spiegelhalter2017]. People performing better and preferring with numerical estimates of risk translates to uncertainty communication more broadly *inc (citations for this)*. Risk communication also needs to have clear objectives, use plain language, limit information to only what is necessary, and segment the audience to allow for differences in interest and knowledge [@Spiegelhalter2017]. These general concepts of communication also extend to communication of uncertainty. On the other hand, it is not sensible to assume that probability specific concepts such as framing, intuitiveness of probabilities, or reference classes are still relevant when we consider uncertainty communication as signal supression, not as a synonym of risk or probability communication. In this same vein, most uncertainty communication papers very often discuss risk-aversion as something that needs to be considered despite it being unclear exactly how risk-aversion is related to uncertainty communication *(citations for the papers that do this)*.

In the same sense that risk and uncertainty are different, risk-aversion and amiguity-aversion (which can be considered the uncertainty version of risk aversion) are also not the same. Risk-aversion is the tendency of people to prefer outcomes of low uncertainty to those outcomes with high uncertainty despite the outcomes with high uncertainty having a higher or equal expected outcome. In the case of communicating risk, the only reason this heuristic would be relevant was if risk-aversion was a mistake that needed to somehow be corrected for, rather than a heuristic that *reveals the utility of decreased uncertainty*. Risk-aversion is often treated as a mistake or something that should be avoided by the uncertainty visualisation literature even though that is not necessarily true, depending on what is causing the risk-aversion (e.g. is it due to poorer estimates or increased awareness of negative outcomes). If a particular communication method leads people to place a greater value on certainty or make worse estimates, that is the aspect of importance, not risk-aversion which conflates these factors. The work that advocates for transparency in risk-communication gives you whiplash when it's subtext argues that transparency leads to "incorrect" conclusions. Risk-aversion has a large number of confounding factors that make it difficult to understand if it is something to be considered in risk communication, ambiguity-aversion has a similar problem. Ambiguity-aversion is the tendency of people to prefer to take on certain risks rather than unknown risks. It is essentially risk-aversion applied to risk. For this reason discussions of ambiguity-aversion have the same issues as the conversations about risk-aversion, where they do not consider ambiguity-aversion to be an estimate of the utility of decreased uncertainty, it contains a subtext that argues against transparency, and it is considered a valuable insight in of itself despite conflating many factors.

The idea that risk and ambiguity aversion reveal the utility of decreased variance seems to seldom be considered in the literature, therefore it should be mentioned here, otherwise this issue might continue. However, considering risk aversion is at its core, a trade off between bias and variance, it is bizzare to assume it to be a mistake when statisticians perform these trade offs all the time. If risk-aversion is truly "irrational" behaviour, every statistical textbook that discusses the use of a biased but consistent estimator in a finite sample case (a common example would be specific cases of the maximum likelihood estimator) should be tossed out. Discussing risk and ambiguity aversion as though it is some diversion from a perfectly rational, mathematical choice, implies there is *no value to a decrease in variance which is not even true within mathematics*. The concept of risk aversion treats risk as "noise", as though it is something that only exists to be ignored, but if the common place use of biased MLE's is an indication of anything, it is that *ignoring uncertainty* is the irrational choice.

Trust is another concept that is related to uncertainty communication, but fact that it is often front and center in the discussion has a somewhat ominous subtext. If the purpose of displaying uncertainty information is to appropriately hedge a signal with noise, then it should be assumed that trust is only related to uncertainty communication through increased transparency and honesty. It does seem to be the case that uncertainty communication increases trust because it is a proxy for *(add citations)*. Unfortunately a large proportion of the literature discusses trust as something that is directly related to uncertainty visualisation rather than as an observable product. In viewing uncertainty communication as directly related to trust, not related through the proxy of transparency, several unobserved variables are conflated. The amount of uncertainty that is present in the information, whether or not the information is trustworthy or makes sense, prior beliefs of the participants, and the trustworthiness of the source are a few examples of variables that are conflated when authors directly consider trust to be of direct interest. Similarly to the issues in risk-aversion directly considering trust, and not transparency, as the metric of importance in uncertainty communication leads to a questionable subtext that argues against transparency. Being concerned about audiences perception of trust, without first establishing if what we are communicating is *trustworthy*, leads to the implication that details that result in information not being considered trustworthy should be avoided. 

Authors that imply that a decrease in trust or an increase in ambiguity or risk aversion are metrics of importance in of themselves do not understand the purpose of uncertainty communication as methods to increase transparency. Science communication should be primarily concerned with accuracy, setting trust and risk-aversion as the variables of interest implicitly encourages statisticians to set trust and risk-aversion as the primary goals of communication. The issue of trust being divorced from trustworthiness has been commented on by other authors [@ONeill2018], however the issue still persists in the uncertainty visualisation literature [@Zhao2023]. Additionally, it seems that risk-aversion being divorced from the value of risk has gone unnoticed.

While this is a focus on uncertainty visualisation, these issues in uncertainty communication need to be acknowledged because they bleed into the uncertainty visualisation literature. Uncertainty visualisation are compared on their ability to communicate explicit risk, elicit trust, and prevent risk or ambiguity avoidance, continuing these issues in uncertainty communication at large. While uncertainty visualisation can make it easier to communicate the complicated details of uncertainty information, it carries some challenges that are unique to the visualisaition, specifically with respect to the "signal supression" concept. In simple estimates or verbal communication, the signal is often easy to identify because it is what we are explicitly saying. Visualisations are used in both data exploration and communication. This means what exactly is a *signal* in any particular visualisation is hard to identify, since we often let the visualisation *tell us* what the signal is. Additionally, you cannot add noise to *every single possible* signal one might take from a visualisation. Two people looking at the same visualisation might, just by chance, develop two entirely different insights. These unique and facinating challenges that are faced by viewing uncertainty visualisaiton through the lense of "signal supression" have been almost completely untouched by the literature. 

# 4 LITERATURE SUMMARY
- methods used in uncertainty visualisation evaluation remain ad hoc. studies approach issue from a usability perspective instead of asking WHY representations DO or DO NOT work. Studies do not follow any methodology commonly agreed upon. @Kinkeldey2014
- Lack of formalisation and eigour in empirical methdos is an issue that is much broader than uncertainty visualisation (extends to info-vis and related domains). Therefore this paper is a starting point, not a definitive summary @Kinkeldey2014
- The question in these papers is often "does method a work better than method B" which is an engineering approach, where the toal is to improve a particular product rather than create general principals @Kinkeldey2014
- Authors then try to generalise beyond the specific constraints of the test even though the framing does not allow for it @Kinkeldey2014
- Alternatively, we should ask how/why does method A work better than method B, which grounds work in perceptual and cognative theory and provides a framwork @Kinkeldey2014

# 4.?? Similar work (other literature reviews)
- If you just keep using rankings, you need to compare everything one by one forever to get an idea of the space @Hullman2016
- people sometimes ignore uncertainty and instead look at the pattern (e.g. distance between means) and use a heuristic to answer the question. be proactive about possible heuristics, look for signes of heuristics in responses, ask subjects to describe their strategy, consider including degree of heuristic as a dependent variable @Hullman2016
- People believe in the law of small numbers but dont believe in the lar of large numbers @Hullman2016
- experiencing an event makes your percieved probability higher than if you see a description of it @Hullman2016
- Blur and fog are suggested because they are metaphors for uncertainty, however the usefulness of these metaphors is rarely investigated @Kinkeldey2014
- Frequency thing means that more effective communication to participants in a study means more effective science and we have a roll on effect @Hullman2016
- Probability judgements are less relative than other psychophysical judgements, 0%, 50% and 100% all have well understood meanings. Subjective probability distributions are spontaneous, and subjects are constructing a distribution upon being asked rather than articulating a fully formed mental representation of their beliefs. Therefore it is subject to heuristics. consider anchoring a scale with farmiliar probabilities . not sure if being asked for the entire distribution helps, e.g. the graphical one reduces noise but asking people for an interval estimation leads to overprecision and overconfidence @Hullman2016
- If the goal is to compare visualisation, ground truth is easy, if the goal is to evaluate how accurate the subjective probability distributions are, the ground truth becomes complex. ground truth issue e.g. if you provide a sample from a distribution, is the true value the mean of that sample or the population mean? @Hullman2016
- believed that offering enough decisions with varying rewards you can observe the true subjective probability, however whether or not probability is the right representation of subjective uncertainty is a topic of debate @Hullman2016
- she also discusses graphical interface (users have to draw the probability distribution) and framing probabilities as natural frequencies are better choices because they reduce noise and stuff.@Hullman2016
- Consider incorperating utility functions. Include "probability-coherence" checks @Hullman2016
- evidence that people do not follow the axioms of probability when conveying subjective probabilities. How do you deal with this as an analyst (e.g. if the probabilities do not sum to 1 @Hullman2016
- If someone answers a question correctly it is hard to tell if it was because of the information provided or if the person was using a heuristic @Hullman2016
- You cant tell if subjects in an experiment do poorly because the visualisation was bad or because the question was misinterpreted @Hullman2016
- Visualisations risk: consider a good summary table as a visualisation, use multiple formats because no single representation suits all members of an audience, illuminate graphics with words and numbers, design graphics to allow a part-to-whole comparison on an appropriate scale, narrative labels are important (show magnitude through tick marks), use narrative, images and metaphors to gain/retain attention but dont arouse undue emotion, assume low numeracy of a general public audience and adopt a less-is-more approach by reducing the need for inderence and making clear and explicit comparison, be cautious about interactivity and animations (may introduce uncessary complexity), avoid chart junk (like 3d bar charts), assess the needs of the audience, and iterate towards a final design. @Spiegelhalter2017
- Graphical features that improve accuracy, features that facilitate behavioural change, and features that viewers like may be different (are not necessarily the same). @Spiegelhalter2017
- also should consider graphical literacy @Spiegelhalter2017
- graphs are good for gists and different graphics are good at different things (i.e. trends vs comparison etc) but hard to avoid framing (show information as part of a whole) @Spiegelhalter2017
- scatting in pictograph increases the impression of randomness but makes it difficult to count @Spiegelhalter2017
- people with high numeracy count pictographs but low numeracy assess area @Spiegelhalter2017

# 4.1 Plot Comparison Papers
# 4.1.1 The problem of incomparable plots
Visual inference can be seen as a process that combines multiple fields, from mathematics to psychology, to convey meaning. Unfortunately which tasks belong to mathematics, computer science, or psychology is poorly defined. The uncertainty visualisation taxonomies make the blur between these fields. A visualisation is, in a lot of ways, a statistic, or at the very least statistics are calculated prior to the visualisation being calculated, however this may be invisible to the user. A density plot does not just "appear" from the data, usually a smoothing function (that should be calibrated according to the sample size however how this should be done is a different matter of dispute) generates an estimated density function (statistics) which is then depicted on a plot using a line (design or computer science) which is then converted into information in your head using perceptual tasks and heuristics (psychology). 

These overlapping but distinct fields create a high burden of spanning expertise to understand if two visualisation are equivalent in each field. This is largely caused by the concept of "information" differing dramatically between fields. According to mathematics, a sufficient statistic for a parameter contains the same amount of information as an entire sample. Therefore, if you are trying to estimate a population mean, the sample mean and the entire data set are both equivalent, however a similar statistic, such as the median, is not and will become a worse and worse approximation for the mean the more skewed your underlying distribution in. Computer science considered two pieces of information to be equivalent if they come from the same data [^This is the justification that I have come across in several computer science field papers, I am not sure if this is true for the whole field so this is definitely a question for Sarah]. This definition of information is much looser and is contained within the mathematical definition of information. 

Most uncertainty visualisation papers show that the definition of information *should* be the one defined by mathematics, not the one defined by computer science. A close look at the visual inference literature shows this to be true, but a large amount of noise is added to the research through visual heuristics. This is simply because, if one visualisation depicts a "significant statistic" and another visualisation shows a similar statistic, then the graphic that shows the significant statistic holds more relevant information to the question and will lead to more accurate interpretations. If the information that is depicted on a plot cannot be used to *mathematically* generate a best estimate for a specific statistic, I do wonder how the visual system would be expected to cover that gap if *not* for heuristics. 

While mathematical information *should* be considered the starting point in considering if two visualisations are equivalent, it is not the only consideration. A large amount of work in perceptual tasks, attention, and psychology surrounding charts shows that all the information depicted is not paid attention to in equal measure. Elements such as colour can have sizable impacts on the way a graphic is percieved. Mathematics identifies the information that is *shown* while psychology identifies the difference in information *recieved*. A failure to understand this distinction leads to graphics that are different not only in the way information is depicted but in the information itself. This problem is pervasive in uncertainty visualisation literature.

- The big data paradox shrinks confidence intervals but magnifies bias [@Bradley2021] because the confidence intervals shrink due to the sample size but the imprecise target group creates bias.

# 4.?? Noise vs Signal (question +)
The first issue that arises with uncertainty research is that uncertainty is frequently treated as a signal, not noise. This discrepancy is also noted by @Kinkeldey2014 who, in their own literature review on uncertainty methods, comment that the literature seems unsure if uncertainty is simply another variable or if it should be treated differently. This is a fair question to ask. If uncertainty is simply a signal we are trying to convey, why should it be treated any different to any other signal in a visualisation? 

- if the experiment DOES use noise as a signal, it is often very unclear how the participants should include that in their respose and what would be a valid response from participants (e.g. the likelihood to how much question and the cone of uncertainty questions). separate to the data, uncertainty has its own goal. Is it to impact confidence in the estimate (like a bayesian thing)? to supress signal (like a barely noticiable difference line up plot thing)? to facilitate more efficient decisions (so accounting for risk thing)? 

- will it impact their decision at all? there are a couple of papers where you can answer all the questions by just ignoring the uncertainty information because of the way they have set it up. e.g. difference in means when the variance is the same, experiments that check if someone can still get the signal and make sure the uncertainty information isnt in the way. 

- The literature also does not discuss if uncertainty is another variable, or if it needs to be treated differently (i,.e. is it metadata of another variable). Most papers studies on uncertainty dont consider this issue, and the question remains, should someone have two separate values, or an integrated view uncertain data view @Kinkeldey2014

This is best seen with a simple look at some papers
(Make into table)
Questions where uncertainty is a signal

Questions where uncertainty is a noise


## 4.?? Greater Infoviz Issues
- Since there are so many papers that treat uncertainty as a signal, not a noise, issues in the uncertainty visualisation literature also help to understand some difficulties in the information visualisation literature as large.

- Suggests that it is straightforward to show a value but much more complex to show uncertainty @Hullman2016 (I think this is interesting in relation to how people SEE uncertainty visualisation. Why is uncertainty hard to visualise?)
- Authors provide little justification for their chosen response models (e.g. absolute accuracy vs relative measures) @Hullman2016
- When asking questions, researchers should focus on events that can be repeated (because of a misunderstanding of confidence intervals) @Hullman2016
- Uncertainty defined as a by product of the process of making a visualisation, from data collection to commucating @Hullman2016
- Some questions (e.g. what are the chances that the no.6 bus will arrive first) you can elicit the viewers subjective probability distribution @Hullman2016
- communication tasks: This category comprises map reading tasks involving data and uncertainty value retrieval (which location is most uncertain?). For tasks from this category, visualisation techniques can be chosen following the traditional rules from cartography. @Kinkeldey2014 (basically saying why dont you just use what is already here if you are treating uncertainty as a signal, which I agree with) 
- papers usually have same variance if there is a signal question, and use the mean as a proxy to answer the question. Really should ask a changing signal question where the signal IS impacted by the uncertainty information.
- Weirdly When setting their ground truth it seems like a lot of papers EXPECT participants to completely ignore uncertainty information as noise. Like the participants are actually considered to be incorrect if they dampen their signal understanding because of noise considerations which I find FUNNY. 
- I think the questions where uncertainty is a signal, not a noise, allow the authors of visualisation experiments to ignore the context and motivation that usually governs a visualisation.

# 4.?? Tasks and Motivating Questions (uncertainty about.. .what?)
- "cannot assess the quality of risk communication unless the objectives are clear" circles back to motivation. He assumes we are fulfilling a duty to inform. @Spiegelhalter2017
## 4.?? What are you uncertain about? A mathematical considerations
- Should add plot comparison mathematical framework example here
- take a plot and work through the entire example as a flow chart thing
- tasks e.g. "What is the average of X" (how well lined up you are moving down the data pipeline) and resolution (uncertainty is very often moving back a resolution level in statisitcs - also can be an assumption - bootstrap is a replacement for an assumption, not a resolution change, but bootstrapping is centered on the data)  e.g. "What is the noise around the estimate of the value of X"
## 4.1?? In other statistics
What is meant by "uncertainty" may seem obvious to some, but when you attempt to quantify or visualise it you will quickly find yourself asking, "uncertainty about... what?". Do you mean uncertainty on an estimate? On a forecast? How many steps ahead is this forecast? Are we only considering the uncertainty in the estimate or in the parameters or are we considering the possibility of measurement error or biased inputs? Signal and noise can only be untangled in the presence of a motivating question.

The idea that uncertainty can only be defined in the presence of a motivating question is well grounded in most areas of statistics. The entire process of data analysis, from deciding what should be observed as data through to communicating that data in a plot is governed by human decision and the goal of an analysis. At the philosophical level, applied statistics is simply taking real world entities and boiling them down into probabilistic objects, an ontological process that is largely dependent on our goals [@Otsuka2023]. When we move onto data provenance the issue persists, as what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the resolution of the question [@Meng2014]. After moving onto modelling this issue continues as each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to signal devoid of meaning [@Carlin2023]. Even at the final stages of visualisation a lack of understanding of the motivating question make it difficult to untangle what is signal and what is noise, leaving many uncertainty visualisation studies with conflicting results [@Kinkeldey2014]. These cases highlight that uncertainty is defined at *every* stage in relation to our motivating question, from data collection to visualisation. Discussions of uncertainty cannot be had if we are not clear *what* we are uncertain about. Once it is established what we are uncertainty about, we can consider the other elements of uncertainty that need to be defined.

## 4.1?? General task notes
- heath had a wide range of different metrics they adopted to communicate chronic risk of adverse events in the future @Spiegelhalter2017 (use this for motivation based statistics since they define new statistics based off what they need)
- Mentioned alleatory vs epistemic uncertainty, in the visualisation literature alleatory uncertainty is more commonly the focus. also mentions dependence and joint pdfs as a consideration @Hullman2016
- Most studies involve intrinsic approaches such as colour, transparency, and those that used extrinsic techniques typically used glyphs, error bars, grid based techniques, or contouring @Kinkeldey2014
- most studies used coincident approaches, static visualisations, are applied to a specific domain and dont have generalised results @Kinkeldey2014
- The tasks for participants were most commonly value retrival (some where uncertainty and data value are retrived separately, others where they are some separately). also also be asked to extract a relative value @Kinkeldey2014
- How does the complexity of uncertainty relate to the cateogies of user and task @Kinkeldey2014
- Several studies probide evidence that the usability of uncertainty representations can be highly user and task dependent @Kinkeldey2014
- The nature of the task plays an important role for the usability of uncertainty techniques, this may explain many of the insonsitent outcomes from the studies under review since two studies assessing sifferent techniques and user groups are not comparable @Kinkeldey2014
- Doing a follow up publication to discuss issues with reasoning and decision making based on uncertainty visualisations which is not considered or discussed in this paper @Kinkeldey2014
- Main take away is that "we need to systematize future empirical studies on uncertainty visualisation to better enable comparison and generalisation of the findings"
- One way to advance this goal is a taxonomy, however existing taxonomies are focused on data types, uncertainty categories, and representation types @Kinkeldey2014
- he lists a bunch of tasks that were tested with uncertainty visualisationsat the end @Kinkeldey2014

- risk is uncertainty as a signal, and when uncertainty communication is spoken about this is typically what is discussed, uncertainty as signal supression is what is usually meant when we talk about visualisations but that area is functionally ignored.
This idea, that uncertainty can only be defined in the face of a motivating question, is well grounded in the literature but seldom explicitly mentioned

In section 2.1 it was mentioned that uncertainty must be defined within a specific motivating question, otherwise it inherently does not make sense. A large difficulty with the uncertainty visualisation evalusation studies is that this rule is not followed. There are a shockingly large number of evaluation studies that seem to pay no attention to the information that is relevant to the question they are asking, and show participants a selection of seemingly random visualisations from a statistical point of view. 

- inferential uncertainty and outcome uncertainty ARE NOT THE SAME THING??? they visualise DIFFERENT DISTRIBUTIONS

We define two primary motivations for uncertainty visualisation.
  1) To prevent deterministic conclusions from a random signal (uncertainty as noise)
  2) To convey information about a variance, probability, or other random event (uncertainty as signal)

Uncertainty as signal papers have incredibly predictable results
Uncertainty as noise papers should follow a similar protocol to the line up papers

Notes from previous sections that were moved here
- The concept of uncertainty being task dependent is *particularly* salient for uncertainty visualisation, and it is repeatedly identified as a problem in previous reviews of the uncertainty visualisation literature [@Kinkeldey2014; @Hullman2016] as well as across many sub domains and applications [@Wallsten1997; @Munzner2009; @Fischhoff2014; @Meng2021; @Amar2005]. The fact that this conclusion is repeatedly reached shows both the importance and the lack of acknowledgement this concept receives.
- @Fischhoff2014 discusses how decisions with different levels of flexibility should have different uncertainty visualisations. If the user has no choice they should simply be made aware of the risk, if they have several options, the user needs to see the uncertainty in each, if they need to come up with the options themselves, they need to see how things work and how the system fails.
- @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations.
- Statistics is, at its core, the study of uncertainty. Therefore discussing uncertainty visualisation a a separate sub domain to "normal" data visualisation is inherently confusing. What is typically meant by "uncertainty" visualisation is "noise", that is, we want to present some signal cushioned by its natural variance. Unfortunately, this distinction between "signal" and "noise" is entirely goal dependent.
- @Bella2005 found that most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. (**also the paper sherry sent me**)

- Need to compare to an approripate ground truth, but this is a philisophical exercse @Hullman2016
## 4.?? Incomparable plots
- But when different techniques are to be compared, it is important that the scenarios and datasets are informationally equivalent, i.e. according to Larkin and Simon (1987, p. 67) ‘[t]wo representations are informationally equivalent if all of the information in the one is also inferable from the other, and vice versa’. A goal in testing, then, is often to determine whether they are also computationally equivalent, or whether one depiction has an advantage over another, i.e. ‘[t]wo representations are computationally equivalent if they are informationally equivalent and, in addition, any inference that can be drawn easily and quickly from the information given explicitly in the one can also be drawn easily and quickly from the information given explicitly in the other, and vice versa’ (Larkin and Simon 1987, p. 67). @Kinkeldey2014

- The way we ask questions and the types of questions we ask are selected with little justification. This paper makes suggestions to reduce the noise in the data from these papers. @Hullman2016

- Most commonly check abolute measures of accuracy, where accuracy is abs(subjective error - actual probabiltiy). Some papers also check relative measures of accuracy and ask subjects to find regions of least certainty or to rank targets by uncertainty. studies also consider response time and confidence rating. some studies also ask users for their own expression of uncertainty (e.g. by asking when sketches are uncertainty and ranking various depictions of uncertainty). Some studies ask subjects to make some decision using the data. ometimes things like complexity of task, degree of visual overload, ease of use, visual appeal, and preferences @Hullman2016
# 4.?? Source of Uncertainty
- rarely used extension is aggregation of uncertainty over an area (retrieve overall estimation from a spatial distribution of uncertainty). Other tasks include comparisons, rankings. also have a task called "search" where participants have to identify entities that fulfill certainty characteristics (high or low values) @Kinkeldey2014
- Whether to use classed or unclassed uncertainty is rarely discussed @Kinkeldey2014. e.g. land map with remotely sensed images has ambiguity in the boundaries, which can result from multiple sources (land cover class, measurement error, images from different dates). If you combine them all it might not make sense for someone who only needs one.

- I think the dimension of the uncertainty should be considered every time there is aggregation from multiple sources of uncertainty and you want to maintain those sources. You can project the uncertainty down into one dimension if the source information is in of itself unimportant.
# 4.?? Importance (and unimportance) of Secondary and Meta information
- Specifies uncertainty can be represented by three components attribute (what) position (where) and temporal (when) @Kinkeldey2014
- Studies dealing with uncertainty information typically deal with attribute but rarely position and time @Kinkeldey2014
- 10/44 studies involve multiple types of uncertainty @Kinkeldey2014

- When we boil down data into a statistical object, we boil away many aspects on context. Birth and coin flips become an object that is one in the same, and often the visualisation 
- Why are spatial and temporal data considered special? What is the statistical element that remains after they have been away? Inexchangeability. Temporal data is one dimensional inexchangeability, spatial data has two dimensional inexchangeability.
  - There are other things that have inexchangeability, and not every question related to spatial or temporal data requires the inexchangeability.
  - 
- Yes the data type is relevant but its not the most important thing
  - one with temporal, one with that temporal paper but you didn't know it was temporal, one with the bus time cdf
  - screenshot from these three papers, the connecting factor is the question, not the data type
  - The data type: spatial, temporal, etc, usually is mentioned as the 
  - Not a feature of the data, but it is a feature of the question "how do I show uncertainty on a map?". This is a fine question to ask, but the lack of specificity in what is being done, and the zoning in on functionally irrelevant elements leads the literature as a huge mess.


- Hurricane risk is not only the path, but also the storm surge and wind speed. @Spiegelhalter2017
- coloured maps for earthquake risk are easily interpretable @Spiegelhalter2017
# 5 Great Examples
- best things you have seen and why they are the best
- can't have a best practices because you need experimental results and my point is that the experiments aren't testing the right things
- pixel map
  - Similar to what could be good but its computation isnt quite what I would like
  - heaps of sample depictions are animated and that is not always possible, but this is a clear indicator of how you can do a sample visualisaiton
  - hasnt been tested so it is unclear if this actually works at signal supression
- VSUP
  - The purpose of uncertainty is clear in the design and it is signal supression
  - Supression is not the only uncertainty feature we should be interested in (shape is also important which is why I have beef with over smoothing and something visualisation is very good to express) but by being clear in its goal it knows what it is
  - It addresses several key issues with the bivariate map that arrise in general and especially when it is used for visualising uncertainty 
- the census dot map (has been taken down but CNN replicated it [here](https://edition.cnn.com/interactive/2021/us/census-race-ethnicity-map/))
  - I love a good depiction of a sample
  - You can see the sparse regions and the pop 
  - it is similar to the pixel map (could also be singal supression but it is not tested)
  - the sample depiction works well with the interactivity and you can see the signal that is appropriate for a particular level of zoomed in. If you zoom out the points turn into solid colours and show clear signal.
- The new york times class mobility and race animated plot [here](https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html) 
- the animated surface classifications from that satelite images experimental paper
- the climate change scenario uncertainty. There are quite a few of these:
  - [e.g.1](https://earthobservatory.nasa.gov/features/GlobalWarming/page5.php)
  - [e.g.2](https://climatechange.chicago.gov/sites/production/files/2016-07/scenariotempgraph_0.jpg)
  - there are a few examples on the [climate change scenario wiki](https://en.wikipedia.org/wiki/Climate_change_scenario)
  - Good example of uncertainty that has uncertainty from two different sources (human decision in the form of inputs and statistical uncertainty), one of which cant be quantified.
  - a good example in how you can combine two different  types of uncertainty
  - [This](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/graphical-cartograms-in-arcgis-pro/) has some good maps and some bonkers ones 

Notes
- Since a lot of the good visualisations are made by random people creating a new visualisation (likely through back and forth communication which is HOW you get good visualisations) 
- A lot of these show the data instead of an inference which is probably better for this scenario because sample size is important for uncertainty. It is not always appropriate and sometimes dots are not exactly equal to one draw. 
  - comments about smoothing in density plots and bin width in histograms. I do think the sample size should be communicated to people. While it is not the only thing that matters it seems kind of pointless to go to such lengths to hide it.
- Sometimes points are used for things that are NOT samples which can get confusing and honestly I dont like it.


# 5. Future work/Conclusion


# Appendix (or information I am holding)
## Experiment paper checklist
General
- Specific Field of Research
- Question asked
- Risk or Uncertainty: 
- Noise vs signal (risk vs uncertainty)
- Major task goal
    - e.g. identify a bias or misunderstanding, compare two visualisation methods
- Minor task goal
    - Communication(value retrieval)
    - Analytical(complex consideration)
    - Exploration(free for all)

Uncertainty Considerations
	- Source
	- Level (statistical-ignorance)
	- Nature (Epistemic/Aleatory)

Visual Uncertainty taxonomy
- Explicit/implicit
    - (directly mapping or showing multiple outcomes)
- Intrinsic/Extrinsic 
    - (using existing symbols e.g. colour value, or new objects e.g. grids)
- Visually integral/separable 
    - (can or cannot be separated from the data) 
- Coincidence/adjacent 
    - (if data and uncertainty are in integrated or separate views
- Static/dynamic 
    - (animation/interaction) 

Other plot considerations
	- Dimension of data
	- Dimension of uncertainty
	- Feature mappings

Extra (not necessarily recorded)
	- Metrics used (and recorded)
	- Possible Heuristics
	- What is the ground truth?
	- Participant literacy

 

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "ideas/confirmation/confirmationreport.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```