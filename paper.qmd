---
title: "Uncertainty Literature Review"
author: Harriet Mason
bibliography: references.bib
date: last-modified
format: pdf
editor_options: 
  chunk_output_type: console
---


# 1. Introduction
Think back to the last time you made some sort of data visualisation. What was the purpose of that visualisation? Was it to better understand your data? Was it to help you make a decision? Was it to to communicate that decision to someone else? Now think about the last time you expressed some form of uncertainty. Was it a set of numerical confidence intervals? Maybe they were expressed as a set of values in a table. Did you consider visualising your uncertainty instead? There are many stages in our analysis that benefit from the power of data visualisation, however this does not mean it is always done with success. Visualization is an important step in exploratory data analysis and it is often utilised to **learn** what is important about a data set. The importance of data driven discovery is highlighted by data sets such as Anscombe's quartet [@anscombe] or the Datasaurus Dozen [@datasaurpkg]. Each of the pairwise plots in these data sets have the same summary statistics but strikingly different information when visualised. Anscombe quartet is shown in @fig-anscombe, because describing the data is never the same as seeing it. Instead of having to repeatedly check endless hypothesis to find interesting numerical features, visualisations **tell** us what is important about our data. This powerful aspect of data visualisation is poorly or seldom used in later stages when we are communicating our findings, specifically with respect to uncertainty.

- @Suh2023 found that the main issue with deployment of AI was communication between data science and subject matter experts, since subject matter experts didn't want technical presentations that didn't explain exactly when and how a model would fail.

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: "The four scatter plots that make up Anscombe's quartet. The four scatter plots are visually distinct but have the same mean, standard deviation, and correlation. The visualisation highlights the importance of plotting your data to identify interesting features that are hidden by other summary statistics."
library(tidyverse)
tibble(x = c(anscombe$x1, anscombe$x2,
             anscombe$x3, anscombe$x4),
       y = c(anscombe$y1, anscombe$y2,
             anscombe$y3, anscombe$y4),
       Plot = c(rep("Plot 1",11), rep("Plot 2",11), 
                rep("Plot 3",11), rep("Plot 4",11))) %>%
  ggplot(aes(x,y)) +
  geom_point(aes(fill=Plot), colour="black", 
             size=3, pch=21, alpha=0.75) +
  facet_wrap(~Plot) +
  theme_classic() +
  theme(aspect.ratio = 1,
        legend.position = "none") +
  scale_fill_brewer(type = "qual", palette = 4)
```


# 2. Defining Uncertainty
Uncertainty visualisation is not made any easier by the fact that the term "uncertainty" lacks a commonly accepted definition in the literature. @Lipshitz1997 even commented that “there are almost as many definitions of uncertainty as there are treatments of the subject”. This mishmash of terminology leads to a large body of work, all claiming to finding the best visualisation or expression of of "uncertainty" but most don't even seem to agree on what uncertainty is. 

## 2.1 Motivating Questions
Signal and noise can only be untangled in the presence of a motivating question. While it is seldom mentioned in the uncertainty visualisation literature, but is still important to understand that "uncertainty" does not exist without a motivating question. The entire process of data analysis, from deciding what should be observed as data through to communicating that data in a plot is governed by human decision and the goal of an analysis. At the philosophical level, applied statistics is simply taking real world entities and boiling them down into probabilistic objects, an ontological process that is largely dependent on our goals [@Otsuka2023]. When we move onto data provenance the issue pursists, as what is kept as data and what is tossed away is determined by the motivation of an analysis and what was previously noise can be shown to become signal depending on the resolution of the question [@Meng2014]. After moving onto modelling this issue continues as each research question can be can be categorised as descriptive, predictive, or causal, each of which has its own appropriate statistical methods and motivation agnostic model selection leads to signal devoid of meaning [@Carlin2023]. Even at the final stages of visualisation a lack of understanding of the motivating question make it difficult to untangle what is signal and what is noise, leaving many uncertainty visualisation studies with conflicting results [@Kinkeldey2014]. These cases highlight that the signal that is being conveyed and the noise that hampens that signal cannot be untangled and therefore communicated without a predetermined motivating question. Therefore any discussion of uncertainty must be done within the context of a motivating question as it is exactly what makes the distinction between signal and noise.

## 2.2 Taxonomies
Within the context of a motivating uncertainty can be classified and 
### 2.2.1 Taxonomies of Uncertainty


More commonly uncertainty is defined using a taxonomy rather than a strict definition.

@fig-taxonomy is an illustration of the taxonomy presented by @utypo. In this taxonomy, there are three things we need to consider for each "uncertainty" we encounter through the modelling process. First, we need to consider the source of the uncertainty. Is this uncertainty coming from inaccurate measurements or a poorly defined model? This is the *location* of the uncertainty. Second, consider how well you can quantify this uncertainty. Do you know exactly how much measurement error there is in each observation or are you not even aware if there is a measurement error? This is the *level* of your uncertainty, and it ranges from discrete to total ignorance. Finally, consider how this uncertainty came into existence. Is it a result of a naturally random process (epistemic) or is it due to imperfect information and could be improved (aleatory). This is the *nature* of your uncertainty. @utypo then goes on to describe mapping our uncertainty in a 3D space that is defined by its location, level, and nature, but I think the taxonomy is more easily understood as a series of questions we need to consider when we are trying to quantify uncertainty.

![Depicts an illustration of the taxonomy described in @utypo. From right to left the drawing shows the location, level and nature of uncertainty with examples of that category underneath. A specific source of uncertainty from the location can be mapped to a level of ignorance that can increase or decrease (i.e. moving up or down the green line) depending on the nature of the uncertainty. Identifying the location, level and nature of your uncertainty allows you to better understand it. ](taxonomyvis.jpeg){#fig-taxonomy}

*Notes from other section*
- The usually uncertainty is only considered at the final stage but it needs to be considered through all stages. The big data paradox shrinks confidence intervals but magnifies bias [@Bradley2021] because the confidence intervals shrink due to the sample size but the imprecise target group creates bias.
- @Meng2021 highlights that data needs to be treated as an output from another process rahter than an value neutral input.
- If the distinction between noise and uncertainty is largely task dependent, considerations of quantifying and expressing uncertainty should be considered for the entire duraction of a project; another concept that has been independently identified by several authors [@Kinkeldey2014; @Refsgaard2007]
- The most encompassing definition of uncertainty I have seen comes from @utypo who define uncertainty as **"any deviation from the unachievable ideal of completely deterministic knowledge of the relevant system"**. This definition encapsulates many concepts adjacent to randomness such as predictions, probability mass distributions (PMF), estimate error, and any data that is not a set of deterministic outcomes. This is the definition of uncertainty I will use for the rest of this report.
- While quantification of the uncertainty that arrises from statistical philosophy and data provenance are beyond the scope of this paper, how this uncertainty is communicated is not. The ontological process discussed by @Otsuka2023 where information is tossed as we boil our information down into statistical objects requires us to map our statistical objects back to real world entities. This means the noise introduced and signal lost in earlier stages of the data analysis need to be identified, quantified and expressed in terms of real world outcomes. Visualisation is often the final stage of larger data science project and the down stream effects of poor abstraction and problem at the lower levels become impossible to ignore [@Munzner2009]. 

### 2.2.1 Taxonomies of Visual Uncertainty
  
# 3.General Attitudes...
## 3.1 Towards uncertainty in general
## 3.2 Towards uncertainty visualisation


# 4 The current landscape of uncertainty visualisation research
In this section we will first go over the current state of uncertainty visualisation research, which is currently a collection of evaluation studies and review papers, and discuss what it is doing well and what could be done better. Following that, we will provide general reccomendations from the literature.
# 4.1 The landscape
In section 2.1 it was mentioned that uncertainty must be defined within a specific motivating question, otherwise it inherently does not make sense. A large difficulty with the uncertainty visualisation evalusation studies is that this rule is not followed. There are a shockingly large number of evaluation studies that seem to pay no attention to the information that is relevant to the question they are asking, and show participants a selection of seemingly random visualisations from a statistical point of view. 

- inferential uncertainty and outcome uncertainty ARE NOT THE SAME THING??? they visualise DIFFERENT DISTRIBUTIONS

We define two primary motivations for uncertainty visualisation.
  1) To prevent deterministic conclusions from a random signal (uncertainty as noise)
  2) To convey information about a variance, probability, or other random event (uncertainty as signal)

Notes from previous sections that were moved here
- The concept of uncertainty being task dependent is *particularly* salient for uncertainty visualisation, and it is repeatedly identified as a problem in previous reviews of the uncertainty visualisation literature [@Kinkeldey2014; @Hullman2016] as well as across many sub domains and applications [@Wallsten1997; @Munzner2009; @Fischhoff2014; @Meng2021; @Amar2005]. The fact that this conclusion is repeatedly reached shows both the importance and the lack of acknowledgement this concept receives.
- @Fischhoff2014 discusses how decisions with different levels of flexibility should have different uncertainty visualisations. If the user has no choice they should simply be made aware of the risk, if they have several options, the user needs to see the uncertainty in each, if they need to come up with the options themselves, they need to see how things work and how the system fails.
- @Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations.
- Given that uncertainty estimation and communication seem to be largely task depenent, what questions should we be testing? This leads to several unanswered questions. Is our goal in uncertainty communication to acknowledge and convey the inherrent randomness that exists in all statistical calculations and models? Is it to supress signal that could be the result of random variance? Is it to find the best way to convey the mass function of a random variable? A survey of the literature will leave the answers to these questions unclear, however this inherrent issue has not gone unnoticed.
- Statistics is, at its core, the study of uncertainty. Therefore discussing uncertainty visualisation a a separate sub domain to "normal" data visualisation is inherently confusing. What is typically meant by "uncertainty" visualisation is "noise", that is, we want to present some signal cushioned by its natural variance. Unfortunately, this distinction between "signal" and "noise" is entirely goal dependent.
- @Bella2005 found that most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. (**also the paper sherry sent me**)
# 4.2 Key points from the literature

# 5 Great Examples
- best things you have seen and why they are the best
- can't have a best practices because you need experimental results and my point is that the experiments aren't testing the right things

# 5. Future work/Conclusion




 

# Bibliography

```{r, include=FALSE, eval=FALSE}
library(spelling)
qmd <- "ideas/confirmation/confirmationreport.qmd"
ignore <- readLines("WORDLIST")
check_spelling <- spell_check_files(
  qmd,
  ignore = ignore,
  lang = "en_GB"
)
if (nrow(check_spelling) > 0) {
  print(check_spelling)
  stop("Check spelling in Qmd files!")
}
```