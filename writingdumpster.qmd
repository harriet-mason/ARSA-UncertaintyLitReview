---
title: "Writing Trash Bin"
author: "Harriet Mason"
format: html
---
# Introduction
- @Spiegelhalter2017 noted we "cannot assess the quality of risk communication unless the objectives are clear".
- @Fischhoff2014  takes this approach by considerting how decisions with different levels of flexibility should have different uncertainty visualisations. If the user has no choice they should simply be made aware of the risk, if they have several options, the user needs to see the uncertainty in each, if they need to come up with the options themselves, they need to see how things work and how the system fails.

### Motivation for uncert vis
- @Suh2023 found that the main issue with deployment of AI was communication between data science and subject matter experts, since subject matter experts didn't want technical presentations that didn't explain exactly when and how a model would fail.
- (maybe include some of the noise vs signal stuff)
- The use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as in the transition from fossil fuels to clean energy. The energy sector needs better heuristics to make energy supply analysis less costly to conduct [@Stenclik2021], therefore it is an incredibly relevant application of uncertainty visualisation techniques. 

# Definition Notes
- Failing to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]

## General
- @uncertchap2022 People do not over react when an ensemble member barely misses their town and People overreact more when 1/9 enselbles appear to be hitting their town than when 1/33 ensembles.
- @uncertchap2022 Correll and Gleicher [9] found that violin plots (mapping probability to area or width at a given y position) and gradient plots (mapping probability to opacity) lead to more intuitive assessments of value likelihood and “ability to surprise,” which are more closely aligned with statistical defini- tions of uncertainty." wtf is the statistical definition of uncertainty
- @Thomson2005 titled their paper "A typology for visualizing uncertainty" however it is a taxonomy for the specific types of uncertainty that we may want to visualise and a way to calculate that uncertainty. While this paper is more about mathematics than visualisation, it is clearly written as a precursor to visualisation.
- @Griethe2006 commented that uncertainty appears at every step in the analyis pipeline, and commented on some the mathematical definitions that seek to quantify uncertianty, but did not explain how these mathematical definitions can be combined across levels. Unfortunately this quantification is needed since she mentioned that in order to visualise uncertainty, it needs to be measurable.
- mentions uncertainty can be variance, precision, accuracy, reliability, or related concepts @Hullman2016
- @uncertchap2022 "can enter the analysis pipeline during the mea- surement, modeling, and forecasting phases"


## Redefined uncertainty (cite gap 3)


#### Uncertainty Taxonomies (Cite Gap 7)

- @Gustafson2019 organises uncertainty into the four groups: Deficient: Lack of knowledge; Technical: modelling approximations and measurement error; Scientific Unknown unknowns; Consensus: Disagreement among parties
- @uncertchap2022 mention aleatric, epistemic, ontological divide and also differentiates risk with uncertainty
- Mentioned alleatory vs epistemic uncertainty, in the visualisation literature alleatory uncertainty is more commonly the focus. also mentions dependence and joint pdfs as a consideration @Hullman2016

# Visual Uncertainty Definition

### Context relevance
- most studies used coincident approaches, static visualisations, are applied to a specific domain and dont have generalised results @Kinkeldey2014

### How was uncertainty quantified (Cite Gap 4)
> The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase but go on to *quantify* uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation.
> This definition swap happens so subtly that it seems to go unnoticed by authors writing the papers, and the fact that "uncertainty visualisation" methods dont even notice they are exclusively focus on depictions of easily quantifiable uncertainty, such as error [*HERE*], mass [*HERE*], or variance [*HERE*]
> Many papers will boast a title that claims to be about uncertainty visualisaiton, but simply depicts different visual representations of a PDF, however it takes reading the methodology to find this out

- @uncertchap2022 do specify that they are going to refer to quantified uncertainty that can be visualised ("most commonly as a probability distribution")
- @Griethe2006 mentions that there are detailled classifications of uncertainty but the literature and subsequent techniques are not ballanced, and instead concentrate on scalar values. In facing this lack of ballance she creates a new classification system for uncertainty visualisation. 

### We need cool uncertainty visuailsations (Cite Gap 6)
> This has left the expressions of uncertainty that are hard to quantify or visually differentiate untouched, despite many papers calling for their invention [*HERE*]

- Whether to use classed or unclassed uncertainty is rarely discussed @Kinkeldey2014. e.g. land map with remotely sensed images has ambiguity in the boundaries, which can result from multiple sources (land cover class, measurement error, images from different dates). If you combine them all it might not make sense for someone who only needs one.
- Most studies involve intrinsic approaches such as colour, transparency, and those that used extrinsic techniques typically used glyphs, error bars, grid based techniques, or contouring @Kinkeldey2014
- rarely used extension is aggregation of uncertainty over an area (retrieve overall estimation from a spatial distribution of uncertainty). Other tasks include comparisons, rankings. also have a task called "search" where participants have to identify entities that fulfill certainty characteristics (high or low values) @Kinkeldey2014

### Bar/pie uncertainty graphics (Cite Gap 8)
> For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, however at least one or both of these charts are used in most "uncertainty visualisation" experiments [@Ibrekk1987 *MORE HERE*]

### Vis uncertainty Taxonomies (Cite Gap 9)
- @uncertchap2022 two broad categories of uncertainty visualisation techniques, 1) graphical annotation (mean, error bar, hops) 2) map probability of cI to a visual encoding channel 3) hybrid approaches (e.g. contour boxplot, probability density and interval plot)

### High dimensional vs psychological motivation (Cite Gap 10)
> Papers will often discuss uncertainty in relation to one of these motivating reasons, the evaluation experiments motivated by (1) often perform different visualisations of PDFs, while the papers motivated by (2) will focus on trying to impute uncertainty as error within one of the existing channels, such as colour

- @Griethe2006 clearly sees uncertainty as additional information or data, and therefore sees the uncertainty visualisation problem as a specific case of the high dimensional visualisation issues. 
- It is important to keep in mind that just because information is in a graphic, that does not mean it will be "seen". The phenomena of inattentional blindness shows that there is no perception without attention and it is powerful enough that participants can fail to be aware of random objects appearing on a screen or a gorilla walking through a basketball game [@simons1999gorillas; @mack2003inattentional]. Including uncertainty is worth very little if no attention is left to see it. 
- Glyph maps can be used to depict the multi-dimensional information in spatial-temporal data by mapping line plots to map locations, but we still need to decide what information is important. The map can either present trends in the global or local variance depending on whether or not the line plot is scaled globally or locally [@Wickham2012], however smaller details are almost impossible to convey. No matter how much information we try and put in a plot, there will always be only a handful of key takeaways.
- Elements from a single distribution should be displayed using a single plot, since displaying the features of one distribution across multiple plots makes the information hard to combine and results in some details (such as the estimate error) being completely ignored [@moritz2017trust; Correll2018]. 

# Noise vs Signal
- Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty.
- @uncertchap2022 "Effectively communicating uncertainty is necessary for establishing scientific transparency"
- @Griethe2006 defined uncertainty as "the degree to which the lack of knowledge about the amount of error is responsible for hesitancy in accepting results and observations without caution". 
- confidence intervals provided in text form only are less likely to be misinterpreted than graphics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass.
  - This kind of harks back to the "if our main concern is accuracy, we should show the values in a table" issue.
- A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a].
- Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020].
- The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].
- At least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey.  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 

# Experiment stuff
### Information & Mathematical stuff (Cite Gap 12 & 13 & 14)
- we should aim to show enough information to solve a task while avoiding irrelevant distracting information [@kosslyn2006graph]. 
- @Ibrekk1987 used several displays of a probability density functions, since “formally equivalent representations are often not psychologically equivalent”.
- @Hullman2016 commented on the lack of justification of particular questions used in experiments and the lack of generalisability in using plot ranking as a response technique.
- @Hullman2016 discussed the lack of clarity in exactly what "uncertainty" authors are trying to ask about. Whether the participants are supposed to answer according to some ground truth or their subjective probability is unclear, as is whether the *uncertainty* depicted represents the uncertainty of a sampling or population distribution. 
- @Bella2005 found that most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. (Note: Uncertainty intervals can only be validly compared to a constant value, you compare Data to a theoretical distribution, OR uncertainty to a theoretical threshold (not data). The participants should convert the distributions to A - B and then see if the 95% of THAT distribution overlaps with 0)
- @Blenkinsop2000 tailored their questions according to the visualisation shown, which implies an understanding that different visualisations contain different information. Despite this the authors still compared the visualisations according to which were "useful for uncertainty visualisation". 
- When asking questions, researchers should focus on events that can be repeated (because of a misunderstanding of confidence intervals) @Hullman2016 (moved here because it is clear she is touching on the whole "what are we uncertain about" thing)
- @Nathonours found that a scatter plot is better than a line plot if you want to convey the correlation between two time series but we cannot be sure if this was influenced by swapping the distribution depicted or by dropping the irrelevant feature of inexchangeability.

### Logical fallacy interpretation
- If I tell you my cousins age, and then ask you to guess my age, it would not be unreasonable to guess the age I provided for my cousin. Citing this work to state "people incorrectly believe my cousins age is my age" would
- Deterministic questions about a random event cannot be answered. 
- @uncertchap2022 took the results of other paper to mean "when participants viewed mean temperature forecasts that included 95% confidence intervals depicted as bars with end caps, they incorrectly believed that the error bars represented high and low temperatures" (this one will depend on that study)
- @Boone2018 the method of eliciting prior beleifs was, however questionable. Said people did or did not think something was likely but the question was weird.
- @uncertchap2022 Sources 3, 54, 55, 56 also used to suggest "people interpret uncertainty information as deterministic information". Exists with visualisation and not with text. Suggests this is because of a "visual-spatial bias". suggest HOPs to aleviate this and I refuse to entertain the notion. 

### Participants getting confused by overcomplicated tasks
- @Gschwandtnei2016 When answering “What is the statistical probability (in %) that the interval has already ended at the marked point in time (red line)?” many answers gave the estimated probability to which the marked point in time falls into the interval. This is correct when asking for the probability to which the interval has already started at the marked position. However, in case of the probability to which the interval has already ended at the marked position, this answer is the inverted probability of what we were looking for. They believe this is their own mistake so they so they don’t present these results.
- @Ibrekk1987 highlights that in the face of a vague question, participants will use the plot to decide what the authors mean.
- @uncertchap2022 visual boundaries = cognative categories. Use the cone of uncertainty work as a justification of this. I don't even think THOSE papers found evidence of this. Comment that "Viewers understandably assume the value of a boundary is meaningful, particularly when the information about how the visualization was generated is insufficient" and "Viewers have no choice but to assume that the scientists who made the forecast are indicating an important distinction with the boundary". Shows they know providing irrelevant information is stupid, specify that the issue is that there is rarely a justification for the boundary. YES.

### Value Extraction (Cite Gap 15)
- This idea also extends to interval estimation, where questions about probability are best answered with gradients and questions about start and end times are easiest to answer with ambiguation [@Gschwandtnei2016].
- @uncertchap2022 Attribute substitution: people will substitute a hard mental calculation for an easy one (attribute substitution heuristic). Name "deterministic constructual error" when people try to substitute uncertainty information for deterministic information (e.g. error bars for high and low temperature even when the visualisation was explained - from icing study. This makes sense because temperature is GIVEN as high and lows so it is a natural interpretation). I think this result is in line with the idea that people give the best estimate based on what the visualisation suggests. i.e. people might have assumed you meant "what is the lowest possible temperature". There is a level of "asking a deterministic question for a probabilistic question". 
- @uncertchap2022 Quantile dot plot created so people dont have to visually calculate the area under the curve (so authors are aware of the mental challenge of some of these things) Decrease variance with respect to density plots (it makes sense because you can directly count and quantify raher than guesstimate) (both come from the same source, when-ish is my bus)
- Authors provide little justification for their chosen response models (e.g. absolute accuracy vs relative measures) @Hullman2016 
- @Correll2018 “place your 5 ships on the safest locations on the board.” (minimise danger) It is a convoluted value extraction task, where participants basicaly have to look at the palette and see that if it is in the lower uncertainty, and lower error it is better, but higher uncertainty is better
- @Ibrekk1987 expected participants to calculate the mean using a pie chart that had numerical bins combined (e.g. segment one was 0-2 inches of snow, and segment two was 2-4 inches of snow) so it could not be done by calculation. The authors also expected participants to use a CDF to calculate the mean, which needs to be converted *back* into a PDF to do so mathematically, so it was unclear how participants were supposed to do this calculation visually unless they were to just make a blind guess. The authors interpreted this as the participants misinterpreting how the CDF works, not seeming to realise they gave them an impossible task.
- @Sanyal2009 users performed significantly well using Glyph-size when the task was to search for locations of least uncertainty. However, both Glyph-color and Surface-color performed better than Glyph- size when the task was to search for locations of high uncertainty. We did not expect to find a significant difference between the two search tasks since both the tasks were designed to find extremes in the data
- The tasks for participants were most commonly value retrival (some where uncertainty and data value are retrived separately, others where they are some separately). also also be asked to extract a relative value @Kinkeldey2014

### Decision making stuff (Cite Gap 16)
- Providing a categorical decision alone is somewhat useless [@Joslyn2012], and visualising a single estimate is akin to providing a decision or expressing no uncertainty at all. 
- This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997].
- There is some research that suggests laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Bayesian thinking) [@Hoekstra2014; @Bella2005] but there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. 

- @Boone2018 organised responses from participants by the approach they used
- @Hullman2016 suggested authors look for signs of heuristics in responses and ask subjects to describe their strategy in their response. 
- @Hullman2016 What was particularly interesting was people's tendancy to ignore uncertainty in a plot all together, and use a pattern in the signal (such as distance between means) to answer questions that require uncertainty information. 
- @Kim2019 believed an understanding of participants prior beliefs could assist in interpreting uncertainty visualisation results that tend to be incredibly noisy.
- @Daradkeh2015 found that people tended to to make decisions on a small number of salient pieces of information rather than on systematic evaluations of the costs and benefits of choices.
- @Daradkeh2015 found that people tended to ignore or give little weight probability information and instead focused on the severity of undersierable outcomes.
- believed that offering enough decisions with varying rewards you can observe the true subjective probability, however whether or not probability is the right representation of subjective uncertainty is a topic of debate @Hullman2016
- she also discusses graphical interface (users have to draw the probability distribution) and framing probabilities as natural frequencies are better choices because they reduce noise in responses [@Hullman2016]
- Consider incorporating utility functions. Include "probability-coherence" checks [@Hullman2016]
- evidence that people do not follow the axioms of probability when conveying subjective probabilities. How do you deal with this as an analyst (e.g. if the probabilities do not sum to 1 @Hullman2016
- people will discount information they perceive as discountable if they are able. Including mean estimates on depictions of mass can cause people to discount the uncertainty information and use the difference between means as a proxy for the probability distribution [@Kale2021].
- Some questions (e.g. what are the chances that the no.6 bus will arrive first) you can elicit the viewers subjective probability distribution @Hullman2016

### Please Ignore uncertainty (Cite Gap 17)
- @Blenkinsop2000 did try to include uncertainty as noise, asking participants to search for a specific outcome (the land classified as grass) in a random . However the participants failing to identify the signal (something that would be expected of an uncertainty plot) was seen as a sign the task was too complicated. While the task in this experiment *was* certainty far too complicated (and poorly communicated) this discussion indicates the authors did not consider the signal suppression to be the goal of the visualisation, and rather an annoying distraction. 

### Trust stuff (Cite Gap 18)
- @uncertchap2022  mentions "other studies have found that people trust icon arrays more than other common visualization techniques" without commenting on whether or not that trust is good?
- A 6-month survey of anti-mask groups on Facebook during the COVID-19 pandemic showed that the anti-maskers thought carefully about their grammar of graphics and made persuasive visualisations using the same data as pro-mask groups. They did this by exploiting information ignored by the pro-maskers [@Lee2021].
- displaying numerical estimates of uncertainty information has shown to lead to greater trust in predictions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty regarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

# Communication considerations
## Uncertainty
- Expressing uncertainty verbally decreases the perceived reliability and trustworthiness of the source [@VanderBles2020].
- @Gustafson2019 found that the framing of our uncertainty, (i.e. if the source of uncertainty is from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) was found to not have a detrimental effect on the belief in the estimates, perceived credibility, or behavioural intentions of the decision makers.
- Risk communication also needs to have clear objectives, use plain language, limit information to only what is necessary, and segment the audience to allow for differences in interest and knowledge [@Spiegelhalter2017].
- @Spiegelhalter2017 it is better to understand your audience and iterate towards a final design, rather than only consider one option.
- Some markers of probability that have common uses, probabilities such as 0 (impossible), 0.5 (a fair coin flip), 1 (certain) are easy for people to have an internal sense of [@Hullman2016]
- It is incredibly hard to communicate small risks, because people cannot differentiate between 1/100 an 1/1000, and communicating low probability but high impact risks are particularly challenging [@Spiegelhalter2017]. This issue can be somewhat alleviated by communicating the likelihood of an event in relation to another event on a similar scale (such as being struck by lightning) [@Spiegelhalter2017]. 
- The framing of how we communicate risks matters due to the affect heuristic, so there will be a difference in peoples behaviours if, for example, you tell people they have a 90% chance of survival vs a 10% chance of death, and when expressing a frequency, a larger numerator communicates a larger risk [@Spiegelhalter2017]. 
- The reference class of a particular risk should be explicitly stated to avoid confusion, for example if we are communicating the chance of rain, the time period over which we are expecting this chance of rain (i.e. 50% chance today or this week) needs to be specified [@Spiegelhalter2017]. 
- industrial accidents have a "tollerable" risk chart based on risk and number of fatalities @Spiegelhalter2017
- People believe in the law of small numbers but dont believe in the lar of large numbers @Hullman2016
- experiencing an event makes your percieved probability higher than if you see a description of it @Hullman2016
- Frequency thing means that more effective communication to participants in a study means more effective science and we have a roll on effect @Hullman2016
- Probability judgements are less relative than other psychophysical judgements, 0%, 50% and 100% all have well understood meanings. Subjective probability distributions are spontaneous, and subjects are constructing a distribution upon being asked rather than articulating a fully formed mental representation of their beliefs. Therefore it is subject to heuristics. consider anchoring a scale with farmiliar probabilities . not sure if being asked for the entire distribution helps, e.g. the graphical one reduces noise but asking people for an interval estimation leads to overprecision and overconfidence @Hullman2016
- The big data paradox shrinks confidence intervals but magnifies bias [@Bradley2021] because the confidence intervals shrink due to the sample size but the imprecise target group creates bias.

## Visualisation
- Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012].
- There is a reasonable amount of evidence that cumulative displays or discrete displays (such as a quantile dot-plots or histograms) are the best ways to express mass for decision making and probability estimates [@Fernandes2018; @Hofmann2012; Kay2016; @Hullman2018; @kale2019decision]. 
- Visualisation allow for interactive graphics that provide a more in depth understanding of probability [@Potter2009; @Ancker2009] and 
- Infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 
- Any confusion caused by expressing uncertainty as a visualisation could also be due to a lack of exposure, since @Kay2016 found people repeatedly exposed to the same uncertainty visualisations quickly get better at making judgements.
- Even something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those measures [@Hullman2018; @Goldstein2014].
- @uncertchap2022 Icon array and quantile dot plots both support area and frequency based judgements.
- @uncertchap2022 frequency framing: (e.g. 1 out of 10 rather than 10%) better (plot: icon array, quantile dot plot). 
- @Spiegelhalter2017 even if most audience members can read a plot, they may not read it in the same way. People with high numeracy skills count pictographs, but people with low numeracy assess area, and seemingly unrelated elements may not equally influence all these methods of reading, for example scattering in a pictograph makes counting particularly more difficult. *also add that perceptual task audience thing*
- @Spiegelhalter2017 common sentiments such as avoiding chart junk, considering a table in place of a visualsiaiton, use of narrative labels, managing emotion, less is more approach, etc, are the main concepts expressed. 
- @Spiegelhalter2017 suggests that the graphic should allow "part-to-whole" comparison on an appropriate scale, which seems to be an extension of the framing concept discussed when speaking about uncertainty communication more broadly.
- @Hullman2016 discussed the role of heuristics in the analysis of results
- @Kinkeldey2014 stress the importance of selecting the correct audience, as numerical and visual literacy can differ vaslty between subpopulations so the results from one populations may not reflect the results from another. 
- we consider a summary table as a visualisation, illuminate graphics with words and numbers, design graphics to allow a part-to-whole comparison on an appropriate scale, use images and metaphors to gain/retain attention but don't arouse undue emotion, assume low numeracy of a general public audience and adopt a less-is-more approach by reducing the need for inference and making clear and explicit comparison, be cautious about interactivity and animations (may introduce unnecessary complexity), avoid chart junk (like 3d bar charts) @Spiegelhalter2017
- @Maceachren2012 found __????_ to be the worst aesthetics to map to uncertainty to as they don't have an intuitive interpretation.
- Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. 
- No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012]. 
- This idea also extends to interval estimation, where questions about probability are best answered with gradients and questions about start and end times are easiest to answer with ambiguation [@Gschwandtnei2016].
- coloured maps for earthquake risk are easily interpretable @Spiegelhalter2017

# New Notes
## @Vanderplas2020

1. Introduction
- The development of graphs and charts has been linked to the development of coordinate systems and abstract representations of data
- the aim of most experimental research in statistical graphics focuses on the initial perception and graph comprehension, very little work has been done to understand the effect of charts and graphs on higher cognitive processes such as learning or analysis
1.1 Design of Statistical Graphs
- improved tech allow charts to be made easily for exploratory purposes
- consider purposes of presentation, entertainmen, analysis. Also considr a continum of utalitarian to artistry as a purpose.
- infographic tells a story (has chart junk) vs a statistical graphic is one of intent (and any visual enhancements should contribute to that aim)
- Points out that the ocean colour and state boundaries in the hurriance map only exist for geographic purposes. The map depicted here has the warnings information along with the cone.
1.2 Statistics Mapping using a Grammar of Graphics
- Systems that attempt to categorize charts based on their geometric representations generally make no effort to include all types of graphics, and they have difficulty accommodating charts that may fall into two or more categories. The classifications of graphics based on the underlying components and their relationships, as in the grammar of graphics developed byWilkinson, are more robust; they also provide an elegant framework for comparing different types of graphical representations separate from the underlying data structure. The former is like treating plots like creatures in a zoo, with a unique name for each, while the latter is analogous to having a phylogeny based on genetic data showing how plots are related.
- Has a visual explanation of the grammar of graphics.
- The grammar of graphics creates visual statistics (and conversely, allows plots to be considered statistics). The data plot can then be treated like other statistics: by imagining what the plot might look like in the absence of any structure, we can use the plot of the actual observed data to test for the likelihood of any perceived structure being significant.
- Using the grammar ofgraphics, it is easy for experimenters to compare different types of charts using the same data, as the underlying structure of the graph remains the same. 
- this approach to transformations and scales allows experimenters to easily test judgments made utilizing different axis transformations and color scales to compare perceptual accuracy

2. Testing Methods
- explicit = partiicpants answer specific questions
- implicitly structured=participants must infer the question of interest from the provided stimuli
2.1 Explicitly structured graphical tests
- Preattentive perceptual effects are those that do not require sustained cognitive attention; they are processed automati- cally within the first 500 ms of viewing a chart or graph. Components processed preattentively include color and shape, as well as some basic information about coarse relationships between individual components
- After the preattentive stage, attention is necessary for subsequent processing. Most of the insights we gain from charts and graphs are due to the cognitive processes that occur after attention is focused on specific aspects of the graph.
2.1.1 Preattentive graph perception
- Preattentively perceived features show a near-constant reaction time over increasing display size, while features that are processed attentively show an increasing reaction time with increased dis- play size
- A primary question in the discussion of preattentive graph perception is whether there are advantages in designing a graph to promote the preattentive perception of features, ideally reducing cognitive load.
- We distinguish between tests that use graphical forms and more primitive tests that use basic geometric elements during the testing process. The results from more primitive experimental designs still apply to the design ofgraphs and charts, but the experimental design does not involve any display of actual data
- Preattentively processed features include shape, angle, size, and texture; however, combinations of preattentive features which represent separate features in the data are processed attentively, with at least one major exception hue and brightness are integrated, that is, that even though they can be separately manipulated, they are still perceived preattentively as a single unit. 
- Experiment on this topic are: HealeyCG,BoothKS, Enns JT. 1996. High-speed visual estimation using preattentive processing; and Callaghan TC. 1984. Dimensional interaction of hue and brightness in preattentive field segregation. Percept.
2.1.2 Attention medicated testing methods
2.1.2.1 Direct Observation: numerical estimation, speed, error rates
- simplest ways to test the utility ofa graph is to verify that information can be accurately read from it
- the participants are provided with a chart and asked to estimate some quantity or answer a predefined question using the information provided in the chart
- It is important to ensure that the specific charts and questions used are aligned; studies are commonly critiqued on the basis that the charts or the questions were not appropriate for the task
- Not always perceptual ranking. The point vs line example in figure 8 shows there is some cases where perceptual tasks do not perfectly differentiate? Or is it that the PDF you highlight is of importance.
- open-ended estimation tasks elicit certain well-known biases such as the tendency to round to multiples of 5 or 10
- To approach situations with more complicated graphics, or charts that are known to induce perceptual biases in the participants, consider using eye tracking to measure attention and motivation, or the use of verbal descriptions to assess more complicated graphs (since they facilitate specific parts of the perceptual process)
2.1.2.2 Psychophysics and signal detection theory
- Psychophysical experimental design is focused on whether an effect is detectable and whether the magnitude of the effect can be accurately estimated
- Common methods, such as the method of constant stimuli and the method of adjustment, involve repeatedly presenting a participant with charts and asking them to evaluate the chart on the basis of a particular question of interest
- In the method of adjustment, this is done with the control of the participant, who adjusts the stimuli interactively until the effect is just barely noticeable; in the method of constant stimuli, the effect size changes randomly from trial to trial to reduce continuity effects.
- Psychophysics methods also seem to be relatively common in studies of map perception, particularly when the goal is to estimate the amount of exaggeration or other corrective distortion necessary for realistic perception of the map (for
2.1.2.3 Thinking aloud
- ask participants to talk through their thoughts as they read and use a graph in a realistic setting
- allows experimenters to examine the use ofcom- plex graphics in the wild, or at least in situations that are less artificial than the paradigms allowed by numerical estimation and psychophysics methods
- allow researchers to attempt to measure insight, and reasoning in complex situations such as experimental design, decision-making, or the process of weather fore- casting
- While the data that result from the think-aloud protocol are typically more qualitative and less quantitative than results produced using other methods, they provide significant additional insight into the underlying cognitive processes affecting visualization, which cannot be obtained through other means
2.1.2.4 Eye tracking
- eye tracking facilitates insight into the process of visual attention, providing data on the approximate spatial location of visual focus
- Eye tracking allows researchers to determine that viewers spend relatively little time examining the axes in scatter plots, but significant amounts of time examining the axes in parallel coordinates plots, suggesting that the process of reading these two chart types is fundamentally different
- Can also be used to identify features that provide useful information during the graph reading process for several different types of charts
2.1.2.5 Combination Experiemnts
2.2 Implicit Graphical Tests Using Visual Inference
- In an implicit graphical test, the user must identify both the purpose and function of the plot and use that information to evaluate the plot as shown
- Explicit tests are typically conducted on plots that have been created to showcase specific structure in the data in order to present results; in contrast, implicit tests are designed to inform exploratory data analysis and the iterative model diagnostic process. 
- e.g. lineup plot
- The advantage to implicit testing is that lineups do not require a specification of a feature of interest in the testing framework
- Much ofthe historical research of comparing different types of charts has been criticized because the specific question phrasing does not provide readily generalizable results; the lineup protocol removes this obstacle by charging the user with the task of identifying the most different looking plot and thereby selecting the feature with the visually most salient difference compared with the other plots. 
- The real power ofthe lineup protocol is that when combined with the grammar of graphics, we can hold the underlying data and summary statistics constant, isolating the effect ofdifferent plot types, coordinate transformations, and aesthetic mappings on our ability to detect effects in the data.
- It can be difficult to specify the null data-generating model in a way that adequately mimics the data plots, which suggests that visually, we are able to identify many more features than those typically tested using standard quantitative hypothesis tests. This implicit testing of many different hypotheses does make null distribution specification challenging, but it also highlights the power ofvisual cognition to detect subtle differences in data.

3 Current Best Graphical Practice
- no best plot can lead to a rather fragmented approach when describing best practice, and so in order to avoid this, we examine graphical practice using the principle of “first, do no harm” from the Hippocratic oath.
3.1 Cognitive Principles
- Take advantage of human visual system and allows us to make charts that require less cognitive effort to read
3.1.1 Proximity
- objects or shapes that are close to one another appear to form groups. For plot design, proximity is used to place items to compare close together, and less important comparisons further apart.
3.1.2 Similarity
- The gestalt principle of similarity suggests that we group things that have similar appearances and exclude objects with different appearances. 
. Vanderplas & Hofmann (2017) showed that the addition of color and shape to a scatter plot increases the likelihood that individuals will perceive clustered groups of points.
- the coloring of bars allows us to easily see that the sim- ilarly colored rectangles represent the same group of people, even though the bars are separated by facets and other groups.
3.1.3 Common Region
- The gestalt principle of common region suggests that elements contained within a common region belong together.
- confidence bands and bounding ellipses also activate this gestalt principle by grouping points within the boundaries together (Vanderplas & Hofmann 2017), highlighting the presence of outliers that do not belong to the main group.
3.1.4 Common Fate
- The gestalt principle of common fate describes the tendency to group objects that are moving together in the same direction and at the same speed together
- Common fate is certainly active in animated plots that use fading or transitions over time, but even in static plots, continuity can be activated when multiple time series plots are shown together (common fate creates bias in identifying correlation)
- Strong negative association is not easily detected from overlaid line plots, but it is easily seen in a scatter plot (makes me think of that HOPS plot experiment)
3.1.5 Working Memory
- Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require signifi- cant attentional resources, because it is not possible to hold the legend mapping in working memory.
3.1.6 Change Blindness
- change blindness, affects both static and interactive plots. In static plots, it can be difficult to compare between dif- ferent small multiples or facets because the contents of the plots are not reliably represented in workingmemorywhen switching attention between them. In animated plots, it is important to use transition effects to connect successive frames of the animation: This reduces change blindness and also activates the gestalt principle of common fate, allowing us to quickly identify groups of objects that are transitioning in the same direction
3.1.7 Ease of Comparison
- Much of the psychophysics research on statistical charts exam- ines the accuracy of comparisons and quantitative evaluations made during the process of under- standing a plot.
-This ranking ofcognitive tasks provides some consistent guidance for chart design: If the same data can be represented in a way that allows the user to make a comparison more accurately (based on the hierarchy), then that design is preferable.
- While the hierarchy of graphical comparisons provides some guidance, there are other design choices that can be informed by experimental research in a less systematic way.
3.2 Chart Design
- The use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification ofcorresponding data features. 
- In addition, dual encoding increases the accessibility ofa chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia).
3.2.1 Colour
- Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use ofblue for cold temperatures and red/orange for warm temperatures.
- It is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors be- cause the human perceptual system evolved to work in the natural world, where shades of green are plentiful.
- the distance between points in a given color space may not be the same as the distance between points in perceptual space
- Color-blindness is common enough that it is reasonable to expect that any given chart used in a presentation or publication will be read by someone with a color perception deficiency. The use of dual encoding allows color-blind individuals to more readily read graphics that utilize color, and as hue and lightness can be varied separately, it is possible to use dual encoding without adding another aesthetic.

4. Open Questions and Future Research
- Another open question is the acceptance of results—much historical research on best practices exists, but how much of it is being put into practice? One

## @Tierney2023

## @Pu2020

## @Leland2005

# General thoughts/questions
- Integrable vs separable
  - kinsley did not mean it in the way that the integrable seperable paper means it. They basically meant colour on the same axis
  - should read [this paper](https://dl.acm.org/doi/fullHtml/10.1145/3290605.3300899#:~:text=Two%20visual%20channels%20are%20considered,from%20the%20other%20%5B48%5D.) and others along the same lines.
- grammar of graphics describes what your visualisation *is* given the data you have. The reason the grammar of graphics is not equipt to handle uncertainty is the same reason the tidyverse is not equipt to handle it. It is generated and carries through between these independent steps.
  - unclear how uncertainty from the source would make it in.
  - Grammar of graphics describes what *is*. Uncertainty is what *might be*.
  - Every uncertainty visualisation is entirely definable within the grammar of graphics. The *difficulty* with implementing uncertainty does not come from a failing within the grammar of graphics but from the absent definition of uncertainty. Just like normal statistics, visual statistics defined by the grammar of graphics *must* be well defined, if they are not, they cannot be implemented in the grammar. 
- Does the flip (hypothesis test vs uncertainty visualisation) have mathematical implications in how the plots should be assessed? This implies a visible signal in an uncertainty plot should indicate some divergence from the null.
- This increased sensisitivity could be the result of another aspect of uncertainty that is ignored. Statstical tests are typically built upon a series of assumptions, and it is difficult to identify *which* assumption failure caused the data to fail the statistical tests. Lineup plots allow us to see *how* our data is different to the assumed distribution, and better understand *why* our data may have failled a test ^[Did Patricks paper compare statistical tests to lineup protocols in the event an earlier assumption of the tests, such as, independent observation? I think he did check some. I wonder if you can quantify the information difference you get from a rejected statistical test vs a rejected line plot]. 
- If information that can be used to calculate a statistic, but not the statistic itself, is shown, people will need to use a heuristic to extract that information which will decrease the precision with which that information is communicated. 
- put uncertainty in the context of other meta-data of variables
	
# Removed Sections
@Munzner2009 created a nested model for visualisation that highlighted how the first mistake that can be made in a visualisation is in the problem characterisation, and failing to do it well can cause downstream effects and damage the effectiveness of a visualisation.

The absence of an encompassing definition of uncertainty is mentioned by every uncertainty visualisation directly [@Spiegelhalter2017; @Griethe2006] or indirectly by describing a myriad of ways it can be considered in the literature [@Kinkeldey2014; @Hullman2016], although it is never commented on as a source of the noise in the field.

In order to visualise uncertainty, it needs to be quantifiable [@Griethe2006; Leland2005], in order to quantify uncertainty, it needs to be mathematically defined. The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase, (e.g. @utypo defined it as any deviation from complete determinism) but go on to quantify uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation. This definition swap happens so subtly that it seems to go unnoticed by authors

Utilizing bias and variance to define uncertainty is well-established, but it is absent concerns about imputed data, assumptions, sampling methods, and even the human element of analyst decisions
