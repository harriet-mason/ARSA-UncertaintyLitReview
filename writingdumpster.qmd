---
title: "Writing Trash Bin"
author: "Harriet Mason"
format: html
---
# Introduction
- @Spiegelhalter2017 noted we "cannot assess the quality of risk communication unless the objectives are clear".
- @Fischhoff2014  takes this approach by considerting how decisions with different levels of flexibility should have different uncertainty visualisations. If the user has no choice they should simply be made aware of the risk, if they have several options, the user needs to see the uncertainty in each, if they need to come up with the options themselves, they need to see how things work and how the system fails.

### Motivation for uncert vis
- @Suh2023 found that the main issue with deployment of AI was communication between data science and subject matter experts, since subject matter experts didn't want technical presentations that didn't explain exactly when and how a model would fail.
- (maybe include some of the noise vs signal stuff)
- The use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as in the transition from fossil fuels to clean energy. The energy sector needs better heuristics to make energy supply analysis less costly to conduct [@Stenclik2021], therefore it is an incredibly relevant application of uncertainty visualisation techniques. 

# Definition Notes
- Failing to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]

## General
- @uncertchap2022 People do not over react when an ensemble member barely misses their town and People overreact more when 1/9 enselbles appear to be hitting their town than when 1/33 ensembles.
- @uncertchap2022 Correll and Gleicher [9] found that violin plots (mapping probability to area or width at a given y position) and gradient plots (mapping probability to opacity) lead to more intuitive assessments of value likelihood and “ability to surprise,” which are more closely aligned with statistical defini- tions of uncertainty." wtf is the statistical definition of uncertainty
- @Thomson2005 titled their paper "A typology for visualizing uncertainty" however it is a taxonomy for the specific types of uncertainty that we may want to visualise and a way to calculate that uncertainty. While this paper is more about mathematics than visualisation, it is clearly written as a precursor to visualisation.
- @Griethe2006 commented that uncertainty appears at every step in the analyis pipeline, and commented on some the mathematical definitions that seek to quantify uncertianty, but did not explain how these mathematical definitions can be combined across levels. Unfortunately this quantification is needed since she mentioned that in order to visualise uncertainty, it needs to be measurable.
- mentions uncertainty can be variance, precision, accuracy, reliability, or related concepts @Hullman2016
- @uncertchap2022 "can enter the analysis pipeline during the mea- surement, modeling, and forecasting phases"


## Redefined uncertainty (cite gap 3)


#### Uncertainty Taxonomies (Cite Gap 7)

- @Gustafson2019 organises uncertainty into the four groups: Deficient: Lack of knowledge; Technical: modelling approximations and measurement error; Scientific Unknown unknowns; Consensus: Disagreement among parties
- @uncertchap2022 mention aleatric, epistemic, ontological divide and also differentiates risk with uncertainty
- Mentioned alleatory vs epistemic uncertainty, in the visualisation literature alleatory uncertainty is more commonly the focus. also mentions dependence and joint pdfs as a consideration @Hullman2016

# Visual Uncertainty Definition

### Context relevance
- most studies used coincident approaches, static visualisations, are applied to a specific domain and dont have generalised results @Kinkeldey2014

### How was uncertainty quantified (Cite Gap 4)
> The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase but go on to *quantify* uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation.
> This definition swap happens so subtly that it seems to go unnoticed by authors writing the papers, and the fact that "uncertainty visualisation" methods dont even notice they are exclusively focus on depictions of easily quantifiable uncertainty, such as error [*HERE*], mass [*HERE*], or variance [*HERE*]
> Many papers will boast a title that claims to be about uncertainty visualisaiton, but simply depicts different visual representations of a PDF, however it takes reading the methodology to find this out

- @uncertchap2022 do specify that they are going to refer to quantified uncertainty that can be visualised ("most commonly as a probability distribution")
- @Griethe2006 mentions that there are detailled classifications of uncertainty but the literature and subsequent techniques are not ballanced, and instead concentrate on scalar values. In facing this lack of ballance she creates a new classification system for uncertainty visualisation. 

### We need cool uncertainty visuailsations (Cite Gap 6)
> This has left the expressions of uncertainty that are hard to quantify or visually differentiate untouched, despite many papers calling for their invention [*HERE*]

- Whether to use classed or unclassed uncertainty is rarely discussed @Kinkeldey2014. e.g. land map with remotely sensed images has ambiguity in the boundaries, which can result from multiple sources (land cover class, measurement error, images from different dates). If you combine them all it might not make sense for someone who only needs one.
- Most studies involve intrinsic approaches such as colour, transparency, and those that used extrinsic techniques typically used glyphs, error bars, grid based techniques, or contouring @Kinkeldey2014
- rarely used extension is aggregation of uncertainty over an area (retrieve overall estimation from a spatial distribution of uncertainty). Other tasks include comparisons, rankings. also have a task called "search" where participants have to identify entities that fulfill certainty characteristics (high or low values) @Kinkeldey2014

### Bar/pie uncertainty graphics (Cite Gap 8)
> For example @Leland2005 mentions that popular graphics, such as pie charts and bar charts omit uncertainty, however at least one or both of these charts are used in most "uncertainty visualisation" experiments [@Ibrekk1987 *MORE HERE*]

### Vis uncertainty Taxonomies (Cite Gap 9)
- @uncertchap2022 two broad categories of uncertainty visualisation techniques, 1) graphical annotation (mean, error bar, hops) 2) map probability of cI to a visual encoding channel 3) hybrid approaches (e.g. contour boxplot, probability density and interval plot)

### High dimensional vs psychological motivation (Cite Gap 10)
> Papers will often discuss uncertainty in relation to one of these motivating reasons, the evaluation experiments motivated by (1) often perform different visualisations of PDFs, while the papers motivated by (2) will focus on trying to impute uncertainty as error within one of the existing channels, such as colour

- @Griethe2006 clearly sees uncertainty as additional information or data, and therefore sees the uncertainty visualisation problem as a specific case of the high dimensional visualisation issues. 
- It is important to keep in mind that just because information is in a graphic, that does not mean it will be "seen". The phenomena of inattentional blindness shows that there is no perception without attention and it is powerful enough that participants can fail to be aware of random objects appearing on a screen or a gorilla walking through a basketball game [@simons1999gorillas; @mack2003inattentional]. Including uncertainty is worth very little if no attention is left to see it. 
- Glyph maps can be used to depict the multi-dimensional information in spatial-temporal data by mapping line plots to map locations, but we still need to decide what information is important. The map can either present trends in the global or local variance depending on whether or not the line plot is scaled globally or locally [@Wickham2012], however smaller details are almost impossible to convey. No matter how much information we try and put in a plot, there will always be only a handful of key takeaways.
- Elements from a single distribution should be displayed using a single plot, since displaying the features of one distribution across multiple plots makes the information hard to combine and results in some details (such as the estimate error) being completely ignored [@moritz2017trust; Correll2018]. 

# Noise vs Signal
- Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty.
- @uncertchap2022 "Effectively communicating uncertainty is necessary for establishing scientific transparency"
- @Griethe2006 defined uncertainty as "the degree to which the lack of knowledge about the amount of error is responsible for hesitancy in accepting results and observations without caution". 
- confidence intervals provided in text form only are less likely to be misinterpreted than graphics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass.
  - This kind of harks back to the "if our main concern is accuracy, we should show the values in a table" issue.
- A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a].
- Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020].
- The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].
- At least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey.  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 

# Experiment stuff
### Information & Mathematical stuff (Cite Gap 12 & 13 & 14)
- we should aim to show enough information to solve a task while avoiding irrelevant distracting information [@kosslyn2006graph]. 
- @Ibrekk1987 used several displays of a probability density functions, since “formally equivalent representations are often not psychologically equivalent”.
- @Hullman2016 commented on the lack of justification of particular questions used in experiments and the lack of generalisability in using plot ranking as a response technique.
- @Hullman2016 discussed the lack of clarity in exactly what "uncertainty" authors are trying to ask about. Whether the participants are supposed to answer according to some ground truth or their subjective probability is unclear, as is whether the *uncertainty* depicted represents the uncertainty of a sampling or population distribution. 
- @Bella2005 found that most participants were ignorant to the fact that error bars are used for both confidence intervals and standard error bars, two wildly different indicators of precision. (Note: Uncertainty intervals can only be validly compared to a constant value, you compare Data to a theoretical distribution, OR uncertainty to a theoretical threshold (not data). The participants should convert the distributions to A - B and then see if the 95% of THAT distribution overlaps with 0)
- @Blenkinsop2000 tailored their questions according to the visualisation shown, which implies an understanding that different visualisations contain different information. Despite this the authors still compared the visualisations according to which were "useful for uncertainty visualisation". 
- When asking questions, researchers should focus on events that can be repeated (because of a misunderstanding of confidence intervals) @Hullman2016 (moved here because it is clear she is touching on the whole "what are we uncertain about" thing)
- @Nathonours found that a scatter plot is better than a line plot if you want to convey the correlation between two time series but we cannot be sure if this was influenced by swapping the distribution depicted or by dropping the irrelevant feature of inexchangeability.

### Logical fallacy interpretation
- If I tell you my cousins age, and then ask you to guess my age, it would not be unreasonable to guess the age I provided for my cousin. Citing this work to state "people incorrectly believe my cousins age is my age" would
- Deterministic questions about a random event cannot be answered. 
- @uncertchap2022 took the results of other paper to mean "when participants viewed mean temperature forecasts that included 95% confidence intervals depicted as bars with end caps, they incorrectly believed that the error bars represented high and low temperatures" (this one will depend on that study)
- @Boone2018 the method of eliciting prior beleifs was, however questionable. Said people did or did not think something was likely but the question was weird.
- @uncertchap2022 Sources 3, 54, 55, 56 also used to suggest "people interpret uncertainty information as deterministic information". Exists with visualisation and not with text. Suggests this is because of a "visual-spatial bias". suggest HOPs to aleviate this and I refuse to entertain the notion. 

### Participants getting confused by overcomplicated tasks
- @Gschwandtnei2016 When answering “What is the statistical probability (in %) that the interval has already ended at the marked point in time (red line)?” many answers gave the estimated probability to which the marked point in time falls into the interval. This is correct when asking for the probability to which the interval has already started at the marked position. However, in case of the probability to which the interval has already ended at the marked position, this answer is the inverted probability of what we were looking for. They believe this is their own mistake so they so they don’t present these results.
- @Ibrekk1987 highlights that in the face of a vague question, participants will use the plot to decide what the authors mean.
- @uncertchap2022 visual boundaries = cognative categories. Use the cone of uncertainty work as a justification of this. I don't even think THOSE papers found evidence of this. Comment that "Viewers understandably assume the value of a boundary is meaningful, particularly when the information about how the visualization was generated is insufficient" and "Viewers have no choice but to assume that the scientists who made the forecast are indicating an important distinction with the boundary". Shows they know providing irrelevant information is stupid, specify that the issue is that there is rarely a justification for the boundary. YES.

### Value Extraction (Cite Gap 15)
- This idea also extends to interval estimation, where questions about probability are best answered with gradients and questions about start and end times are easiest to answer with ambiguation [@Gschwandtnei2016].
- @uncertchap2022 Attribute substitution: people will substitute a hard mental calculation for an easy one (attribute substitution heuristic). Name "deterministic constructual error" when people try to substitute uncertainty information for deterministic information (e.g. error bars for high and low temperature even when the visualisation was explained - from icing study. This makes sense because temperature is GIVEN as high and lows so it is a natural interpretation). I think this result is in line with the idea that people give the best estimate based on what the visualisation suggests. i.e. people might have assumed you meant "what is the lowest possible temperature". There is a level of "asking a deterministic question for a probabilistic question". 
- @uncertchap2022 Quantile dot plot created so people dont have to visually calculate the area under the curve (so authors are aware of the mental challenge of some of these things) Decrease variance with respect to density plots (it makes sense because you can directly count and quantify raher than guesstimate) (both come from the same source, when-ish is my bus)
- Authors provide little justification for their chosen response models (e.g. absolute accuracy vs relative measures) @Hullman2016 
- @Correll2018 “place your 5 ships on the safest locations on the board.” (minimise danger) It is a convoluted value extraction task, where participants basicaly have to look at the palette and see that if it is in the lower uncertainty, and lower error it is better, but higher uncertainty is better
- @Ibrekk1987 expected participants to calculate the mean using a pie chart that had numerical bins combined (e.g. segment one was 0-2 inches of snow, and segment two was 2-4 inches of snow) so it could not be done by calculation. The authors also expected participants to use a CDF to calculate the mean, which needs to be converted *back* into a PDF to do so mathematically, so it was unclear how participants were supposed to do this calculation visually unless they were to just make a blind guess. The authors interpreted this as the participants misinterpreting how the CDF works, not seeming to realise they gave them an impossible task.
- @Sanyal2009 users performed significantly well using Glyph-size when the task was to search for locations of least uncertainty. However, both Glyph-color and Surface-color performed better than Glyph- size when the task was to search for locations of high uncertainty. We did not expect to find a significant difference between the two search tasks since both the tasks were designed to find extremes in the data
- The tasks for participants were most commonly value retrival (some where uncertainty and data value are retrived separately, others where they are some separately). also also be asked to extract a relative value @Kinkeldey2014

### Decision making stuff (Cite Gap 16)
- Providing a categorical decision alone is somewhat useless [@Joslyn2012], and visualising a single estimate is akin to providing a decision or expressing no uncertainty at all. 
- This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997].
- There is some research that suggests laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Bayesian thinking) [@Hoekstra2014; @Bella2005] but there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. 

- @Boone2018 organised responses from participants by the approach they used
- @Hullman2016 suggested authors look for signs of heuristics in responses and ask subjects to describe their strategy in their response. 
- @Hullman2016 What was particularly interesting was people's tendancy to ignore uncertainty in a plot all together, and use a pattern in the signal (such as distance between means) to answer questions that require uncertainty information. 
- @Kim2019 believed an understanding of participants prior beliefs could assist in interpreting uncertainty visualisation results that tend to be incredibly noisy.
- @Daradkeh2015 found that people tended to to make decisions on a small number of salient pieces of information rather than on systematic evaluations of the costs and benefits of choices.
- @Daradkeh2015 found that people tended to ignore or give little weight probability information and instead focused on the severity of undersierable outcomes.
- believed that offering enough decisions with varying rewards you can observe the true subjective probability, however whether or not probability is the right representation of subjective uncertainty is a topic of debate @Hullman2016
- she also discusses graphical interface (users have to draw the probability distribution) and framing probabilities as natural frequencies are better choices because they reduce noise in responses [@Hullman2016]
- Consider incorporating utility functions. Include "probability-coherence" checks [@Hullman2016]
- evidence that people do not follow the axioms of probability when conveying subjective probabilities. How do you deal with this as an analyst (e.g. if the probabilities do not sum to 1 @Hullman2016
- people will discount information they perceive as discountable if they are able. Including mean estimates on depictions of mass can cause people to discount the uncertainty information and use the difference between means as a proxy for the probability distribution [@Kale2021].
- Some questions (e.g. what are the chances that the no.6 bus will arrive first) you can elicit the viewers subjective probability distribution @Hullman2016

### Please Ignore uncertainty (Cite Gap 17)
- @Blenkinsop2000 did try to include uncertainty as noise, asking participants to search for a specific outcome (the land classified as grass) in a random . However the participants failing to identify the signal (something that would be expected of an uncertainty plot) was seen as a sign the task was too complicated. While the task in this experiment *was* certainty far too complicated (and poorly communicated) this discussion indicates the authors did not consider the signal suppression to be the goal of the visualisation, and rather an annoying distraction. 

### Trust stuff (Cite Gap 18)
- @uncertchap2022  mentions "other studies have found that people trust icon arrays more than other common visualization techniques" without commenting on whether or not that trust is good?
- A 6-month survey of anti-mask groups on Facebook during the COVID-19 pandemic showed that the anti-maskers thought carefully about their grammar of graphics and made persuasive visualisations using the same data as pro-mask groups. They did this by exploiting information ignored by the pro-maskers [@Lee2021].
- displaying numerical estimates of uncertainty information has shown to lead to greater trust in predictions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty regarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

# Communication considerations
## Uncertainty
- Expressing uncertainty verbally decreases the perceived reliability and trustworthiness of the source [@VanderBles2020].
- @Gustafson2019 found that the framing of our uncertainty, (i.e. if the source of uncertainty is from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) was found to not have a detrimental effect on the belief in the estimates, perceived credibility, or behavioural intentions of the decision makers.
- Risk communication also needs to have clear objectives, use plain language, limit information to only what is necessary, and segment the audience to allow for differences in interest and knowledge [@Spiegelhalter2017].
- @Spiegelhalter2017 it is better to understand your audience and iterate towards a final design, rather than only consider one option.
- Some markers of probability that have common uses, probabilities such as 0 (impossible), 0.5 (a fair coin flip), 1 (certain) are easy for people to have an internal sense of [@Hullman2016]
- It is incredibly hard to communicate small risks, because people cannot differentiate between 1/100 an 1/1000, and communicating low probability but high impact risks are particularly challenging [@Spiegelhalter2017]. This issue can be somewhat alleviated by communicating the likelihood of an event in relation to another event on a similar scale (such as being struck by lightning) [@Spiegelhalter2017]. 
- The framing of how we communicate risks matters due to the affect heuristic, so there will be a difference in peoples behaviours if, for example, you tell people they have a 90% chance of survival vs a 10% chance of death, and when expressing a frequency, a larger numerator communicates a larger risk [@Spiegelhalter2017]. 
- The reference class of a particular risk should be explicitly stated to avoid confusion, for example if we are communicating the chance of rain, the time period over which we are expecting this chance of rain (i.e. 50% chance today or this week) needs to be specified [@Spiegelhalter2017]. 
- industrial accidents have a "tollerable" risk chart based on risk and number of fatalities @Spiegelhalter2017
- People believe in the law of small numbers but dont believe in the lar of large numbers @Hullman2016
- experiencing an event makes your percieved probability higher than if you see a description of it @Hullman2016
- Frequency thing means that more effective communication to participants in a study means more effective science and we have a roll on effect @Hullman2016
- Probability judgements are less relative than other psychophysical judgements, 0%, 50% and 100% all have well understood meanings. Subjective probability distributions are spontaneous, and subjects are constructing a distribution upon being asked rather than articulating a fully formed mental representation of their beliefs. Therefore it is subject to heuristics. consider anchoring a scale with farmiliar probabilities . not sure if being asked for the entire distribution helps, e.g. the graphical one reduces noise but asking people for an interval estimation leads to overprecision and overconfidence @Hullman2016
- The big data paradox shrinks confidence intervals but magnifies bias [@Bradley2021] because the confidence intervals shrink due to the sample size but the imprecise target group creates bias.

## Visualisation
- Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012].
- There is a reasonable amount of evidence that cumulative displays or discrete displays (such as a quantile dot-plots or histograms) are the best ways to express mass for decision making and probability estimates [@Fernandes2018; @Hofmann2012; Kay2016; @Hullman2018; @kale2019decision]. 
- Visualisation allow for interactive graphics that provide a more in depth understanding of probability [@Potter2009; @Ancker2009] and 
- Infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 
- Any confusion caused by expressing uncertainty as a visualisation could also be due to a lack of exposure, since @Kay2016 found people repeatedly exposed to the same uncertainty visualisations quickly get better at making judgements.
- Even something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those measures [@Hullman2018; @Goldstein2014].
- @uncertchap2022 Icon array and quantile dot plots both support area and frequency based judgements.
- @uncertchap2022 frequency framing: (e.g. 1 out of 10 rather than 10%) better (plot: icon array, quantile dot plot). 
- @Spiegelhalter2017 even if most audience members can read a plot, they may not read it in the same way. People with high numeracy skills count pictographs, but people with low numeracy assess area, and seemingly unrelated elements may not equally influence all these methods of reading, for example scattering in a pictograph makes counting particularly more difficult. *also add that perceptual task audience thing*
- @Spiegelhalter2017 common sentiments such as avoiding chart junk, considering a table in place of a visualsiaiton, use of narrative labels, managing emotion, less is more approach, etc, are the main concepts expressed. 
- @Spiegelhalter2017 suggests that the graphic should allow "part-to-whole" comparison on an appropriate scale, which seems to be an extension of the framing concept discussed when speaking about uncertainty communication more broadly.
- @Hullman2016 discussed the role of heuristics in the analysis of results
- @Kinkeldey2014 stress the importance of selecting the correct audience, as numerical and visual literacy can differ vaslty between subpopulations so the results from one populations may not reflect the results from another. 
- we consider a summary table as a visualisation, illuminate graphics with words and numbers, design graphics to allow a part-to-whole comparison on an appropriate scale, use images and metaphors to gain/retain attention but don't arouse undue emotion, assume low numeracy of a general public audience and adopt a less-is-more approach by reducing the need for inference and making clear and explicit comparison, be cautious about interactivity and animations (may introduce unnecessary complexity), avoid chart junk (like 3d bar charts) @Spiegelhalter2017
- @Maceachren2012 found __????_ to be the worst aesthetics to map to uncertainty to as they don't have an intuitive interpretation.
- Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. 
- No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012]. 
- This idea also extends to interval estimation, where questions about probability are best answered with gradients and questions about start and end times are easiest to answer with ambiguation [@Gschwandtnei2016].
- coloured maps for earthquake risk are easily interpretable @Spiegelhalter2017

# New Notes
## @Vanderplas2020

1. Introduction
- The development of graphs and charts has been linked to the development of coordinate systems and abstract representations of data
- the aim of most experimental research in statistical graphics focuses on the initial perception and graph comprehension, very little work has been done to understand the effect of charts and graphs on higher cognitive processes such as learning or analysis
1.1 Design of Statistical Graphs
- improved tech allow charts to be made easily for exploratory purposes
- consider purposes of presentation, entertainmen, analysis. Also considr a continum of utalitarian to artistry as a purpose.
- infographic tells a story (has chart junk) vs a statistical graphic is one of intent (and any visual enhancements should contribute to that aim)
- Points out that the ocean colour and state boundaries in the hurriance map only exist for geographic purposes. The map depicted here has the warnings information along with the cone.
1.2 Statistics Mapping using a Grammar of Graphics
- Systems that attempt to categorize charts based on their geometric representations generally make no effort to include all types of graphics, and they have difficulty accommodating charts that may fall into two or more categories. The classifications of graphics based on the underlying components and their relationships, as in the grammar of graphics developed by Wilkinson, are more robust; they also provide an elegant framework for comparing different types of graphical representations separate from the underlying data structure. The former is like treating plots like creatures in a zoo, with a unique name for each, while the latter is analogous to having a phylogeny based on genetic data showing how plots are related.
- Has a visual explanation of the grammar of graphics.
- The grammar of graphics creates visual statistics (and conversely, allows plots to be considered statistics). The data plot can then be treated like other statistics: by imagining what the plot might look like in the absence of any structure, we can use the plot of the actual observed data to test for the likelihood of any perceived structure being significant.
- Using the grammar ofgraphics, it is easy for experimenters to compare different types of charts using the same data, as the underlying structure of the graph remains the same. 
- this approach to transformations and scales allows experimenters to easily test judgments made utilizing different axis transformations and color scales to compare perceptual accuracy

2. Testing Methods
- explicit = partiicpants answer specific questions
- implicitly structured=participants must infer the question of interest from the provided stimuli
2.1 Explicitly structured graphical tests
- Preattentive perceptual effects are those that do not require sustained cognitive attention; they are processed automati- cally within the first 500 ms of viewing a chart or graph. Components processed preattentively include color and shape, as well as some basic information about coarse relationships between individual components
- After the preattentive stage, attention is necessary for subsequent processing. Most of the insights we gain from charts and graphs are due to the cognitive processes that occur after attention is focused on specific aspects of the graph.
2.1.1 Preattentive graph perception
- Preattentively perceived features show a near-constant reaction time over increasing display size, while features that are processed attentively show an increasing reaction time with increased dis- play size
- A primary question in the discussion of preattentive graph perception is whether there are advantages in designing a graph to promote the preattentive perception of features, ideally reducing cognitive load.
- We distinguish between tests that use graphical forms and more primitive tests that use basic geometric elements during the testing process. The results from more primitive experimental designs still apply to the design ofgraphs and charts, but the experimental design does not involve any display of actual data
- Preattentively processed features include shape, angle, size, and texture; however, combinations of preattentive features which represent separate features in the data are processed attentively, with at least one major exception hue and brightness are integrated, that is, that even though they can be separately manipulated, they are still perceived preattentively as a single unit. 
- Experiment on this topic are: HealeyCG,BoothKS, Enns JT. 1996. High-speed visual estimation using preattentive processing; and Callaghan TC. 1984. Dimensional interaction of hue and brightness in preattentive field segregation. Percept.
2.1.2 Attention medicated testing methods
2.1.2.1 Direct Observation: numerical estimation, speed, error rates
- simplest ways to test the utility ofa graph is to verify that information can be accurately read from it
- the participants are provided with a chart and asked to estimate some quantity or answer a predefined question using the information provided in the chart
- It is important to ensure that the specific charts and questions used are aligned; studies are commonly critiqued on the basis that the charts or the questions were not appropriate for the task
- Not always perceptual ranking. The point vs line example in figure 8 shows there is some cases where perceptual tasks do not perfectly differentiate? Or is it that the PDF you highlight is of importance.
- open-ended estimation tasks elicit certain well-known biases such as the tendency to round to multiples of 5 or 10
- To approach situations with more complicated graphics, or charts that are known to induce perceptual biases in the participants, consider using eye tracking to measure attention and motivation, or the use of verbal descriptions to assess more complicated graphs (since they facilitate specific parts of the perceptual process)
2.1.2.2 Psychophysics and signal detection theory
- Psychophysical experimental design is focused on whether an effect is detectable and whether the magnitude of the effect can be accurately estimated
- Common methods, such as the method of constant stimuli and the method of adjustment, involve repeatedly presenting a participant with charts and asking them to evaluate the chart on the basis of a particular question of interest
- In the method of adjustment, this is done with the control of the participant, who adjusts the stimuli interactively until the effect is just barely noticeable; in the method of constant stimuli, the effect size changes randomly from trial to trial to reduce continuity effects.
- Psychophysics methods also seem to be relatively common in studies of map perception, particularly when the goal is to estimate the amount of exaggeration or other corrective distortion necessary for realistic perception of the map (for
2.1.2.3 Thinking aloud
- ask participants to talk through their thoughts as they read and use a graph in a realistic setting
- allows experimenters to examine the use ofcom- plex graphics in the wild, or at least in situations that are less artificial than the paradigms allowed by numerical estimation and psychophysics methods
- allow researchers to attempt to measure insight, and reasoning in complex situations such as experimental design, decision-making, or the process of weather fore- casting
- While the data that result from the think-aloud protocol are typically more qualitative and less quantitative than results produced using other methods, they provide significant additional insight into the underlying cognitive processes affecting visualization, which cannot be obtained through other means
2.1.2.4 Eye tracking
- eye tracking facilitates insight into the process of visual attention, providing data on the approximate spatial location of visual focus
- Eye tracking allows researchers to determine that viewers spend relatively little time examining the axes in scatter plots, but significant amounts of time examining the axes in parallel coordinates plots, suggesting that the process of reading these two chart types is fundamentally different
- Can also be used to identify features that provide useful information during the graph reading process for several different types of charts
2.1.2.5 Combination Experiemnts
2.2 Implicit Graphical Tests Using Visual Inference
- In an implicit graphical test, the user must identify both the purpose and function of the plot and use that information to evaluate the plot as shown
- Explicit tests are typically conducted on plots that have been created to showcase specific structure in the data in order to present results; in contrast, implicit tests are designed to inform exploratory data analysis and the iterative model diagnostic process. 
- e.g. lineup plot
- The advantage to implicit testing is that lineups do not require a specification of a feature of interest in the testing framework
- Much ofthe historical research of comparing different types of charts has been criticized because the specific question phrasing does not provide readily generalizable results; the lineup protocol removes this obstacle by charging the user with the task of identifying the most different looking plot and thereby selecting the feature with the visually most salient difference compared with the other plots. 
- The real power ofthe lineup protocol is that when combined with the grammar of graphics, we can hold the underlying data and summary statistics constant, isolating the effect ofdifferent plot types, coordinate transformations, and aesthetic mappings on our ability to detect effects in the data.
- It can be difficult to specify the null data-generating model in a way that adequately mimics the data plots, which suggests that visually, we are able to identify many more features than those typically tested using standard quantitative hypothesis tests. This implicit testing of many different hypotheses does make null distribution specification challenging, but it also highlights the power ofvisual cognition to detect subtle differences in data.

3 Current Best Graphical Practice
- no best plot can lead to a rather fragmented approach when describing best practice, and so in order to avoid this, we examine graphical practice using the principle of “first, do no harm” from the Hippocratic oath.
3.1 Cognitive Principles
- Take advantage of human visual system and allows us to make charts that require less cognitive effort to read
3.1.1 Proximity
- objects or shapes that are close to one another appear to form groups. For plot design, proximity is used to place items to compare close together, and less important comparisons further apart.
3.1.2 Similarity
- The gestalt principle of similarity suggests that we group things that have similar appearances and exclude objects with different appearances. 
. Vanderplas & Hofmann (2017) showed that the addition of color and shape to a scatter plot increases the likelihood that individuals will perceive clustered groups of points.
- the coloring of bars allows us to easily see that the sim- ilarly colored rectangles represent the same group of people, even though the bars are separated by facets and other groups.
3.1.3 Common Region
- The gestalt principle of common region suggests that elements contained within a common region belong together.
- confidence bands and bounding ellipses also activate this gestalt principle by grouping points within the boundaries together (Vanderplas & Hofmann 2017), highlighting the presence of outliers that do not belong to the main group.
3.1.4 Common Fate
- The gestalt principle of common fate describes the tendency to group objects that are moving together in the same direction and at the same speed together
- Common fate is certainly active in animated plots that use fading or transitions over time, but even in static plots, continuity can be activated when multiple time series plots are shown together (common fate creates bias in identifying correlation)
- Strong negative association is not easily detected from overlaid line plots, but it is easily seen in a scatter plot (makes me think of that HOPS plot experiment)
3.1.5 Working Memory
- Typically, working memory is limited to approximately seven (plus or minus two) items, or chunks. In practice, this means that categorical scales with more than seven categories decrease readability, increase comprehension time, and require signifi- cant attentional resources, because it is not possible to hold the legend mapping in working memory.
3.1.6 Change Blindness
- change blindness, affects both static and interactive plots. In static plots, it can be difficult to compare between dif- ferent small multiples or facets because the contents of the plots are not reliably represented in workingmemorywhen switching attention between them. In animated plots, it is important to use transition effects to connect successive frames of the animation: This reduces change blindness and also activates the gestalt principle of common fate, allowing us to quickly identify groups of objects that are transitioning in the same direction
3.1.7 Ease of Comparison
- Much of the psychophysics research on statistical charts exam- ines the accuracy of comparisons and quantitative evaluations made during the process of under- standing a plot.
-This ranking ofcognitive tasks provides some consistent guidance for chart design: If the same data can be represented in a way that allows the user to make a comparison more accurately (based on the hierarchy), then that design is preferable.
- While the hierarchy of graphical comparisons provides some guidance, there are other design choices that can be informed by experimental research in a less systematic way.
3.2 Chart Design
- The use of redundant aesthetics that activate the same gestalt principles (such as color and shape in a scatter plot, which both activate similarity) results in higher identification ofcorresponding data features. 
- In addition, dual encoding increases the accessibility ofa chart to individuals who have impaired color vision or perceptual processing (e.g., dyslexia, dysgraphia).
3.2.1 Colour
- Cognitive load can also be reduced by selecting colors with cultural associations that match the data display, such as the use of blue for men and red (or pink) for women, or the use ofblue for cold temperatures and red/orange for warm temperatures.
- It is also important to consider the human perceptual system, which does not perceive hues uniformly: We can distinguish more shades of green than any other hue, and fewer shades of yellow, so green univariate color schemes will provide finer discriminability than other colors be- cause the human perceptual system evolved to work in the natural world, where shades of green are plentiful.
- the distance between points in a given color space may not be the same as the distance between points in perceptual space
- Color-blindness is common enough that it is reasonable to expect that any given chart used in a presentation or publication will be read by someone with a color perception deficiency. The use of dual encoding allows color-blind individuals to more readily read graphics that utilize color, and as hue and lightness can be varied separately, it is possible to use dual encoding without adding another aesthetic.

4. Open Questions and Future Research
- Another open question is the acceptance of results—much historical research on best practices exists, but how much of it is being put into practice? One

## @Tierney2023

## @Pu2020

## @Leland2005

# General thoughts/questions
- Integrable vs separable
  - kinsley did not mean it in the way that the integrable seperable paper means it. They basically meant colour on the same axis
  - should read [this paper](https://dl.acm.org/doi/fullHtml/10.1145/3290605.3300899#:~:text=Two%20visual%20channels%20are%20considered,from%20the%20other%20%5B48%5D.) and others along the same lines.
- grammar of graphics describes what your visualisation *is* given the data you have. The reason the grammar of graphics is not equipt to handle uncertainty is the same reason the tidyverse is not equipt to handle it. It is generated and carries through between these independent steps.
  - unclear how uncertainty from the source would make it in.
  - Grammar of graphics describes what *is*. Uncertainty is what *might be*.
  - Every uncertainty visualisation is entirely definable within the grammar of graphics. The *difficulty* with implementing uncertainty does not come from a failing within the grammar of graphics but from the absent definition of uncertainty. Just like normal statistics, visual statistics defined by the grammar of graphics *must* be well defined, if they are not, they cannot be implemented in the grammar. 
- Does the flip (hypothesis test vs uncertainty visualisation) have mathematical implications in how the plots should be assessed? This implies a visible signal in an uncertainty plot should indicate some divergence from the null.
- This increased sensisitivity could be the result of another aspect of uncertainty that is ignored. Statstical tests are typically built upon a series of assumptions, and it is difficult to identify *which* assumption failure caused the data to fail the statistical tests. Lineup plots allow us to see *how* our data is different to the assumed distribution, and better understand *why* our data may have failled a test ^[Did Patricks paper compare statistical tests to lineup protocols in the event an earlier assumption of the tests, such as, independent observation? I think he did check some. I wonder if you can quantify the information difference you get from a rejected statistical test vs a rejected line plot]. 
- If information that can be used to calculate a statistic, but not the statistic itself, is shown, people will need to use a heuristic to extract that information which will decrease the precision with which that information is communicated. 
- put uncertainty in the context of other meta-data of variables
	
# Removed Sections
@Amar2005 suggested a taxonomy for information visualisation based on the types of tasks we use visualisations for and suggest 10 "analytical primitives" that we can then map to visualisations, which could be a good starting point. 

The beauty of visualisation is that it *tells* us what we should be concerned about in our analysis. This aspect disappears the second we ask a direct question about our visualisation. 

@Griethe2006 commented that "if visualization is used as a means to explore a data volume or to communicate its contents the uncertainty has to be included".

After all, if we consider a choropleth map, which ignores uncertainty entirely to be at one end of the spectrum (it is a map that only cares about showing signals reguardless of whether or not they are valid) then a completely colourless map is at the other end of the spectrum (it is a map that only cares about suppressing signals regardless of whether or not they are valid). 

@Kinkeldey2014 categorised uncertainty according to five criteria depicted in fig-tax which considers if a visualisation is implicit or explicit, intrinsic or extrinsic, visually integrable or seperable, coincident or adjacent, and static or dynamic. A similar version of this taxonomy was presented by @uncertchap2022 who commented that visualisation can be organised into two categories, "graphical annotations of distributional properties" and "visual encodings of uncertainty" which seems to functionally align with the intrinsic/extrinsic distinction by @Kinkeldey2014. @Griethe2006 organised uncertainty visualisations into two cases (1) a hypothesis test was preformed to confirm the validity of the visualisation and (2) the visualisation has uncertainty depicted. @Potter2012 organised several existing uncertainty visualisations into groups based on the dimensionality of the data (1D, 2D, 3D, and No Dimension) and the dimensionality of the uncertainty (Scalar, Vector, Tensor).  However, because the term "PDF", function that is used to describe a random variable, is used to describe both the data and the uncertainty for all dimensions. @Grewal2021 created a taxonomy that mapped uncertainty visualisations to some point in a 2D space defined by the "domain expertise" and "continuum of discreteness" (that scaled from "point estimate" to "continuous distribution"). 

Visualisation authors are almost unanimous in commenting that the "information" in two plots must be the same in order for the visual techniques to be compared [@Cleveland1984; @Kinkeldey2014]. @Kinkeldey2014 adopts an existing definition also that suggests two graphics are informationally equivalent if all the information in one plot is inferable from the other and vice-versa, but adds that two plots are computationally equivalent if that information can be extracted from both plots with similar easy and speed. 

 @Hofman2020 commented that "theoretically" the sampling distribution of the mean and the prediction interval of a new observation are equal "so long as one knows the sample size", but does not seem to provide participants with that sample size, or recognise the assumptions and background knowledge that would be required to compare the two.

If two graphics are visually different but identical in the information they contain, they must differ in how that information is depicted. 
Once we have a mathematical expression of uncertainty, the visualisation of that uncertainty is theoretically identical to the visualisation process of any other variable. For simple tasks such as value extraction, there is a hierarchy to perceptual tasks where extracting visual information in some forms is easier than others. The hierarchy was originally established 40 years ago by @Cleveland1984, below is an updated version summarised by @Vanderplas2020:  
  
1) Position along a common scale. 
2) Position along a non-aligned scale. 
3) Length, direction, angle, slope
4) Area
5) Volume, density, curvature
6) Shading, colour saturation, colour hue
7) Discriminable shape
8) Indiscriminable shape

This hierarchy is a good general rule, however it can change from person to person [@Davis2022] Additionally, there are other graphical rules to consider such as gestalt principles, broader methods of extraction, and attention principles [@Vanderplas2020]. These established visualisation concepts allow us to anticipate the ease with which certain pieces of information will be extracted from a plot. We can use these concepts to understand the computational complexity of a graphic. A bizarre feature of the uncertainty visualisation literature is that it does not work to build upon these existing principles or identify the ways in which uncertainty visualisations may diverge from these rules. These building block concepts of visualisation are seldom mentioned. 

It is difficult to find examples of uncertainty visualisation experiments where the plots do contain the same information, however when they do, the results align with existing information visualisation research. Technically, a PDF and a mean with confidence intervals both have enough information to extract the mean of the distribution, however they both have a very different computational cost. To extract the mean using a PDF, a participant would need to identify the point along the x axis that splits the area under the curve in half. If a participant is provided with a mean with a confidence interval, extracting the average is a simple task of reading the position on an aligned scale. 

Similar results to this occur over and over again in the uncertainty visualisation literature.   (*Cite Gap 15: Examples of replicated perceptual task experiments*). 

These results show that uncertainty is not technically different to any other variable. When trying to anticipate the results of these studies, we can use the same principles of information equivalence and difficulty of relevant mental tasks to understand which plots will outperform others. This does not mean that visualising uncertainty as a signal is incorrect or bad, it just means that uncertainty as a signal is no different from any other information visualisation.

- decision making experiments are usually displaying the wrong signals

The exploration step of an analysis, which includes descriptive statistics, exploratory data visualisation, and unsupervised machine learning techniques, is performed without a prior hypothesis, however misunderstandings of this fact appears frequently in the literature.

@wu2023rational recognised and tried to eliminate information asymmetry by looking at a visualisation and identifying what information can be extracted and then using a "rational agent benchmark" to determine how a rational person would use that information for a decision making task. While this solution is interesting, it may have different results depending on who is applying it.

For example, @Hofman2020 asked participants to judge the effectiveness of a particular treatment. One group was shown the prediction interval around the mean, which indicates the uncertainty associated with treatment *for a particular person* and another group was shown the the sampling interval which depicted the uncertainty associated with the *average effectiveness of the treatment*.  The contribution of this experiment hinges on the belief that sharing a common descriptive statistic (in this case, the mean) is grounds to compare two unrelated inferential statistics, a belief which is clearly false when viewing uncertainty through the lens of inference. 

@Boukhelifa2012 tried to quantify the strength of the intuitive connection between a line attribute called "sketchiness" and uncertainty. Participants were shown the six scenarios depicted in @fig-sketchy and asked to interpret what they believed the squiggly line indicates. The authors were aware that uncertainty seemed to have some "task" dependence but interpreted this as *context* dependence rather than dependence on a particular inferential statistic. Therefore, "sketchiness" was added to aspects of the visualization that had no obvious inferential statistic associated with it, so participants simply ignored the "sketchiness" or assumed it represented something else, such as an alternative option. 

![The graphics displayed by @Boukhelifa2012 to identify if there is an intuitive connection between sketchiness and uncertainty. These graphic were made without concern as to what (i.e. which statistic) in the image is supposed to be uncertain. This leads to the images being difficult to connect to uncertainty even if we assume tha that is what it is supposed to represent. For example, (S5) implies that participants should have read the plot to mean that the creators of a rail network map would be "uncertain" about the existence of a train line, something that is defies common logic.](sketchiness.png){#fig-sketchy width=50%}

@Padilla2021 found that high uncertainty in the model estimates and low forecaster confidence  both caused participants to have decreased confidence in their results and suggested modelers express both if they are relevant.

@Kale2019 discusses the importance of communicating decisions made in the data analysis pipeline and being aware of the alternatives.

@Tierney2023 generates a visualisation for missing values, however because it fits nicely into the grammar of graphics framework, it is inherrently a descriptive statistic.

Signal can be considered the information that *promotes* a message we are trying to infer and while noise is the information that *suppresses* that message, if that message is invalid.

This is why quantifiable uncertainties, such as confidence intervals, dominate the literature while visualisations for more complicated sources of uncertainty, such as the effects of assumptions, imputed missing variables, and model choices, remain elusive.

@Munzner2009 created a nested model for visualisation that highlighted how the first mistake that can be made in a visualisation is in the problem characterisation, and failing to do it well can cause downstream effects and damage the effectiveness of a visualisation.

The absence of an encompassing definition of uncertainty is mentioned by every uncertainty visualisation directly [@Spiegelhalter2017; @Griethe2006] or indirectly by describing a myriad of ways it can be considered in the literature [@Kinkeldey2014; @Hullman2016], although it is never commented on as a source of the noise in the field.

In order to visualise uncertainty, it needs to be quantifiable [@Griethe2006; Leland2005], in order to quantify uncertainty, it needs to be mathematically defined. The uncertainty visualisation literature is completely awash with papers that define uncertainty using a vague encompassing phrase, (e.g. @utypo defined it as any deviation from complete determinism) but go on to quantify uncertainty as a PDF, error, or some other easily quantified mathematical object when it comes time to do the visualisation. This definition swap happens so subtly that it seems to go unnoticed by authors

Utilizing bias and variance to define uncertainty is well-established, but it is absent concerns about imputed data, assumptions, sampling methods, and even the human element of analyst decisions


# Great Examples 
Despite the common problems detailed in the previous sections, there is some interesting work in the uncertainty visualisation space. This can come in the form of an uncertainty visualisation that attempts to visualise a typically ignored aspect of uncertainty, or an experiment that avoids the pitfalls detailed in the previous sections.

The literature focus on "quantifiable" uncertainty leaves the variance that occurs at early stages in our analysis that is difficult to quantify ignored and forgotten. Some authors have chosen to focus on these unquantifiable cases and look at expressing more complicated cases of uncertainty.  @Tierney2023 builds upon the tidy data principles to allow users to handle missing values. This includes data plots with a missing value "shadow" that allows visualisation authors to identify if the variables used in a plot have any structure in their missing values, which would contribute to uncertainty. Another example of uncertainty that is often ignored is the uncertainty resulting from human choices. Climate scenario uncertainty, shown in @fig-climatescenario, attempts to display the range of climate change outcomes that can result from a range of best and worse case human choices. 

![Notice: I will make my own Eversion of a climate scenario plot, this one is just here from a random site as a place holder. A climate scenario plot shows the forecasts of different emission scenarios depending on human choices. The model visualisation wants to show the impact of human choice on the visualisation, rather than assume it away, so five different cases are shown in the model. Each of the case studies also depicts the statistical uncertainty of the model using a confidence interval. ](climatescenario.png){#fig-climatescenario}

Another visualisation method that has a lot of potential is visualisation of samples. Visualisations that opt to express a signal as a sample rather than an estimate have the potential to suppress signal since it is not explicitly visualised, however this has yet to be shown in evaluation experiments. This is not to say that visualisations of mass would not be able to perform signal suppression, but a sample can easily be expressed using aesthetics such as colour on a map and mass visualisation often struggling with issues such as over or under smoothing. These sampling methods can show more of the messiness of the data that sits behind a model. This may not have a detrimental effect on the viewers ability to extract global statistics, as it seems they can be extracted from a visualisation of a sample with ease [Franconeri2021]. Sample visualisations have been used in maps with the pixel map, shown in @fig-samples [Lucchesi2017], but is more commonly used in animation with the HOPs plots [@Hullman2015] or similar concepts [@Blenkinsop2000]. This method has also been adopted by visualisation authors outside of academia as can be seen in the  and the New York Times class mobility figure, shown in @fig-samples, an animated version of which can be found [here](https://www.nytimes.com/interactive/2018/03/19/upshot/race-class-white-and-black-men.html). 

::: {#fig-samples layout-ncol=2}

![Pixel map](pixelmap.png){#fig-pixelmap}

![Outcome plot](classmobility.png){#fig-nyt}

Notice: Will replace both plots with my own r visualisations. The pixel map depicts a map of the different poverty rates in local government areas in the US state of Missouri. The NYT visualisation shows an screencap of an animation that depicts the economic status of white and black boys who were raised in wealthy families. By displaying a psudo-sample rather than an estimate and a variance, the visualisation does some signal suppression as the true poverty rate is masked and can only be extracted by taking the global statistic of a sample. You can also depict the signal using explicit values, as the the NYT visualisation did, but it is important that the main visual features depict the uncertainty in the estimate.
:::

A final method to perform signal suppression is simply to visualise the data if it is available and relevant to the uncertainty distribution. An example of this is shown in @fig-census for racial distributions spatially in America, an interactive version of the plot can be found [here](https://edition.cnn.com/interactive/2021/us/census-race-ethnicity-map/). This map shows the typical causes of uncertainty in a spatial model, (e.g. regions where data is sparse, ethnically diverse areas, uneven distribution of points within boundaries, etc) but it avoids the need to create a visualisation with a specific signal in mind. This is the technique typically employed by exploratory data analysis, which means it's lack of a specified signal means there is both *no* uncertainty (since we are technically not performing inference) but in the event we *do* implicitly perform inference, there is some hedging. In this sense, the best uncertainty visualisation you can get without specifying a signal you want to convey is visualising the data itself. The census dot map's addition of interactivity also allows users to zoom in and see the details that caused "uncertainty" in the form of inconsistent colours at lower resolutions when they were zoomed out. 

![Census dot map](censusdotmap.png){#fig-census}

# Statistic notes
X
E[X] 
n 
Var(X) 
Max(Var(X))
Min(Var(X))


# Experiment notes

@Sanyal2009 colored glyphs outperformed error bars in a search task study, which boils down to asking users to find the most extreme color, largest size, or longest length. They found people were better at finding the value with the most extreme color than the longest length.

@Kale2018 found users were better at identifying if there was a general trend in a series of line plots when shown hops rather than error bars or ensembles.


“growth” or “no growth” judgement about one bar chart and provided + a rating of their confidence on a scale of 50

How likely is the red outcome?

How likely is the outcome where can- didate A gets 55% of the vote?

How likely is candi- date B to win the election?

# Vis inference stuff
objects or shapes that are close to one another appear to form groups.
  proximity is used to place items to compare close together, and less important comparisons further apart

we group things that have similar appearances and exclude objects with different appearances.

elements con- tained within a common region belong together
  confidence bands and bounding ellipses also activate this gestalt principle by grouping points within the boundaries together

tendency to group objects that are moving together in the same direction and at the same speed together

working memory is limited to approximately seven (plus or minus two) items, or chunks.

If comparisons and quantitative evaluations made during the process of under- standing a plot. This research can be distilled into a hierarchy of comparisons 
1. Position (common scale) 
2. Position (nonaligned scale)
3. Length, direction, angle, slope 
4. Area 
5. Volume, density, curvature 
6. Shading, color saturation, color hue 
7. Discriminable shape 
8. Indiscriminable shape
This ranking ofcognitive tasks provides some consistent guidance for chart design: If the same
data can be represented in a way that allows the user to make a comparison more accurately (based on the hierarchy), then that design is preferable.

# Data description
Consider our Iowa temperature map, let $X$ be the 12pm temperature taken within each county on for each day in May from 2000 to 2024. Unfortunately, we are given a data set that does not include $X$, instead we have the average of $X$, $\hat{X_k}$ and the variance of $X$, $\hat{VarX}$ for each county, $\forall k=1,...,n$ where $n$ is the total number of counties. Theoretically, the estimate and its variance should provide enough information for us to 

# EDA
Regardless as to whether the map colors are implicitly or explicitly indistinguishable, the point at which the colors blend together can only be validated by a single hypothesis. Therefore, using the methods discussed thus far, it would be impossible to design signal-suppression for EDA. If we are designing a graphic for communication then we can suppress the signal we were seeking to communicate, but this would rely on the distinction between EDA and communication to be somewhat clear cut. Visualization authors may design a graphic with a particular purpose, but what the visualization *is actually* used for is determined by the viewer and some authors have discussed the validity of there being any distinction between visualisation motivation's at all [@Hullman2021].

# Bivariate map saturation mapping
Since changes in the color value and hue can also cause the colors to blend together, the mapping of uncertainty to saturation would also affect the signal-suppression in these maps. To correctly map uncertainty, we would need to be able to answer how many units of saturation a 1 unit change in variance worth? There are no existing guidelines, but similar to the suppression method, the choice in colour mapping would depend on the signals we hope to suppress.

# Variance is not uncertainty
Some authors simply confuse variance for uncertainty. For example @Potter2010 aimed to create a summary plot that "concisely presented data with uncertainty information" to create an exploratory visualisation tool that visualised uncertainty. 

This approach to uncertainty is embedded in the idea that descriptive statistics do not have uncertainty, which some readers may disagree with. Specifically, because it means uncertainty is *not* a latent attribute of data, but rather an attribute of a specific hypothesis or estimate. Deniers of this fact follow a consistent logical path and it is easy to identify the common mistake. We know that variance and probability are typically considered types of "uncertainty" *and* descriptive statistics can have variance and probabilities *therefore* descriptive statistics must have uncertainty. The flaw in this logic comes from the first step, assuming that the tools with which we measure uncertainty *are* uncertainty in of themselves. This confusion is common and there are many papers that spend a great deal of time clarifying the difference. @Begg2014 highlight that uncertainty is related to not knowing a specific value, while variability refers to the range of values a quantity can take at different locations, times or instances. @Spiegelhalter2017 made sure to comment on the difference between precise random events (such as the probability associated with a coin flip), and uncertainty (such as the estimated probability associated with a coin that might be biased). The variance of a sample variance can be calculated and know, therefore it is not uncertain but rather it a precise description of dispersion. If we were to discuss drawing a new observation, or estimating the true mean of a population *then* the variance would become relevant in our discussions of uncertainty. 

# Removed comments on experiments
The final method used by authors is to just explicitly ask about uncertainty and signal information separately. @Sanyal2009 mapped uncertainty to dots and signal to a 3D surface and asked participants to identify areas of high and low signal and high and low uncertainty. Participants were not asked to combine that information in any way, and the signal and the noise were treated as separate variables. @Correll2014 asked participants to separately extract the mean and variance from four uncertainty visualisations. These methods explicitly view the uncertainty and signal as two separate variables that should be extracted from a plot, and not two variables that should be interpreted together. Even viewing these questions as a routine check to make sure the signal information isn't impacted by the uncertainty is counter intuitive, because the whole point *of* the uncertainty is to impact the signal information.

# Alternative purpose section
This purpose or task dependency appears repeatedly in the literature, although its connection to inference is seldom mentioned. Multiple authors have commented on the need to consider quantifying and expressing uncertainty at every stage of a project as the "goal" shapes every step of the analysis [@Kinkeldey2014; @Hullman2016; @Refsgaard2007]. This is because "signal" or "noise" are not inherent qualities of our data, they are simply the terms we give to the information we wants to keep, and the information we want to throw away. Whether or not information is signal or noise depends entirely on the questions we are are seeking to answer [@Meng2014]. This distinction between noise and signal occurs as early as data collection, where the entire process of deciding how to boil real world entities down into data depends on the relationships we seek to identify [@Otsuka2023]. This is true for our entire analysis, right up to deciding how to display information, as the way we display information should be based on that the user is supposed to do with it [@Fischhoff2014]. Even the process we use to combine uncertainty from different stages of an analysis depends on why the uncertainty matters [@Wallsten1997]. It is clear that as we move through the stages of our analysis noise is the status given to the information we throw away that inhibits our ability to identify signal. We want to reintroduce some noise back into our visualisation such that we can perform signal-suppression.

# Example for perceptual task problem
Consider the typical uncertainty visualisations shown in @fig-mean, which includes a density plot, a sample, and an mean with confidence intervals. These graphics each show the distribution of the standard normal random variable X using a density plot, a sample, and a mean with confidence bars. Each of these technically show a piece of information with zero uncertainty. The density plot provides a deterministic interpretation of the PDF, the sample shows exactly the values we observed, and the mean with error bars shows three explicitly calculated statistics ($X_{0.05}, \bar{x}, X_{0.95}$). If we were to use these visualisations to extract the mean, the role of signal in the chart shifts, and instead of directly reading the values, we need to do mental calculations. The density plot requires us to find the midpoint along the x axis that could spit the area under the density curve perfect in half. The sample requires us to find the point that minimizes the sum of squares errors, i.e. the total lengths from each point to the mean. The mean with error bars allows us to simply read the mean off the graphic using a basic perceptual task. If we were to use the visualisations to extract the variance instead, we could describe a similar process. 

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-mean
#| fig-cap: "This visualisation "
#| fig-subcap: 
#|   - "Probability density function"
#|   - "Sample"
#|   - "Mean with error bars"
#| layout-ncol: 3
#| layout-valign: "bottom"
set.seed(1)
xdata <- tibble(x=rnorm(100))

# Density function
x1 <- xdata |>
  ggplot(aes(x=x)) +
  geom_density(colour="#1e81b0", fill="#abdbe3", alpha=0.5) + 
  scale_x_continuous(name = "x") +
  scale_y_continuous(name = "P(X)") +
  theme_minimal() + 
  theme(aspect.ratio=1)


# Sample

# Mean with error bars
x3 <- xdata %>%
  summarise(avg = mean(x),
          conf_95a = quantile(x, probs=c(0.025)),
          conf_95b = quantile(x, probs=c(0.975))) %>%
  ggplot(aes(y="NA")) +
  geom_errorbar(aes(xmin = conf_95a, xmax = conf_95b), width = 0.1, colour="#abdbe3") +
  geom_point(aes(x=avg), colour="#1e81b0") +
  scale_x_continuous(name = "X",
                     breaks = seq(-2,2, 0.5)) +
  theme_minimal() +
  theme(axis.line.y=element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        aspect.ratio=1)

```

# GT table code
```{r}
#| echo: false
#| message: false
#| warning: false
# Make nice table
example_table <- my_map_data |>
  mutate(variance = sqrt(variance)) |>
  select(c(count_id, county_name, temp, variance_class, variance)) |>
  as_tibble()|>
  pivot_wider(id_cols=c(count_id, county_name, temp,), 
              names_from = variance_class, 
              values_from = variance) |>
  head()|>
  gt() |>
   tab_header(
    title = "Average Daily High Temperatures of Iowa Counties",
    subtitle = "Psudo-data with high standard error and low standard error cases") |>
  tab_spanner(
    label = "Standard Error",
    columns = c(highvar, lowvar)
  ) |>
  cols_label( 
    count_id = "ID", 
    county_name  = "County",
    temp = "Temperature (°C)",
    highvar = "High",
    lowvar = "Low"
    ) |>
  fmt_number(columns = c(temp, highvar, lowvar)) |>
  data_color(
    columns = temp,
    method = "numeric",
    palette = "Oranges",
    domain = c(21, 29)
  ) |>
  data_color(
    columns = c(highvar, lowvar),
    method = "numeric",
    palette = "Greens",
    domain = c(0, 3)
  ) |>
  as_latex()
```

# Alternative maps

```{r}
#| echo: false
#| message: false
#| warning: false
#| eval: false
#| label: fig-bivariate2
#| layout-ncol: 3
#| layout-valign: "bottom"

#fig-cap: "A bivariate map that depict the counties of Iowa where each county is coloured acording to it's average daily temperature and the variance in temperature. This map is a variation on the previous bivariate map where instead of variance being mapped to colour saturation, it is mapped to colour value. Even though colour value is intergrable with colour hue, and colour value is naturally accociated with uncertainty, the spatial trend is still visible in the map."

# Bivariate Map
# Make bivariate palette
# Function to devalue by a certain amount
colsupress2 <- function(basecols, hue=1, sat=1, val=1) {
    X <- diag(c(hue, sat, val)) %*% rgb2hsv(col2rgb(basecols))
    hsv(pmin(X[1,], 1), pmin(X[2,], 1), pmin(X[3,], 1))
}

# recurvisely decrease value
v_val = 1.1
bivariatepal2 <- c(basecols,
                   colsupress2(basecols, val=v_val), 
                   colsupress2(colsupress2(basecols, val=v_val), val=v_val),
                   colsupress2(colsupress2(colsupress2(basecols, val=v_val), val=v_val), val=v_val)
                   )

# establish levels of palette
names(bivariatepal2) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# Bivariate maps 2
p2a1 <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = bivariatepal2) +
  theme_void() + 
  theme(legend.position = "none")
  
p2b2 <- p2a1 %+% filter(my_map_data, variance_class=="highvar")


p2a1
p2b2
#show_pal2(colours = bivariatepal2, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)

```

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-transparency
#| fig-cap: "A bivariate map that depict the counties of Iowa where each county is coloured acording to it's average daily temperature and variance. This map is a variation on the previous bivariate map where instead of variance being mapped to colour saturation, it is mapped to transparency."
#| fig-subcap: 
#|   - "Low Variance Data"
#|   - "High Variance Data"
#|   - "Transparency Palette"
#| layout-ncol: 3
#| layout-valign: "bottom"

# ALTERNATIVE TRANSPARENCY INSTEAD OF COLOUR VALUE
# get colour values from transparency transformation
transpal <- lapply(basecols, scales::alpha, alpha=c(0.95, 0.65, 0.35, 0.05))

transpal <- as.vector(matrix(unlist(transpal), ncol = 4, byrow = TRUE))

names(transpal) <- paste(rep(1:8, 4), "-" , rep(1:4, each=8), sep="")

# Transparency maps
pta <- my_map_data |>
  filter(variance_class=="lowvar") %>%
  ggplot() +
  geom_sf(aes(fill = biclass, geometry = geometry), colour=NA) + 
  scale_fill_manual(values = transpal) +
  theme_void() + 
  theme(legend.position = "none")
  
ptb <- pta %+% filter(my_map_data, variance_class=="highvar")

pta
ptb
show_pal2(colours = transpal, ncol=8, borders=NA, myxlab = "Temperature", myylab = "Variance", breaks = 21:29, breaks2 = 0:4)

```

