---
title: "Writing Trash Bin"
author: "Harriet Mason"
format: html
---
# Why this exists
I have an information hoarding problem, this helps me throw out sections of the literature review that are not necessarily relevant without actually throwing it out. Knowing that I can come back to this at any point and pull information back, makes me more comfortable with removing it from the report.

# Moved over notes
- mentions uncertainty can be variance, precision, accuracy, reliability, or related concepts @Hullman2016
-  @Spiegelhalter2017 defines risk as risk known probabilities and uncertainty as estimated probabilities, therefore uncertainty is what we have when you cannot accurately quantify risk.

# Main section outtakes
Failling to understand this key aspect of visualsiation leads to graphics that *contain* the correct amount of information, but that volume of information cannot be readily accessed by the viewer.

## Taxonomies
Some taxonomies contain overlapping terminology but also have alternative definitions and categories. For example *___* organises uncertainty into the two categories, Aleatroy (inherrent randomness) and epistemic (lack of knowledge), but *___* organises uncertainty into the three categories aleatory (inherrent randomness in a forecast) vs epistemic (uncertainty in structure and parameters of statistical models) vs ontological (uncertainty about the entire modelling process as a description of reality). 

To avoid alienating lay people with confusing complexity, many of these taxonomies are overly simplistic. Most taxonomies seek to organise uncertainty based off where the uncertainty is coming from. For example, @Spiegelhalter2017 organises uncertainty into three categories: 
  - Aleatory uncertainty: inevitable unpredictability due to unforeseeable factors 
  - Epistemic uncertainty: uncertainty about the structure and parameters of statistical models
  - Ontological uncertainty: uncertainty about the entire modelling process as a description of reality.
These three categories can be considered to be uncertainty when deciding which model we should use (ontological), uncertainty surrounding parameter estimates (epistemic), and uncertainty surrounding a forecast (aleatory), however it is not hard to find another, very different, example of a taxonomy that organises uncertainty based on what is seen to be causing it. @Gustafson2019 organises uncertainty into the four groups:
  - Deficient: Lack of knowledge
  – Technical: modelling approximations and measurement error
  – Scientific Unknown unknowns
  – Consensus: Disagreement among parties
However the additional comments about the inherent variability of aleatory uncertainty results in a bit of confusion as to if our cateogirisation should change if we decide to treat the parameters of a statistical model to be inherently random (as is done in Bayesian statistics) or if we beleive a forecast variability to be caused by a lack of knowledge.

He kept the taxonomy to three levels to maintain simplicity, however this organisation methods conflates amount of uncertainty present in an estimate with what is causing that uncertainty, and in doing so makes it hard for us to use this definition to better understand uncertainty within our own analysis.
# context section outtakes
Given that uncertainty estimation and communication seem to be largely task depenent, what questions should we be testing? This leads to several unanswered questions. Is our goal in uncertainty communication to acknowledge and convey the inherrent randomness that exists in all statistical calculations and models? Is it to supress signal that could be the result of random variance? Is it to find the best way to convey the mass function of a random variable? A survey of the literature will leave the answers to these questions unclear, however this inherrent issue has not gone unnoticed.
This conceptual way of thinking about uncertainty has been rattling around in the different areas of data science for a while.
For example, when we record coin flips we typically want to model the behavior with a binomial distribution, so we ignore the outcome of a coin landing on its side despite the fact it is a real world possibility.
@Wallsten1997 argue that the best method for evaluating or combining subjective probabilities depends on the uncertainty the decision maker wants to represent and why it matters.  @Munzner2009 created a nested model for visualisation that highlighted how pitfalls in upstream levels, such as problem characterisation and data abstraction, can cause downstream effects and damage the effectivness of a visualisation. @Fischhoff2014 looks at uncertainty visualisation for decision making decides that we should have different ways of communicating uncertainty based off what the user is supposed to do with it.

# General Thoughts For Actual Review
##### General
- I might need to go back through and add in notes about general visualisation stuff and bad papers for motivations
- could also help to include a bunch of papers that talk about density visualisation without mentioning uncertainty visualisation in the "context" section. (for now have left those unsorted)

##### Introduction

##### Defining Uncertainty
  - The more taxonomies I read the more convinced I am the "defining uncertainty" is the best one. Not only is it the most complete, but it also identifies WHY some uncertainties are difficult to visualise (specifically which part of the taxonomy) and suggests HOW you can best communicate these uncertainties.
    - A lot of other papers just make a comment like "oh some of these are hard to communicate" but that paper actually goes into detail instead of hand waving the problem away.
    - It also explains why the uncertainty visualisation literature focuses on a specific subset of uncertainty cases.
  - Examples
    - Null Values
    - Variance
    - Hypothesis Testing
    - Distribution
  - Combining uncertainty is also a reccurring theme in these papers
- I think it is also important to highlight if uncertainty is meta information or if it is a variable in of itself. Meta information (such as contiunity, exhangeability, etc) are usually set to specific visual features (e.g. VSUP clearly sees uncertainty as meta information)
- A lot of work that evaluates uncertainty visualisations checks to see if the mean can be gathered from a visualisation, but if we view uncertainty as meta-information, then with a high uncertainty we wouldn't want the mean to be visible
  - Also an example of why the motivation matters for these things. If you don't know "WHY" someone is using a visualisation for uncertainty and just pick the "best" one, you are going to get a visualisation that does a poor job of its task.
  - put uncertainty in the context of other meta-data of variables

##### Visualisation as a form of Uncertainty communication
- A LOT OF the work in uncertainty communication has a sub-field that is visualisation specific
  - Basically I want a brief section highlighting the importance of visualisation and discussing it in relation to other forms of uncertainty communication.
  - especially considering some uncertainty visualisation papers discuss using just a text based method to communicate uncerainty
  
##### Uncertainty visualisation best practices
  - There is also two kinds of papers, those that compare elements of a plot (like mapping uncertainty to an aesthetic) and those that compare some expression of a distribution (like a scatter plot compared to a box plot).
  - try to map back to david speigleholder paper i.e. uncertainty means lots of different things

# Previous literature review
#####Taxonomy Benefits
- Distinctions at this taxonomy level seem to be important also untangling helps to understand it
  - @Padilla2021 found that low forecaster confidence or high model uncertainty both contribute to more conservative judgements by decision makers.
  - Failing to communicate the nature of your uncertainty can result in underestimation or overestimation of failure probabilities [@Kiureghian2009]
  - @Gustafson2019 found that the framing of our uncertainty, (i.e. if the source of uncertainty is from a lack of knowledge, approximations, unknown unknowns, or disagreement among parties) was found to not have a detrimental effect on the belief in the estimates, perceived credibility, or behavioural intentions of the decision makers.

##### The benefits of Unvertainty visualisaiton
Visualisations can provide a more complete picture of a risk than numerical summaries alone. Even something as simple as sketching a distribution before recalling statistics or making predictions can greatly increase the accuracy of those measures [@Hullman2018; @Goldstein2014]. While there is some evidence that confidence intervals provided in text form only are less likely to be misinterpreted than graphics [@Savelli2013], text is insufficient to express more complicated aspects of a distribution, such as mass. Expressing uncertainty verbally decreases the perceived reliability and trustworthiness of the source [@VanderBles2020]. Any confusion caused by expressing uncertainty as a visualisation could also be due to a lack of exposure, since @Kay2016 found people repeatedly exposed to the same uncertainty visualisations quickly get better at making judgements. Additionally, visualisation allow for interactive graphics that provide a more in depth understanding of probability [@Potter2009; @Ancker2009] and infographics that make uncertainty more accessible for people with poor numeracy skills [@Ancker2009]. 
**application in energy**
The use of uncertainty in high dimensional environments is especially important in energy data. Large models that incorporate spatial-temporal data from many sources and systems are used to predict energy uses in the short and long term. Understanding how to improve and make better decisions in these models is imperative in both the daily operation of the energy sector as well as in the transition from fossil fuels to clean energy. The energy sector needs better heuristics to make energy supply analysis less costly to conduct [@Stenclik2021], therefore it is an incredibly relevant application of uncertainty visualisation techniques. 

##### People dont visualise uncertainty but it is considered important
Despite these benefits, there is evidence that we don't visualise uncertainty as often as we should. A survey conducted by @Hullman2020a found that majority of visualisation authors agreed that expressing uncertainty is important and should be done more often than it currently is, some even agreed that failing to do so is tantamount to fraud. Despite this, only a quarter of respondents included uncertainty in 50% or more of their visualisations [@Hullman2020a]. Meaning participants were convinced that visualising uncertainty is morally important but were able to provide self sufficient reasoning that allows them to avoid doing it. Some economists suggest that visualisation authors are responding to incentives that make it tempting to avoid visualising uncertainty, even if those incentives are based more in perception than reality [@Manski2020]. The study by @Hullman2020a found that the most common reasons authors don't visualise uncertainty despite knowing it's moral importance are: not wanting to overwhelm the audience; an inability to calculate the uncertainty; a lack of access to the uncertainty information; and not wanting to make their data seem questionable [@Hullman2020a].

##### Effect of uncertainty visualisation on decision making
If decision markers are not presented with the uncertainty about an estimate the data analysts have, for all intents and purposes, made the decision for the decision maker. Upon further interviews @Hullman2020a found that authors believed uncertainty would overwhelm the audience and make their data seem questionable because decision makers are unable to understand uncertainty. This belief, while pervasive, is not true. There is some research that suggests laypeople cannot understand complicated concepts in statistical thinking (such as trick questions on hypothesis tests or the difference between Frequentist and Bayesian thinking) [@Hoekstra2014; @Bella2005] but there is a large amount of research suggesting that presenting uncertainty information improves decision making, both experimentally [@Joslyn2012; @Savelli2013; @Kay2016; @Fernandes2018] and in practice [@Al-Kassab2014]. As a matter of fact, doing what many authors currently do (providing only a deterministic outcome with no uncertainty) causes decision makers to be *less* decisive and have completely unbounded expectations on an outcome [@Savelli2013]. This reality cannot be avoided by providing secondary or non-specific information such as explaining calculations [@Joslyn2012], explaining the advantages of a recommendation [@Joslyn2012], or expressing uncertainty in vague terms [@Erev_1990; @Olson_1997], all of which are undesirable for decision makers and lead to measurably worse decisions [@Joslyn2012; @Erev_1990; @Olson_1997]. One of the most popular depictions of uncertainty for decision making is a quantile dotplot, shown in @fig-quantdot. It is important to avoid whittling down the problem **too** much. Providing a categorical decision alone is somewhat useless [@Joslyn2012], and visualising a single estimate is akin to providing a decision or expressing no uncertainty at all. 

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-quantdot
#| fig-cap: "This plot depicts an example of a a quantile dotplot that expresses the uncertainty associated with a daily maximum temperature. The probability associated with each temperature is expressed with discrete countable bins and the predicted temperature is expressed with a line. Discretised depictions such as this, make decision making in the face of uncertainty easier for the viewer."
set.seed(1)
dotplot_data <- tibble(temp = 10 + round(rnorm(40, mean=10, sd=2))) 
mid <- round(mean(dotplot_data$temp), 2)
dotplot_data %>%
  ggplot(aes(x=temp)) +
  geom_dotplot(binwidth = 0.75, fill="grey") +
  theme_classic() +
  geom_segment(x=round(mid), xend=round(mid), 
               y=-1, yend=0.7, size=2) +
  geom_label(x=round(mid), y=0.7, label=paste0(mid, "°C"),
             fill = "black", fontface = "bold",
             colour="white") +
  scale_x_continuous(breaks = seq(10, 30, 5), 
                     limits = c(10, 30)) +
  theme(axis.line.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) +
  labs(x = "Tomorrow's Daily Max Temperature")
```

##### Avoiding uncertainty creates a lack of trust
Not only does communicating uncertainty improve decisions but the mistrust created by communicating certainty in uncertain situations can be exploited. A 6-month survey of anti-mask groups on Facebook during the COVID-19 pandemic showed that the anti-maskers thought carefully about their grammar of graphics and made persuasive visualisations using the same data as pro-mask groups. They did this by exploiting information ignored by the pro-maskers [@Lee2021]. It is understood that deceptive plots can lead viewers to come to incorrect conclusions or significantly overstate effects or risks [@Pandey2015; @Padilla2022] but these incorrect takeaways cannot be mitigated with instructions in how to correctly understand the plot [@Boone2018]. This evidence indicates we are more likely than not to hurt our message when we ignore uncertainty information and trying to raise the general public's plot literacy is an insufficient strategy to curb conspiracy theories and misguided scientific communication.  In direct contrast to this, displaying numerical estimates of uncertainty information has shown to lead to greater trust in predictions [@Joslyn2012; @VanderBles2020]. While @Han2009 found people have more worry when presented with uncertainty regarding health outcomes, this worry is not a bad thing if the concern is warranted given the ambiguous situation.

##### Poeple might simply be chosing to not visualise uncertainty
The disconnect between the research in support of visualising uncertainty and the consensus against it may not be entirely driven by a lack of understanding of the literature. For example, at least one interviewee from the study by @Hullman2020a claimed that expertise implies that the signal being conveyed is significant, but also said they would omit uncertainty if it obfuscated the message they were trying to convey.  Other authors who were capable of calculating and and representing uncertainty well did not do it, and were unable to provide a self-satisfying reason why [@Hullman2020a]. These conflicting motivations are acknowledged in the paper itself where @Hullman2020a says:

> "It is worth noting that many authors seemed confident in stating rationales, as though they perceived them to be truths that do not require examples to demonstrate. It is possible that rationales for omission represent ingrained beliefs more than conclusions authors have drawn from concrete experiences attempting to convey uncertainty". 



#### Continuous vs discrete displays
There are other distributional features, such as discreteness, that are important to consider in uncertainty visualisation. There is a reasonable amount of evidence that cumulative displays or discrete displays (such as a quantile dot-plots or histograms) are the best ways to express mass for decision making and probability estimates [@Fernandes2018; @Hofmann2012; Kay2016; @Hullman2018; @kale2019decision]. 

#### Distributions shoud be in a single plot
Elements from a single distribution should be displayed using a single plot, since displaying the features of one distribution across multiple plots makes the information hard to combine and results in some details (such as the estimate error) being completely ignored [@moritz2017trust; Correll2018]. 

### Perceptual tasks wrt uncertainty
Not only should we considered the hierarchy in the information we display, but we also need to consider the heuristics that connect some pieces of information to specific elementary tasks. For example @Hofmann2012 showed that polar co-ordinates are more effective than cartesian co-ordinates when considering data that depicts a 360 degree direction (a case where polar co-ordinates has a natural interpretation). Uncertainty also has a natural mapping that should be considered when we express it in a plot. Error is best mapped to fuzziness, location, and colour value; arrangement, size and transparency are an OK second choice; but saturation, hue, orientation and shape are unacceptable and have no intuitive connection to variance [@Maceachren2012]. No only do the graphical elements we map our features to matter, but the direction matters too. Graphical elements that are more fuzzy (fuzziness), further from centre (location), lighter (colour value), poorly arranged (arrangement), smaller (size), more transparent (transparency) are perceived to be more uncertain [@Maceachren2012]. This idea also extends to interval estimation, where questions about probability are best answered with gradients and questions about start and end times are easiest to answer with ambiguation [@Gschwandtnei2016]. Heuristics can work against us just as much as they can work for us. The sine illusion can cause the confidence interval of a smoothed sine curve to seem wider at the peaks than the troughs, causing us to underestimate uncertainty associated with changing values [@Vanderplas2015]. Therefore we should not only keep the hierarchy of information in mind when we map features of our distribution, but also take advantage of these intuitive mappings when we can.

